{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b60c215",
   "metadata": {},
   "source": [
    "# Experiment Code Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d3f9f",
   "metadata": {},
   "source": [
    "## Setup and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a72024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import openai          \n",
    "import transformers    \n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.tsa.stattools as stt\n",
    "import statsmodels.tsa.holtwinters as smt\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "import statsmodels.tsa.arima.model as smt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5417758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Ensuring OpenAI API Key is retrieved from .env file\n",
    "load_dotenv()\n",
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36c14a",
   "metadata": {},
   "source": [
    "## S&P 500 Index Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838db252",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Market data\n",
    "### Literature lays out data suggestions and pipeline implementation\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2015-01-01\", end=\"2025-01-01\")\n",
    "\n",
    "# DataFrame with Date index and columns Open, High, Low, Close (used for Returns), Volume, Adj Close\n",
    "sp500['Return'] = sp500['Close'].pct_change()\n",
    "sp500 = sp500.reset_index() # Date column for merging\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date']).dt.date \n",
    "\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date']).dt.strftime('%Y-%m-%d') # Converting Date to string format for merging\n",
    "\n",
    "# Fetching macro data with VIX:\n",
    "vix = yf.download(\"^VIX\", start=\"2015-01-01\", end=\"2025-01-01\")[['Close']]\n",
    "vix.rename(columns={'Close':'VIX'}, inplace=True)\n",
    "vix = vix.reset_index()\n",
    "vix['Date'] = pd.to_datetime(vix['Date']).dt.date\n",
    "vix['Date'] = pd.to_datetime(vix['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merging the two DataFrames on 'Date'\n",
    "merged_data = pd.merge(sp500, vix, on='Date', how='inner')\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data.set_index('Date', inplace=False)\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "merged_data.to_csv('sp500_vix_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0762",
   "metadata": {},
   "source": [
    "## New York Times Article Summary Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66202ff",
   "metadata": {},
   "source": [
    "### General Article Scriping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NYT Article Summary Data\n",
    "import os, time, json, sys\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY = os.getenv(\"NYT_API_KEY\", \"your_real_key\")\n",
    "\n",
    "START_DATE  = date(2015, 1, 1)\n",
    "END_DATE    = date(2025, 1, 1)\n",
    "CSV_PATH    = \"nyt_business_archive.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC   = 60 / REQS_PER_MIN        # The 12s pause forces code to stay within the API rate limit of 5-calls per minute\n",
    "CHECKPOINT  = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "def archive_month(year: int, month: int, max_retries: int = 5) -> Dict: # Wrapper handles 429 and retries\n",
    "    \"\"\"Fetch one month from NYT Archive API with rate-limit back-off.\"\"\"\n",
    "    url    = ARCHIVE_URL.format(year=year, month=month)\n",
    "    params = {\"api-key\": NYT_API_KEY}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        resp = requests.get(url, params=params, timeout=60)\n",
    "        if resp.status_code == 429:                # Too Many Requests\n",
    "            wait = int(resp.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"429 received — sleeping {wait}s and retrying …\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    raise RuntimeError(f\"Failed to fetch {year}-{month:02} after {max_retries} retries\")\n",
    "\n",
    "def iter_months(start: date, end: date):\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        current = date(y, m, 1)\n",
    "        if current > end.replace(day=1):\n",
    "            break\n",
    "        yield current\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(d: date):\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "def append_rows(rows: List[Dict]):\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "def is_business(doc: Dict) -> bool:\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "def main():\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} — {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)      \n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d} docs\")\n",
    "        time.sleep(SLEEP_SEC)                              \n",
    "\n",
    "    print(\"All months processed. CSV downloaded\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted — progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43b836",
   "metadata": {},
   "source": [
    "### Aggregated Sector Mapping Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate Sector Mapping for NYT Article Summaries\n",
    "import os, time, json, sys, re\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "path = \"nyt_business_archive.csv\"\n",
    "\n",
    "df = pd.read_csv(path, header=None)  # no header to see all columns\n",
    "max_cols = df.shape[1]\n",
    "\n",
    "if max_cols == 5 and \"Summary\" not in pd.read_csv(path, nrows=0).columns:\n",
    "    df.columns = [\"Date\", \"Headline\", \"Summary\", \"Section\", \"URL\"]\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"✔ Added missing 'Summary' header and rewrote file.\")\n",
    "else:\n",
    "    print(\"No header problem detected — nothing changed.\")\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY  = os.getenv(\"NYT_API_KEY\", \"your_key\")\n",
    "START_DATE   = date(2015, 1, 1)\n",
    "END_DATE     = date(2025, 1, 1)\n",
    "CSV_PATH     = \"nyt_business_archive.csv\"\n",
    "AGG_PATH     = \"nyt_aggregated_data.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC    = 60 / REQS_PER_MIN       # 12 s → 5 req/min\n",
    "CHECKPOINT   = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL  = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "def archive_month(year: int, month: int, retries=5) -> Dict: # NYT API call with 429 back-off to handle rate limits\n",
    "    url, params = ARCHIVE_URL.format(year=year, month=month), {\"api-key\": NYT_API_KEY}\n",
    "    for _ in range(retries):\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        if r.status_code == 429:                       # Too Many Requests\n",
    "            wait = int(r.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"⚠429 – sleeping {wait}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    raise RuntimeError(f\"Failed after {retries} retries: {year}-{month:02}\")\n",
    "\n",
    "def iter_months(start: date, end: date): # Generator = iterates over months in a date range\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        first = date(y, m, 1)\n",
    "        if first > end.replace(day=1):\n",
    "            break\n",
    "        yield first\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "def load_checkpoint(): # Setting up checkpoint management for resuming crawls\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(d: date): # Saving the last processed month to a checkpoint file\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "def append_rows(rows: List[Dict]): # Rows to the CSV file, creating it if it doesn't exist\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "def is_business(doc: Dict) -> bool: # Business filter\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "def crawl_nyt_archive(): # Main crawling function to fetch NYT archive data\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} – {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d}\")\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "    print(\"NYT crawl complete.\")\n",
    "\n",
    "# Sector classification & aggregation\n",
    "SECTOR_MAP: dict[str, list[str]] = {\n",
    "    # Information Tech Secotr:\n",
    "    \"Software & IT Services\": [\n",
    "        \"software\", \"saas\", \"cloud\", \"it services\", \"consulting\",\n",
    "        \"microsoft\", \"adobe\", \"oracle\", \"sap\", \"salesforce\", \"servicenow\",\n",
    "        \"workday\", \"vmware\", \"accenture\", \"infosys\", \"tcs\", \"capgemini\",\n",
    "    ],\n",
    "    \"Hardware & Devices\": [\n",
    "        \"hardware\", \"pc\", \"laptop\", \"smartphone\", \"iphone\", \"ipad\",\n",
    "        \"dell\", \"hp\", \"lenovo\", \"asus\", \"acer\", \"logitech\",\n",
    "    ],\n",
    "    \"Semiconductors\": [\n",
    "        \"chip\", \"chips\", \"semiconductor\", \"fab\", \"foundry\",\n",
    "        \"intel\", \"amd\", \"nvidia\", \"qualcomm\", \"tsmc\", \"broadcom\",\n",
    "        \"micron\", \"arm holdings\", \"sk hynix\",\n",
    "    ],\n",
    "    \"Internet & Social Media\": [\n",
    "        \"google\", \"alphabet\", \"youtube\", \"search engine\",\n",
    "        \"meta\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"twitter\", \"x corp\", \"snapchat\", \"tiktok\", \"reddit\",\n",
    "        \"linkedin\", \"pinterest\", \"social media\",\n",
    "    ],\n",
    "    # Communication & Media Sector:\n",
    "    \"Telecommunications\": [\n",
    "        \"telecom\", \"5g\", \"wireless\", \"broadband\",\n",
    "        \"verizon\", \"at&t\", \"t-mobile\", \"comcast\", \"charter\",\n",
    "        \"vodafone\", \"telefonica\", \"bt group\", \"rogers\", \"singtel\",\n",
    "    ],\n",
    "    \"Media & Entertainment\": [\n",
    "        \"media\", \"streaming\", \"disney\", \"espn\", \"hulu\",\n",
    "        \"netflix\", \"warner bros\", \"hbo\", \"paramount\", \"peacock\",\n",
    "        \"sony pictures\", \"universal\", \"box office\", \"cinema\",\n",
    "    ],\n",
    "    # Consumer Sector:\n",
    "    \"Retail & E-Commerce\": [\n",
    "        \"retail\", \"e-commerce\", \"amazon\", \"alibaba\", \"shopify\",\n",
    "        \"ebay\", \"etsy\", \"walmart\", \"target\", \"costco\", \"kroger\",\n",
    "        \"best buy\", \"flipkart\", \"mercado libre\",\n",
    "    ],\n",
    "    \"Consumer Goods & Apparel\": [\n",
    "        \"nike\", \"adidas\", \"lululemon\", \"puma\", \"under armour\",\n",
    "        \"apparel\", \"footwear\", \"luxury\", \"lvmh\", \"gucci\", \"burberry\",\n",
    "        \"rolex\", \"hermes\", \"tapestry\",\n",
    "    ],\n",
    "    \"Food & Beverage\": [\n",
    "        \"food\", \"beverage\", \"coca-cola\", \"pepsico\", \"nestlé\",\n",
    "        \"restaurant\", \"fast food\", \"mcdonald\", \"starbucks\",\n",
    "        \"yum brands\", \"kfc\", \"pizza hut\", \"chipotle\",\n",
    "        \"kraft\", \"general mills\", \"heinz\", \"tyson foods\",\n",
    "    ],\n",
    "    \"Hospitality & Leisure\": [\n",
    "        \"hotel\", \"marriott\", \"hilton\", \"hyatt\", \"airbnb\",\n",
    "        \"booking.com\", \"expedia\", \"travel\", \"cruise\", \"carnival\",\n",
    "        \"royal caribbean\", \"las vegas sands\", \"mgm resorts\",\n",
    "    ],\n",
    "    \"Automotive\": [\n",
    "        \"automotive\", \"auto\", \"car\", \"vehicle\", \"ev\",\n",
    "        \"tesla\", \"general motors\", \"ford\", \"stellantis\",\n",
    "        \"volkswagen\", \"toyota\", \"nissan\", \"bmw\", \"mercedes\",\n",
    "        \"hyundai\", \"kia\", \"rivian\", \"lucid\",\n",
    "    ],\n",
    "    # Healthcare Sector:\n",
    "    \"Pharmaceuticals\": [\n",
    "        \"pharma\", \"drug\", \"medicine\", \"vaccine\", \"fda\",\n",
    "        \"pfizer\", \"moderna\", \"johnson & johnson\", \"merck\",\n",
    "        \"novartis\", \"roche\", \"astrazeneca\", \"bayer\", \"gsk\",\n",
    "    ],\n",
    "    \"Biotechnology\": [\n",
    "        \"biotech\", \"gene therapy\", \"crispr\", \"genomics\",\n",
    "        \"illumina\", \"gilead\", \"amgen\", \"biogen\", \"regeneron\",\n",
    "        \"vertex\", \"bluebird bio\",\n",
    "    ],\n",
    "    \"Medical Devices & Services\": [\n",
    "        \"medical device\", \"medtech\", \"diagnostics\", \"surgical\",\n",
    "        \"medtronic\", \"boston scientific\", \"abbott\", \"stryker\",\n",
    "        \"philips healthcare\", \"siemens healthineers\", \"cardinal health\",\n",
    "        \"hospital\", \"clinic\", \"healthcare services\",\n",
    "    ],\n",
    "    # Energy & Utilities Sector:\n",
    "    \"Oil & Gas\": [\n",
    "        \"oil\", \"gas\", \"petroleum\", \"upstream\", \"downstream\",\n",
    "        \"exxon\", \"chevron\", \"bp\", \"shell\", \"totalenergies\",\n",
    "        \"conocophillips\", \"aramco\", \"occidental\", \"slb\",\n",
    "    ],\n",
    "    \"Renewables & Clean Energy\": [\n",
    "        \"renewable\", \"solar\", \"wind\", \"geothermal\", \"hydro\",\n",
    "        \"clean energy\", \"green energy\", \"next era\", \"sunpower\",\n",
    "        \"first solar\", \"enphase\", \"vestas\", \"siemens gamesa\",\n",
    "        \"hydrogen\", \"electrolyzer\", \"fuel cell\",\n",
    "    ],\n",
    "    \"Utilities\": [\n",
    "        \"utility\", \"power grid\", \"electricity\", \"water utility\",\n",
    "        \"natural gas utility\", \"duke energy\", \"southern company\",\n",
    "        \"dominion\", \"pg&e\", \"national grid\", \"aes\",\n",
    "    ],\n",
    "    # Financials Sector:\n",
    "    \"Banks\": [\n",
    "        \"bank\", \"commercial bank\", \"jpmorgan\", \"bank of america\",\n",
    "        \"citigroup\", \"wells fargo\", \"goldman sachs\", \"morgan stanley\",\n",
    "        \"u.s. bancorp\", \"hsbc\", \"barclays\", \"santander\", \"dbs\",\n",
    "    ],\n",
    "    \"Investment & Asset Management\": [\n",
    "        \"asset manager\", \"blackrock\", \"vanguard\", \"fidelity\",\n",
    "        \"state street\", \"schwab\", \"hedge fund\", \"private equity\",\n",
    "        \"kkr\", \"carried interest\", \"mutual fund\", \"etf\",\n",
    "        \"sovereign wealth fund\",\n",
    "    ],\n",
    "    \"Insurance\": [\n",
    "        \"insurance\", \"insurer\", \"aig\", \"allstate\", \"progressive\",\n",
    "        \"metlife\", \"prudential\", \"chubb\", \"berkshire hathaway insurance\",\n",
    "        \"reinsurance\", \"lloyd's\", \"actuarial\",\n",
    "    ],\n",
    "    \"Fintech & Payments\": [\n",
    "        \"fintech\", \"payment\", \"visa\", \"mastercard\",\n",
    "        \"american express\", \"paypal\", \"block inc\", \"square\",\n",
    "        \"stripe\", \"sofi\", \"robinhood\", \"buy now pay later\",\n",
    "        \"klarna\", \"affirm\", \"ant financial\",\n",
    "    ],\n",
    "    \"Cryptocurrency & Blockchain\": [\n",
    "        \"bitcoin\", \"ethereum\", \"crypto\", \"blockchain\",\n",
    "        \"coinbase\", \"binance\", \"defi\", \"nft\",\n",
    "        \"stablecoin\", \"mining rig\", \"hashrate\",\n",
    "    ],\n",
    "    # Industrial Sector:\n",
    "    \"Aerospace & Defense\": [\n",
    "        \"aerospace\", \"defense\", \"boeing\", \"airbus\", \"northrop\",\n",
    "        \"lockheed martin\", \"raytheon\", \"bae systems\", \"general dynamics\",\n",
    "        \"drones\", \"satellite\", \"nasa contract\",\n",
    "    ],\n",
    "    \"Transportation & Logistics\": [\n",
    "        \"shipping\", \"freight\", \"logistics\", \"supply chain\",\n",
    "        \"fedex\", \"ups\", \"dhl\", \"maersk\", \"csx\", \"union pacific\",\n",
    "        \"delta airlines\", \"american airlines\", \"united airlines\",\n",
    "        \"railroad\", \"port congestion\",\n",
    "    ],\n",
    "    \"Manufacturing & Machinery\": [\n",
    "        \"manufacturing\", \"factory\", \"industrial\", \"caterpillar\",\n",
    "        \"3m\", \"general electric\", \"siemens\", \"honeywell\", \"emerson\",\n",
    "        \"robotics\", \"automation\", \"abb\", \"fanuc\",\n",
    "    ],\n",
    "    \"Construction & Engineering\": [\n",
    "        \"construction\", \"engineering\", \"infrastructure\",\n",
    "        \"bechtel\", \"fluor\", \"jacobs\", \"skanska\", \"kiewit\",\n",
    "        \"turner construction\", \"architect\", \"building materials\",\n",
    "    ],\n",
    "    \"Chemicals & Specialty Materials\": [\n",
    "        \"chemical\", \"chemicals\", \"specialty chemical\",\n",
    "        \"dupont\", \"dow\", \"basf\", \"lyondellbasell\", \"air products\",\n",
    "        \"eastman\", \"evonik\", \"synthetic rubber\", \"petrochemical\",\n",
    "    ],\n",
    "    \"Metals & Mining\": [\n",
    "        \"mining\", \"metal\", \"steel\", \"aluminum\", \"copper\",\n",
    "        \"iron ore\", \"rio tinto\", \"bhp\", \"vale\", \"newmont\",\n",
    "        \"glencore\", \"lithium\", \"nickel\", \"rare earth\",\n",
    "    ],\n",
    "    \"Agriculture\": [\n",
    "        \"agriculture\", \"farming\", \"crop\", \"soybean\", \"corn\",\n",
    "        \"wheat\", \"cargill\", \"archer daniels midland\", \"bunge\",\n",
    "        \"deere\", \"monsanto\", \"fertilizer\", \"nutrien\", \"potash\",\n",
    "    ],\n",
    "    # Real Estate Sector:\n",
    "    \"Real Estate\": [\n",
    "        \"real estate\", \"realtor\", \"reit\", \"property\", \"mortgage\",\n",
    "        \"office vacancy\", \"housing market\", \"zillow\", \"redfin\",\n",
    "        \"wework\", \"commercial property\", \"residential property\",\n",
    "        \"industrial park\", \"logistics park\",\n",
    "    ],\n",
    "    # ESG / Government / Education Sectors:\n",
    "    \"Environmental & ESG\": [\n",
    "        \"esg\", \"sustainability\", \"carbon\", \"emissions\",\n",
    "        \"carbon credit\", \"offset\", \"green bond\", \"climate risk\",\n",
    "        \"cop28\", \"environmental regulation\",\n",
    "    ],\n",
    "    \"Government & Policy\": [\n",
    "        \"government\", \"regulation\", \"legislation\", \"policy\",\n",
    "        \"federal reserve\", \"congress\", \"white house\",\n",
    "        \"eu commission\", \"trade tariff\", \"sanction\", \"geopolitics\",\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"education\", \"edtech\", \"university\", \"college\", \"school\",\n",
    "        \"coursera\", \"edx\", \"udemy\", \"chegg\", \"student loan\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def assign_sector(text: str) -> str: # Function to assign sectors based on keywords in the summary text\n",
    "    text_low = text.lower()\n",
    "    for sector, keywords in SECTOR_MAP.items():\n",
    "        if any(kw in text_low for kw in keywords):\n",
    "            return sector\n",
    "    return \"General\"\n",
    "  \n",
    "def aggregate_nyt(df: pd.DataFrame) -> pd.DataFrame: # Aggregation function to combine headlines and summaries by Date and Sector\n",
    "    \"\"\"Combine all Business headlines/summaries into one row per Date × Sector.\"\"\"\n",
    "    return (\n",
    "        df.groupby([\"Date\", \"Sector\"])\n",
    "          .agg({\n",
    "              \"Headline\": lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "              \"Summary\":  lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "          })\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "# Main function to run the entire process\n",
    "def main():\n",
    "    crawl_nyt_archive() # Crawl or resume NYT archive\n",
    "\n",
    "    nyt_df = pd.read_csv(CSV_PATH) # Loading full Business CSV\n",
    "    nyt_df[\"Headline\"] = nyt_df[\"Headline\"].astype(str)\n",
    "    nyt_df[\"Summary\"] = nyt_df[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "    nyt_df[\"Sector\"] = nyt_df[\"Summary\"].fillna(\"\").apply(assign_sector) # Assigning sectors\n",
    "\n",
    "    nyt_aggregated = aggregate_nyt(nyt_df) # Aggregate is saved\n",
    "    nyt_aggregated.to_csv(AGG_PATH, index=False)\n",
    "    print(f\"Aggregated file written: {AGG_PATH}\")\n",
    "  \n",
    "    if \"news_df\" in globals(): # Reuters clean-up if news_df already exists\n",
    "        news_df[\"Article\"] = (\n",
    "            news_df[\"Article\"]\n",
    "            .str.replace(r\"By .*? \\|\", \"\", regex=True)\n",
    "            .str.replace(r\"\\n+\", \" \", regex=True)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        print(\"Reuters news_df cleaned.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted – progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_aggregated = pd.read_csv(\"nyt_aggregated_data.csv\") # Printing the first few rows of the NYT scrapped aggregate data file\n",
    "print(nyt_aggregated.head())\n",
    "\n",
    "merged_data = pd.read_csv(\"sp500_vix_data.csv\") # Printing the first few rows of the merged S&P 500 and VIX data\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cd63b",
   "metadata": {},
   "source": [
    "## Senitment Analysis Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5b30d",
   "metadata": {},
   "source": [
    "### Unified FinBERT and GPT-4 Fall Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hybrid: FinBERT + GPT-4 Sentiment Analysis Pipeline\n",
    "### Follows implementation by ProsusAI https://github.com/ProsusAI/finBERT\n",
    "#  FinBERT gives primary sentiment score (pos‑prob − neg‑prob)\n",
    "#  The FinBERT-onyl score is extracted for benchmarking\n",
    "#  GPT‑4 called upon for fallback when FinBERT is effectively\n",
    "#  Fallback activated when FinBERT score is neutral (|score| < GPT4_CONFIDENCE_THRESHOLD)\n",
    "#  Carried out with Python 3.13\n",
    "\n",
    "import os\n",
    "import time \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "USE_GPT4_FALLBACK = True\n",
    "GPT4_CONFIDENCE_THRESHOLD = 0.05                     \n",
    "OPENAI_MODEL = \"gpt-4o-mini\" # GPT model choice supported by literature\n",
    "OPENAI_MAX_TOKENS = 10\n",
    "\n",
    "DERIVED_PATH = \"derived\"\n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "GPT4_CHECKPOINT_EVERY = 250   # save after every 250 GPT-4 calls\n",
    "\n",
    "if USE_GPT4_FALLBACK: # Add API key to .env in same directory\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise EnvironmentError(\"OPENAI_API_KEY not set.\")\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df = pd.read_csv(\"nyt_aggregated_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df.rename(columns={\"Summary\": \"summary\", \"Headline\": \"headline\", \"Sector\": \"sector\"}, inplace=True)\n",
    "\n",
    "trading_days = set(market_df[\"Date\"].dt.normalize()) # Aligning news dates with trading calendar\n",
    "news_df = news_df[news_df[\"Date\"].dt.normalize().isin(trading_days)].reset_index(drop=True)\n",
    "\n",
    "FINBERT_MODEL = \"ProsusAI/finbert\" # FinBERT model - ProsusAI\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)\n",
    "finbert.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def finbert_score(texts: List[str], batch_size: int = 32) -> Tuple[List[float], List[np.ndarray]]:\n",
    "    scores, probs_all = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        out = finbert(**enc)\n",
    "        probs = torch.nn.functional.softmax(out.logits, dim=1).cpu().numpy()\n",
    "        batch_scores = probs[:,1] - probs[:,2]\n",
    "        scores.extend(batch_scores.tolist())\n",
    "        probs_all.extend(probs)\n",
    "    return scores, probs_all\n",
    "\n",
    "def gpt4_sentiment_single(text):\n",
    "    \"\"\"Call GPT-4 for sentiment fallback, return -1, 0, or 1.\"\"\"        \n",
    "    system_msg = \"You are a financial news analyst.\"\n",
    "    user_msg = (\n",
    "        \"Classify the sentiment of the given NY Times news article summary {summary}, which is closely related to the {sector} industry, as positive for buy, negative for sell, or neutral for hold position, for the US Stock market and provide the probability values for your classification.\"\n",
    "        \"Answer with just one word.\\n\\n\"\n",
    "        f\"Summary: \\\"{text}\\\"\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\", \"content\": user_msg}],\n",
    "            max_tokens=OPENAI_MAX_TOKENS,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        ans = response.choices[0].message.content.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(\"GPT-4 error (treated as neutral):\", e)\n",
    "        return 0.0\n",
    "    if ans.startswith(\"pos\"):\n",
    "        return 1.0\n",
    "    if ans.startswith(\"neg\"):\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "\n",
    "def add_sentiment(news_df, use_gpt4=False):\n",
    "    finbert_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\") # Checkpoint 1: If FinBERT file exists, skip FinBERT and load\n",
    "    if os.path.exists(finbert_path):\n",
    "        print(\"Checkpoint: FinBERT sentiment file found. Loading and skipping FinBERT step.\")\n",
    "        news_df = pd.read_csv(finbert_path, parse_dates=[\"Date\"])\n",
    "    else:\n",
    "        print(\"Scoring FinBERT…\")\n",
    "        start_finbert = time.time()\n",
    "        fin_scores, fin_probs = finbert_score(news_df[\"summary\"].tolist())\n",
    "        end_finbert = time.time()\n",
    "        news_df[\"FinBERT_score\"] = fin_scores\n",
    "        news_df[\"FinBERT_prob_neu\"] = [p[0] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_pos\"] = [p[1] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_neg\"] = [p[2] for p in fin_probs]\n",
    "        news_df[\"FinBERT_confidence\"] = news_df[\"FinBERT_score\"].abs()\n",
    "        news_df[\"Sentiment\"] = news_df[\"FinBERT_score\"]  # Start with FinBERT\n",
    "        print(f\"FinBERT sentiment scored in {end_finbert - start_finbert:.2f} seconds.\")\n",
    "        news_df.to_csv(finbert_path, index=False)\n",
    "        print(f\"Checkpoint: FinBERT sentiment saved to {finbert_path}\")\n",
    "\n",
    "    num_gpt4 = 0\n",
    "    gpt4_path = os.path.join(DERIVED_PATH, \"nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "\n",
    "    if use_gpt4:\n",
    "        if os.path.exists(gpt4_path): # If checkpoint exists, load and skip fallback\n",
    "            print(\"Checkpoint: GPT-4 fallback file found. Loading and skipping fallback step.\")\n",
    "            news_df = pd.read_csv(gpt4_path, parse_dates=[\"Date\"])\n",
    "        else:\n",
    "            print(\"Running GPT-4 fallback…\")\n",
    "            low_conf_mask = news_df[\"FinBERT_confidence\"] < GPT4_CONFIDENCE_THRESHOLD\n",
    "            num_gpt4 = low_conf_mask.sum()\n",
    "            print(f\"News needing GPT-4 fallback: {num_gpt4} of {len(news_df)}\")\n",
    "            start_gpt4 = time.time()\n",
    "            checkpoint_counter = 0\n",
    "            for idx_num, idx in enumerate(news_df[low_conf_mask].index):\n",
    "                news_df.at[idx, \"Sentiment\"] = gpt4_sentiment_single(news_df.at[idx, \"summary\"])\n",
    "                checkpoint_counter += 1\n",
    "                if checkpoint_counter % GPT4_CHECKPOINT_EVERY == 0: # Intermediate checkpoint\n",
    "                    news_df.to_csv(gpt4_path, index=False)\n",
    "                    print(f\"Checkpoint: Saved GPT-4 fallback at {checkpoint_counter} / {num_gpt4} GPT-4 calls.\")\n",
    "            end_gpt4 = time.time()\n",
    "            print(f\"GPT-4 fallback completed in {end_gpt4 - start_gpt4:.2f} seconds.\")\n",
    "            news_df.to_csv(gpt4_path, index=False)\n",
    "            print(f\"Checkpoint: Final GPT-4 fallback saved to {gpt4_path}\")\n",
    "    else:\n",
    "        num_gpt4 = 0\n",
    "\n",
    "    return news_df, num_gpt4\n",
    "   \n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True) # Main Pipeline\n",
    "pipeline_start = time.time()\n",
    "news_df, num_gpt4 = add_sentiment(news_df, use_gpt4=USE_GPT4_FALLBACK)\n",
    "pipeline_end = time.time()\n",
    "print(f\"Total pipeline runtime: {pipeline_end - pipeline_start:.2f} seconds.\")\n",
    "\n",
    "# Calculaitng the daily sentiment aggregate\n",
    "sent_daily = (\n",
    "    news_df.groupby(news_df[\"Date\"].dt.normalize())[\"Sentiment\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sent_daily_path = os.path.join(DERIVED_PATH, \"daily_sentiment_aggregate.csv\")\n",
    "sent_daily.to_csv(sent_daily_path, index=False)\n",
    "\n",
    "# Merging with market data for modeling (LSTM)\n",
    "market_merge = pd.merge(\n",
    "    market_df, \n",
    "    sent_daily, \n",
    "    left_on=market_df[\"Date\"].dt.normalize(), \n",
    "    right_on=sent_daily[\"Date\"].dt.normalize(), \n",
    "    how=\"left\", \n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_path = os.path.join(DERIVED_PATH, \"final_merged_for_lstm.csv\")\n",
    "market_merge.to_csv(final_out_path, index=False)\n",
    "print(f\"Checkpoint: Final merged LSTM-ready data saved to {final_out_path}\")\n",
    "\n",
    "print(f\"All CSVs saved to: {DERIVED_PATH}/\")\n",
    "print(\"Output files:\")\n",
    "print(\" - nyt_with_finbert_sentiment.csv\")\n",
    "print(\" - nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "print(\" - daily_sentiment_aggregate.csv\")\n",
    "print(\" - final_merged_for_lstm.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f24325",
   "metadata": {},
   "source": [
    "### FinBERT-only Sentiment Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c20294",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_only_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\") # Creating daily FinBERT-only sentiment aggregate for benchmarking\n",
    "finbert_df = pd.read_csv(finbert_only_path, parse_dates=[\"Date\"])\n",
    "\n",
    "daily_finbert_sent = (\n",
    "    finbert_df.groupby(finbert_df[\"Date\"].dt.normalize())[\"FinBERT_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "daily_finbert_sent_path = os.path.join(DERIVED_PATH, \"daily_FinBERT_sentiment_aggregate.csv\")\n",
    "daily_finbert_sent.to_csv(daily_finbert_sent_path, index=False)\n",
    "\n",
    "# Merging with market data for modeling (LSTM) FinBERT-only version, acts as a benchmark\n",
    "market_merge_finbert = pd.merge(\n",
    "    market_df,\n",
    "    daily_finbert_sent,\n",
    "    left_on=market_df[\"Date\"].dt.normalize(),\n",
    "    right_on=daily_finbert_sent[\"Date\"].dt.normalize(),\n",
    "    how=\"left\",\n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_finbert_path = os.path.join(DERIVED_PATH, \"final_merged_FinBERT_for_lstm.csv\")\n",
    "market_merge_finbert.to_csv(final_out_finbert_path, index=False)\n",
    "\n",
    "print(f\"Checkpoint: FinBERT-only daily sentiment aggregate saved to {daily_finbert_sent_path}\")\n",
    "print(f\"Checkpoint: Final merged FinBERT-only LSTM-ready data saved to {final_out_finbert_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6b23d",
   "metadata": {},
   "source": [
    "#Note: The OpenAI API usage as shown might need adaptation for actual GPT-4 (which might require using openai.ChatCompletion.create with messages in the new API format rather than the older Completion.create). But the idea stands – send the article text and get a sentiment label. The prompt here is simplified; in practice, one could include examples or a role specification for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f705a",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae406e",
   "metadata": {},
   "source": [
    "#### Implementation laid out by Jakob Aungiers https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756c607",
   "metadata": {},
   "source": [
    "##### Conda virtual enviroment is implemented to run Python 3.11 throughout the remainder of this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1967a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!pip install numpy pandas scikit-learn tensorflow keras matplotlib\n",
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce71d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b459e0",
   "metadata": {},
   "source": [
    "### Hybrid (FinBERT+GPT) Sentiment LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning up columns\n",
    "\n",
    "for col in df.columns: # Ensuring all columns except Date are numeric\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "\n",
    "N_LAGS = 2 # Lagged features added below\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling (IMPORTANT) \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c1989",
   "metadata": {},
   "source": [
    "### FinBERT-only LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d53e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    " \n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning columns\n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']: # Removing column headers like '^GSPC' in Close/Open/etc (non-numeric entries)\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns: # Ensure all except 'Date' are numeric\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering \n",
    "N_LAGS = 2  # Adding lags for target and sentiment\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # aligning with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc6953",
   "metadata": {},
   "source": [
    "#### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829710d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for name in [\"derived/lstm_test_predictions.csv\",\n",
    "             \"derived/lstm_FinBERT_only_test_predictions.csv\"]:\n",
    "    df = pd.read_csv(name, parse_dates=[\"Date\"])\n",
    "    dup = df.duplicated(subset=[\"Date\"]).sum()\n",
    "    print(name, \"rows:\", len(df), \"duplicates:\", dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eec805",
   "metadata": {},
   "source": [
    "## Evaluaiton and Visualizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb66cd8",
   "metadata": {},
   "source": [
    "### Forecast Biasness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "df_gpt   = pd.read_csv(\"derived/lstm_test_predictions.csv\",\n",
    "                       parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\",\n",
    "                       parse_dates=[\"Date\"])\n",
    "market   = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = (df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]]\n",
    "        .rename(columns={\"Return\":\"Market_Return\",\n",
    "                         \"Predicted_Return\":\"LSTM_GPT4\"})\n",
    "      .merge(df_fbert[[\"Date\",\"Predicted_Return\"]]\n",
    "                .rename(columns={\"Predicted_Return\":\"LSTM_FinBERT\"}),\n",
    "             on=\"Date\", how=\"left\")\n",
    "      .merge(market[[\"Date\",\"Return\",\"VIX\"]]\n",
    "                .rename(columns={\"Return\":\"Market_True_Return\"}),\n",
    "             on=\"Date\", how=\"left\")\n",
    "      .dropna(subset=[\"Market_True_Return\",\"LSTM_GPT4\",\"LSTM_FinBERT\"]))\n",
    "\n",
    "y_true   = df[\"Market_True_Return\"].values # Handles\n",
    "pred_gpt = df[\"LSTM_GPT4\"].values\n",
    "pred_fb  = df[\"LSTM_FinBERT\"].values\n",
    "\n",
    "def signal_metrics(y, yhat): # Forecast-Evaluation Metrics \n",
    "    \"\"\"Return dict of point & trading metrics.\"\"\"\n",
    "    mse  = np.mean((y - yhat)**2)\n",
    "    mae  = np.mean(np.abs(y - yhat))\n",
    "    rmse = np.sqrt(mse)\n",
    "    strat_ret = np.where(yhat>0, 1, -1) * y # Trading signal is long if yhat>0, short otherwise\n",
    "    sharpe = np.mean(strat_ret)/(np.std(strat_ret)+1e-9)*np.sqrt(252)\n",
    "    r2 = sm.OLS(y, sm.add_constant(yhat)).fit().rsquared\n",
    "    return dict(MSE=mse, MAE=mae, Directional_Acc=acc,\n",
    "                Sharpe=sharpe, R2=r2)\n",
    "   \n",
    "m_gpt = signal_metrics(y_true, pred_gpt)\n",
    "m_fb  = signal_metrics(y_true, pred_fb)\n",
    "\n",
    "def dm_test(e1, e2, h=1): # Diebold–Mariano (squared-error loss)\n",
    "    d   = e1-e2\n",
    "    T   = len(d)\n",
    "    var = np.var(d, ddof=1) + 2*sum(\n",
    "          np.cov(d[:-k],d[k:])[0,1] for k in range(1,h))\n",
    "    dm  = np.mean(d)/np.sqrt(var/T)\n",
    "    p   = 2*(1-stats.norm.cdf(abs(dm)))\n",
    "    return dm, p\n",
    "\n",
    "dm_stat, dm_p = dm_test((y_true-pred_gpt)**2, (y_true-pred_fb)**2)\n",
    "\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\") # Regime (VIX median) Sharpe comparison\n",
    "vix_med = df[\"VIX\"].median()\n",
    "reg   = {}\n",
    "for regime, lab in [(df[\"VIX\"]>vix_med,\"High-VIX\"),\n",
    "                    (df[\"VIX\"]<=vix_med,\"Low-VIX\")]:\n",
    "    reg[lab] = dict(GPT4   = signal_metrics(\n",
    "                                y_true[regime], pred_gpt[regime])[\"Sharpe\"],\n",
    "                    FinBERT = signal_metrics(\n",
    "                                y_true[regime], pred_fb[regime])[\"Sharpe\"])\n",
    "\n",
    "def bias_tests(y, yhat, label): # Forecast-Bias Section\n",
    "    err = y - yhat\n",
    "\n",
    "    t, p = stats.ttest_1samp(err, 0.0) # Mean-error t-test\n",
    "\n",
    "    X = sm.add_constant(yhat) # Mincer–Zarnowitz regression\n",
    "    mz = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Joint test α=0, β=1  (forecast unbiased)\n",
    "    R = np.eye(2)\n",
    "    q = np.array([0,1])\n",
    "    ftest = mz.f_test((R,q))\n",
    "    return dict(\n",
    "        Model          = label,\n",
    "        Mean_Error     = err.mean(),\n",
    "        MeanErr_tstat  = t,\n",
    "        MeanErr_pval   = p,\n",
    "        MZ_alpha       = mz.params[0],\n",
    "        MZ_beta        = mz.params[1],\n",
    "        MZ_alpha_p     = mz.pvalues[0],\n",
    "        MZ_beta_p      = mz.pvalues[1],\n",
    "        MZ_F_pvalue    = float(ftest.pvalue)\n",
    "    )\n",
    "\n",
    "bias_gpt = bias_tests(y_true, pred_gpt, \"Hybrid (GPT-4)\")\n",
    "bias_fb  = bias_tests(y_true, pred_fb , \"FinBERT-only\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Forecast-evaluation\n",
    "eval_tbl = pd.DataFrame([m_gpt, m_fb], index=[\"Hybrid\",\"FinBERT\"])\n",
    "print(\"Forecast-Evaluation Metrics\")\n",
    "print(tabulate(eval_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "print(\"\\nDiebold–Mariano statistic: {:.3f}   p-value: {:.3f}\" # DM-test summary\n",
    "      .format(dm_stat, dm_p))\n",
    "\n",
    "print(\"Regime-dependent Sharpe\") # Regime Sharpe\n",
    "print(tabulate(pd.DataFrame(reg).T, headers=\"keys\",\n",
    "               tablefmt=\"github\", floatfmt=\".3f\"))\n",
    "\n",
    "bias_tbl = pd.DataFrame([bias_gpt, bias_fb]).set_index(\"Model\") # Bias tests\n",
    "print(\"Forecast-Bias Diagnostics\")\n",
    "print(tabulate(bias_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "from sklearn.metrics import accuracy_score # computing validation accuracy for both models\n",
    "\n",
    "print(\"Validation Accuracy\")\n",
    "\n",
    "val_acc_gpt = accuracy_score(\n",
    "    (df[\"Market_True_Return\"].values > 0),   # true direction\n",
    "    (df[\"LSTM_GPT4\"].values          > 0)    # predicted direction\n",
    ")\n",
    "\n",
    "val_acc_fb = accuracy_score(\n",
    "    (df[\"Market_True_Return\"].values > 0),\n",
    "    (df[\"LSTM_FinBERT\"].values       > 0)\n",
    ")\n",
    "\n",
    "print(f\"Hybrid (GPT-4) Validation Accuracy:  {val_acc_gpt:.2%}\")\n",
    "print(f\"FinBERT-only Validation Accuracy:    {val_acc_fb:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e60e5",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a169775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Aligning data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60): # Computing metrics for each model (with rolling mean/std)\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Creating valid data masks and calculate for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "# Table (include rolling mean/std)\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"\\n===== Model Comparison Table =====\\n\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "# 4. Regime split (by VIX): show model dominance in high/low volatility\n",
    "vix_median = df[\"VIX\"].median()\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime ===\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1364d2",
   "metadata": {},
   "source": [
    "### Resulting Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, r2_score\n",
    ")\n",
    "\n",
    "fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "gpt4 = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "for df in [fbert, gpt4]: # changing objects (str) columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != \"Date\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\") # General settings\n",
    "sns.set(font_scale=1.2)\n",
    "window = 21  # 1 month rolling was chosen\n",
    "\n",
    "df = fbert[[\"Date\", \"Return\", \"VIX\", \"FinBERT_score\", \"Predicted_Return\"]].copy() # Mergeing for side-by-side plots\n",
    "df = df.rename(columns={\"Predicted_Return\": \"FinBERT_Pred\"})\n",
    "df[\"GPT4_Pred\"] = gpt4[\"Predicted_Return\"].values\n",
    "df[\"GPT4_Sentiment\"] = gpt4[\"Sentiment\"].values if \"Sentiment\" in gpt4 else np.nan\n",
    "\n",
    "\n",
    "# Actual vs LSTM Forecasts: Time Series Overlap\n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"], color='black', label='Market Return', linewidth=2)\n",
    "plt.plot(df[\"Date\"], df[\"FinBERT_Pred\"], color='orange', label='FinBERT-only LSTM', alpha=0.8)\n",
    "plt.plot(df[\"Date\"], df[\"GPT4_Pred\"], color='red', label='FinBERT+GPT-4 LSTM', alpha=0.7)\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Market Returns vs LSTM Model Forecasts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Model Error: Residuals Over Time \n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"FinBERT_Pred\"], label=\"Error: FinBERT LSTM\", color=\"orange\", alpha=0.6)\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"GPT4_Pred\"], label=\"Error: GPT-4 LSTM\", color=\"dodgerblue\", alpha=0.6)\n",
    "plt.axhline(0, color=\"black\", linewidth=1, linestyle=\":\")\n",
    "plt.ylabel(\"Residual (Error)\")\n",
    "plt.title(\"Model Residuals: Market - Model Forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Distribution of Forecasts: Histogram & KDE \n",
    "plt.figure(figsize=(14,5))\n",
    "sns.histplot(df[\"Return\"], label=\"Market Return\", color=\"black\", kde=True, stat=\"density\", bins=40)\n",
    "sns.histplot(df[\"FinBERT_Pred\"], label=\"FinBERT LSTM\", color=\"orange\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "sns.histplot(df[\"GPT4_Pred\"], label=\"GPT-4 LSTM\", color=\"dodgerblue\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Market Returns vs LSTM Model Forecasts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Actual vs Predicted: Scatter Plots and Regression Fit\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,6), sharey=True)\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"FinBERT_Pred\"], ax=axs[0], line_kws={\"color\": \"orange\"})\n",
    "axs[0].set_title(\"FinBERT LSTM: Actual vs Predicted\")\n",
    "axs[0].set_xlabel(\"Actual Return\")\n",
    "axs[0].set_ylabel(\"Predicted Return\")\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"GPT4_Pred\"], ax=axs[1], line_kws={\"color\": \"dodgerblue\"})\n",
    "axs[1].set_title(\"GPT-4 LSTM: Actual vs Predicted\")\n",
    "axs[1].set_xlabel(\"Actual Return\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Rolling Model RMSE (21d window)\n",
    "rmse_fbert = (df[\"Return\"] - df[\"FinBERT_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "rmse_gpt4 = (df[\"Return\"] - df[\"GPT4_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.plot(df[\"Date\"], rmse_fbert, label=\"Rolling RMSE: FinBERT LSTM\", color=\"orange\")\n",
    "plt.plot(df[\"Date\"], rmse_gpt4, label=\"Rolling RMSE: GPT-4 LSTM\", color=\"dodgerblue\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"21-Day Rolling RMSE: Model Performance Over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ROC Curve: Directional Signal of Models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "true_bin = (df[\"Return\"] > 0).astype(int)\n",
    "fpr_fbert, tpr_fbert, _ = roc_curve(true_bin, df[\"FinBERT_Pred\"])\n",
    "fpr_gpt4, tpr_gpt4, _ = roc_curve(true_bin, df[\"GPT4_Pred\"])\n",
    "auc_fbert = auc(fpr_fbert, tpr_fbert)\n",
    "auc_gpt4 = auc(fpr_gpt4, tpr_gpt4)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_fbert, tpr_fbert, label=f\"FinBERT LSTM (AUC={auc_fbert:.2f})\", color=\"orange\")\n",
    "plt.plot(fpr_gpt4, tpr_gpt4, label=f\"GPT-4 LSTM (AUC={auc_gpt4:.2f})\", color=\"dodgerblue\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: Model Directional Prediction Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Error Autocorrelation\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "plt.figure(figsize=(8,4))\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"FinBERT_Pred\"])\n",
    "plt.title(\"Error Autocorrelation: FinBERT LSTM\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,4))\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"GPT4_Pred\"])\n",
    "plt.title(\"Error Autocorrelation: GPT-4 LSTM\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cumulative Return Plot\n",
    "df = pd.DataFrame({ # Aligning data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],  # Use market_df[\"Return\"] if better aligned\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)  # Long/short signal\n",
    "    return signal * true_returns\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Establishing valid data masks and calculate for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'] # Align dates\n",
    "\n",
    "gpt_signal = np.where(df.loc[mask_gpt, 'LSTM_GPT4'] > 0, 1, -1) # Computing trading strategy returns for both models\n",
    "finbert_signal = np.where(df.loc[mask_fbert, 'LSTM_FinBERT'] > 0, 1, -1)\n",
    "\n",
    "gpt_strat_returns = gpt_signal * df.loc[mask_gpt, 'Market_True_Return'].values\n",
    "finbert_strat_returns = finbert_signal * df.loc[mask_fbert, 'Market_True_Return'].values\n",
    "\n",
    "gpt_cum_return = np.cumsum(gpt_strat_returns) # Computing cumulative returns\n",
    "finbert_cum_return = np.cumsum(finbert_strat_returns)\n",
    "market_cum_return = np.cumsum(df.loc[mask_gpt, 'Market_True_Return'].values)\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(dates, market_cum_return, label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(dates, gpt_cum_return, label=\"Hybrid Cumulative Return\", color='red')\n",
    "plt.plot(dates, finbert_cum_return, label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "plt.title(\"Cumulative Strategy Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Rolling Directional Accuracy and Sharpe Ratio \n",
    "win = 63      \n",
    "def rolling_directional_acc(true_ret, pred_ret, win=win):\n",
    "    \"\"\"\n",
    "    Percentage of days inside a rolling window where sign(pred)==sign(true).\n",
    "    win=63 ≈ one quarter of trading days.\n",
    "    \"\"\"\n",
    "    hit = (np.sign(true_ret) == np.sign(pred_ret)).astype(int)\n",
    "    return pd.Series(hit).rolling(win, min_periods=1).mean() * 100  # % scale\n",
    "\n",
    "mkt_true = df.loc[mask_gpt, \"Market_True_Return\"].values  # Aligning the same true-return vector you used for Sharpe\n",
    "pred_gpt = df.loc[mask_gpt, \"LSTM_GPT4\"].values\n",
    "pred_fbt = df.loc[mask_fbert, \"LSTM_FinBERT\"].values\n",
    "dates_da = df.loc[mask_gpt, \"Date\"].values\n",
    "\n",
    "da_gpt   = rolling_directional_acc(mkt_true, pred_gpt)\n",
    "da_fbt   = rolling_directional_acc(mkt_true, pred_fbt)\n",
    "\n",
    "dates_all = df.loc[mask_gpt, \"Date\"].values \n",
    "\n",
    "def rolling_sharpe(returns, win=win): # Defining sharpe_gpt and sharpe_fbert\n",
    "    \"\"\"\n",
    "    Calculate rolling Sharpe ratio over a specified window.\n",
    "    Returns annualised Sharpe ratio.\n",
    "    \"\"\"\n",
    "    mean_ret = returns.rolling(win).mean()\n",
    "    std_ret  = returns.rolling(win).std(ddof=0)  # population std\n",
    "    return (mean_ret / (std_ret + 1e-9)) * np.sqrt(252)  # annualised\n",
    "sharpe_gpt   = rolling_sharpe(df.loc[mask_gpt, \"Market_True_Return\"] - df.loc[mask_gpt, \"LSTM_GPT4\"])\n",
    "sharpe_fbert = rolling_sharpe(df.loc[mask_fbert, \"Market_True_Return\"] - df.loc[mask_fbert, \"LSTM_FinBERT\"])\n",
    "\n",
    "trim = win - 1 # Trimming FIRST (win-1) observations where rolling metrics\n",
    "dates_trim        = dates_all[trim:]          # shared for both plots\n",
    "sharpe_gpt_trim   = sharpe_gpt.iloc[trim:]\n",
    "sharpe_fbt_trim   = sharpe_fbert.iloc[trim:]\n",
    "da_gpt_trim       = da_gpt.iloc[trim:]\n",
    "da_fbt_trim       = da_fbt.iloc[trim:]\n",
    "\n",
    "plt.figure(figsize=(15,5)) # Rolling Sharpe (after trim)\n",
    "plt.plot(dates_trim, sharpe_gpt_trim,   label=\"Hybrid rolling Sharpe\",  c=\"red\")\n",
    "plt.plot(dates_trim, sharpe_fbt_trim,   label=\"FinBERT rolling Sharpe\", c=\"darkorange\")\n",
    "plt.axhline(0, ls='--', c='k')\n",
    "plt.title(f\"Rolling Sharpe Ratio ({win}-day Window)\")\n",
    "plt.ylabel(\"Sharpe Ratio (annualised)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5)) # Rolling Directional-Accuracy (after trim)\n",
    "plt.plot(dates_trim, da_gpt_trim, label=\"Hybrid directional accuracy\",  c=\"red\")\n",
    "plt.plot(dates_trim, da_fbt_trim, label=\"FinBERT directional accuracy\", c=\"darkorange\")\n",
    "plt.axhline(50, ls='--', c='k', lw=0.8)       # coin-flip baseline\n",
    "plt.ylim(0, 100)\n",
    "plt.title(f\"Rolling Directional Accuracy ({win}-day Window)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_conf_mat(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true > 0, y_pred > 0)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Down\", \"Up\"], yticklabels=[\"Down\", \"Up\"], ax=ax)\n",
    "    ax.set_xlabel(\"Model Signal\"); ax.set_ylabel(\"Market Direction\"); ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "plot_conf_mat(df.loc[mask_gpt,\"Market_True_Return\"],\n",
    "              df.loc[mask_gpt,\"LSTM_GPT4\"],\n",
    "              \"Hybrid\", axes[0])\n",
    "plot_conf_mat(df.loc[mask_fbert,\"Market_True_Return\"],\n",
    "              df.loc[mask_fbert,\"LSTM_FinBERT\"],\n",
    "              \"FinBERT-only\", axes[1])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# Turnover Function Vs Net-Long Ratio\n",
    "def turnover(signal):\n",
    "    return (np.abs(np.diff(signal)) / 2).mean() # Average fraction of days where direction flips\n",
    "\n",
    "sig_gpt   = np.where(df.loc[mask_gpt,  'LSTM_GPT4']   > 0, 1, -1)\n",
    "sig_fbert = np.where(df.loc[mask_fbert,'LSTM_FinBERT']> 0, 1, -1)\n",
    "\n",
    "print(f\"Hybrid turnover:  {turnover(sig_gpt):.2%}\")\n",
    "print(f\"FinBERT turnover: {turnover(sig_fbert):.2%}\")\n",
    "print(f\"Hybrid net-long ratio:  {(sig_gpt==1).mean():.2%}\")\n",
    "print(f\"FinBERT net-long ratio: {(sig_fbert==1).mean():.2%}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sig_gpt   = np.where(df.loc[mask_gpt,  'LSTM_GPT4']   > 0, 1, -1) # Re-computing signals (or reuse sig_gpt / sig_fbert from earlier)\n",
    "sig_fbert = np.where(df.loc[mask_fbert,'LSTM_FinBERT']> 0, 1, -1)\n",
    "\n",
    "def turnover(s):                      # share of days that flip direction\n",
    "    return (np.abs(np.diff(s)) / 2).mean()\n",
    "\n",
    "turn = [turnover(sig_gpt), turnover(sig_fbert)]\n",
    "netL = [(sig_gpt == 1).mean(), (sig_fbert == 1).mean()]   # net-long ratios\n",
    "\n",
    "labels = [\"Hybrid\\n(FinBERT+GPT-4)\", \"FinBERT-only\"]\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, turn, width,    # Left y-axis => Turnover\n",
    "                color=\"steelblue\", alpha=.85, label=\"Turnover\")\n",
    "ax1.set_ylabel(\"Turnover (% of days)\", color=\"steelblue\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, fontsize=11)\n",
    "ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n",
    "\n",
    "ax2 = ax1.twinx()   # Right y-axis => Net-Long exposure\n",
    "bars2 = ax2.bar(x + width/2, netL, width,\n",
    "                color=\"darkorange\", alpha=.85, label=\"Net-Long Ratio\")\n",
    "ax2.set_ylabel(\"Net-Long Exposure\", color=\"darkorange\")\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkorange\")\n",
    "\n",
    "for rect in bars1 + bars2:   # Annotating bars\n",
    "    height = rect.get_height()\n",
    "    ax = ax1 if rect in bars1 else ax2\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 0.02,\n",
    "            f\"{height:.2%}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.title(\"Turnover vs Net-Long Ratio\", pad=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38106b0",
   "metadata": {},
   "source": [
    "## Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524b9a8",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: Hybrid Method (Shorter Time Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Clean Up Columns\n",
    "\n",
    "for col in df.columns: # Ensuring all columns except Date are numeric\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "N_LAGS = 2 #lagged features\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Directional accuracy\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_prac_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1b240",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: FinBERT-only Method (Shorter Time Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Clean columns \n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']: # Removing non-numeric entry columns\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns: # Set all except 'Date' are numeric\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering\n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # aligning with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5ea5b",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e59b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_prac_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Align data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60): # Computing all metrics for each model (with rolling mean/std)\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Data masks and calculated for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "comparison_table = pd.DataFrame({ # Table\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"Model Comparison Table\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "print(\"Confusion Matrices\") # Confusion matrices for appendix\n",
    "print(\"Hybrid (FinBERT+GPT-4):\\n\", metrics_gpt[\"Confusion_Matrix\"])\n",
    "print(\"FinBERT-only:\\n\", metrics_finbert[\"Confusion_Matrix\"])\n",
    "np.savetxt(\"hybrid_confusion_matrix.csv\", metrics_gpt[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "np.savetxt(\"finbert_confusion_matrix.csv\", metrics_finbert[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "\n",
    "vix_median = df[\"VIX\"].median() # Regime split (by VIX) shows model dominance in high/low volatility\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime ===\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738074c",
   "metadata": {},
   "source": [
    "### Shortened Time-frame Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_prac_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Align data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],  # Use market_df[\"Return\"] if better aligned\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)  # Long/short signal\n",
    "    return signal * true_returns\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Data masks and calculated for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'] # Align dates\n",
    "\n",
    "gpt_signal = np.where(df.loc[mask_gpt, 'LSTM_GPT4'] > 0, 1, -1) # Trading strategy returns for both models\n",
    "finbert_signal = np.where(df.loc[mask_fbert, 'LSTM_FinBERT'] > 0, 1, -1)\n",
    "\n",
    "gpt_strat_returns = gpt_signal * df.loc[mask_gpt, 'Market_True_Return'].values\n",
    "finbert_strat_returns = finbert_signal * df.loc[mask_fbert, 'Market_True_Return'].values\n",
    "\n",
    "gpt_cum_return = np.cumsum(gpt_strat_returns) # Cumulative returns\n",
    "finbert_cum_return = np.cumsum(finbert_strat_returns)\n",
    "market_cum_return = np.cumsum(df.loc[mask_gpt, 'Market_True_Return'].values)\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5)) # Plotting cumulative returns\n",
    "plt.plot(dates, market_cum_return, label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(dates, gpt_cum_return, label=\"Hybrid Cumulative Return\", color='red')\n",
    "plt.plot(dates, finbert_cum_return, label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "plt.title(\"Cumulative Strategy Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
