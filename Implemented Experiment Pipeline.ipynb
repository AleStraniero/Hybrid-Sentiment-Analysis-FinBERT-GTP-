{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b60c215",
   "metadata": {},
   "source": [
    "# Experiment Code Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d3f9f",
   "metadata": {},
   "source": [
    "## Setup and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8fc9736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notebook in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.4.3)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from notebook) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from notebook) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.5,>=4.4.3 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from notebook) (4.4.3)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from notebook) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from notebook) (6.4.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (23.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.7.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (25.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.21.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.15)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (26.4.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=5.6.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.14.3)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab<4.5,>=4.4.3->notebook) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab<4.5,>=4.4.3->notebook) (0.28.1)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab<4.5,>=4.4.3->notebook) (6.29.5)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab<4.5,>=4.4.3->notebook) (2.2.5)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab<4.5,>=4.4.3->notebook) (80.1.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.12.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.5,>=4.4.3->notebook) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.5,>=4.4.3->notebook) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.5,>=4.4.3->notebook) (0.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (9.2.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (7.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (4.3.7)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (310)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (3.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (3.0.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.21.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.13.2)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->notebook) (2.9.0.20241206)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab<4.5,>=4.4.3->notebook) (0.2.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.5.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.2.5)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (3.5)\n",
      "Requirement already satisfied: pillow>=10.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (11.2.1)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (2025.5.26)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (25.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-image) (0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn-intelex in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2025.5.0)\n",
      "Requirement already satisfied: daal==2025.5.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn-intelex) (2025.5.0)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn-intelex) (2.2.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn-intelex) (1.6.1)\n",
      "Requirement already satisfied: tbb==2022.* in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from daal==2025.5.0->scikit-learn-intelex) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tbb==2022.*->daal==2025.5.0->scikit-learn-intelex) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (2.2.5)\n",
      "Requirement already satisfied: rich in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (3.13.0)\n",
      "Requirement already satisfied: optree in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (0.16.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optree->keras) (4.13.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich->keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aless\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a72024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dense, LSTM, Embedding, SpatialDropout1D\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tf_keras \u001b[38;5;28;01mas\u001b[39;00m _tf_keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\_tf_keras\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m callbacks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\activations\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get \u001b[38;5;28;01mas\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize \u001b[38;5;28;01mas\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m KerasVariable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[32m      7\u001b[39m BOOL_TYPES = (\u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m      8\u001b[39m INT_TYPES = (\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint16\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVariable\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_dataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\audio_dataset_utils.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultiprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPool\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_paths\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flatten\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optree.available:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dmtree.available:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aless\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DictWrapper\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import openai          \n",
    "import transformers    \n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.tsa.stattools as stt\n",
    "import statsmodels.tsa.holtwinters as smt\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "import statsmodels.tsa.arima.model as smt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5417758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Ensuring OpenAI API Key is retrieved from .env file\n",
    "load_dotenv()\n",
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36c14a",
   "metadata": {},
   "source": [
    "## S&P 500 Index Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838db252",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Market data\n",
    "### Literature lays out data suggestions and pipeline implementation\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2015-01-01\", end=\"2025-01-01\")\n",
    "\n",
    "# DataFrame with Date index and columns Open, High, Low, Close (used for Returns), Volume, Adj Close\n",
    "sp500['Return'] = sp500['Close'].pct_change()\n",
    "sp500 = sp500.reset_index() # Date column for merging\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date']).dt.date \n",
    "\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date']).dt.strftime('%Y-%m-%d') # Converting Date to string format for merging\n",
    "\n",
    "# Fetching macro data with VIX:\n",
    "vix = yf.download(\"^VIX\", start=\"2015-01-01\", end=\"2025-01-01\")[['Close']]\n",
    "vix.rename(columns={'Close':'VIX'}, inplace=True)\n",
    "vix = vix.reset_index()\n",
    "vix['Date'] = pd.to_datetime(vix['Date']).dt.date\n",
    "vix['Date'] = pd.to_datetime(vix['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merging the two DataFrames on 'Date'\n",
    "merged_data = pd.merge(sp500, vix, on='Date', how='inner')\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data.set_index('Date', inplace=False)\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "merged_data.to_csv('sp500_vix_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0762",
   "metadata": {},
   "source": [
    "## New York Times Article Summary Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66202ff",
   "metadata": {},
   "source": [
    "### General Article Scriping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cb64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 2015-01 … "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept 938 / 6906 docs\n",
      "Fetching 2015-02 … kept 882 / 6437 docs\n",
      "Fetching 2015-03 … kept 871 / 6975 docs\n",
      "Fetching 2015-04 … kept 862 / 6542 docs\n",
      "Fetching 2015-05 … kept 776 / 6634 docs\n",
      "Fetching 2015-06 … kept 822 / 6947 docs\n",
      "Fetching 2015-07 … kept 862 / 6758 docs\n",
      "Fetching 2015-08 … kept 678 / 6086 docs\n",
      "Fetching 2015-09 … kept 746 / 6990 docs\n",
      "Fetching 2015-10 … kept 931 / 7365 docs\n",
      "Fetching 2015-11 … kept 791 / 6241 docs\n",
      "Fetching 2015-12 … kept 744 / 6371 docs\n",
      "Fetching 2016-01 … kept 764 / 6265 docs\n",
      "Fetching 2016-02 … kept 732 / 6448 docs\n",
      "Fetching 2016-03 … kept 776 / 6487 docs\n",
      "Fetching 2016-04 … kept 752 / 5927 docs\n",
      "Fetching 2016-05 … kept 689 / 5921 docs\n",
      "Fetching 2016-06 … kept 688 / 5920 docs\n",
      "Fetching 2016-07 … kept 645 / 5594 docs\n",
      "Fetching 2016-08 … kept 530 / 5224 docs\n",
      "Fetching 2016-09 … kept 606 / 5894 docs\n",
      "Fetching 2016-10 … kept 593 / 5511 docs\n",
      "Fetching 2016-11 … kept 568 / 5331 docs\n",
      "Fetching 2016-12 … kept 541 / 4967 docs\n",
      "Fetching 2017-01 … kept 514 / 5129 docs\n",
      "Fetching 2017-02 … kept 478 / 4935 docs\n",
      "Fetching 2017-03 … kept 591 / 5546 docs\n",
      "Fetching 2017-04 … kept 575 / 4878 docs\n",
      "Fetching 2017-05 … kept 598 / 5384 docs\n",
      "Fetching 2017-06 … kept 574 / 5300 docs\n",
      "Fetching 2017-07 … kept 519 / 4757 docs\n",
      "Fetching 2017-08 … kept 485 / 4854 docs\n",
      "Fetching 2017-09 … kept 432 / 5033 docs\n",
      "Fetching 2017-10 … kept 489 / 5214 docs\n",
      "Fetching 2017-11 … kept 517 / 4834 docs\n",
      "Fetching 2017-12 … kept 408 / 4560 docs\n",
      "Fetching 2018-01 … kept 458 / 4757 docs\n",
      "Fetching 2018-02 … kept 405 / 4573 docs\n",
      "Fetching 2018-03 … kept 504 / 5034 docs\n",
      "Fetching 2018-04 … kept 426 / 4655 docs\n",
      "Fetching 2018-05 … kept 423 / 5127 docs\n",
      "Fetching 2018-06 … kept 445 / 4988 docs\n",
      "Fetching 2018-07 … kept 416 / 4444 docs\n",
      "Fetching 2018-08 … kept 411 / 6381 docs\n",
      "Fetching 2018-09 … kept 363 / 4722 docs\n",
      "Fetching 2018-10 … kept 402 / 5146 docs\n",
      "Fetching 2018-11 … kept 395 / 4829 docs\n",
      "Fetching 2018-12 … kept 337 / 4193 docs\n",
      "Fetching 2019-01 … kept 353 / 4482 docs\n",
      "Fetching 2019-02 … kept 293 / 4122 docs\n",
      "Fetching 2019-03 … kept 390 / 4690 docs\n",
      "Fetching 2019-04 … kept 363 / 4523 docs\n",
      "Fetching 2019-05 … kept 358 / 4737 docs\n",
      "Fetching 2019-06 … kept 326 / 4496 docs\n",
      "Fetching 2019-07 … kept 360 / 4287 docs\n",
      "Fetching 2019-08 … kept 335 / 4109 docs\n",
      "Fetching 2019-09 … kept 352 / 4418 docs\n",
      "Fetching 2019-10 … kept 398 / 4989 docs\n",
      "Fetching 2019-11 … kept 349 / 4420 docs\n",
      "Fetching 2019-12 … kept 290 / 3985 docs\n",
      "Fetching 2020-01 … kept 371 / 4480 docs\n",
      "Fetching 2020-02 … kept 334 / 4240 docs\n",
      "Fetching 2020-03 … kept 449 / 4883 docs\n",
      "Fetching 2020-04 … kept 413 / 5019 docs\n",
      "Fetching 2020-05 … kept 405 / 4347 docs\n",
      "Fetching 2020-06 … kept 413 / 4492 docs\n",
      "Fetching 2020-07 … kept 457 / 4459 docs\n",
      "Fetching 2020-08 … kept 365 / 4439 docs\n",
      "Fetching 2020-09 … kept 371 / 4609 docs\n",
      "Fetching 2020-10 … kept 439 / 5257 docs\n",
      "Fetching 2020-11 … kept 379 / 5114 docs\n",
      "Fetching 2020-12 … kept 382 / 4154 docs\n",
      "Fetching 2021-01 … kept 403 / 7001 docs\n",
      "Fetching 2021-02 … kept 400 / 4260 docs\n",
      "Fetching 2021-03 … kept 466 / 4786 docs\n",
      "Fetching 2021-04 … kept 447 / 4592 docs\n",
      "Fetching 2021-05 … kept 416 / 4265 docs\n",
      "Fetching 2021-06 … kept 394 / 4260 docs\n",
      "Fetching 2021-07 … kept 402 / 4467 docs\n",
      "Fetching 2021-08 … kept 356 / 4223 docs\n",
      "Fetching 2021-09 … kept 383 / 4482 docs\n",
      "Fetching 2021-10 … kept 440 / 4254 docs\n",
      "Fetching 2021-11 … kept 386 / 4006 docs\n",
      "Fetching 2021-12 … kept 388 / 3933 docs\n",
      "Fetching 2022-01 … kept 412 / 3799 docs\n",
      "Fetching 2022-02 … kept 397 / 4059 docs\n",
      "Fetching 2022-03 … kept 446 / 4310 docs\n",
      "Fetching 2022-04 … kept 386 / 3934 docs\n",
      "Fetching 2022-05 … kept 363 / 4179 docs\n",
      "Fetching 2022-06 … kept 390 / 4374 docs\n",
      "Fetching 2022-07 … kept 387 / 3835 docs\n",
      "Fetching 2022-08 … kept 375 / 3983 docs\n",
      "Fetching 2022-09 … kept 372 / 4087 docs\n",
      "Fetching 2022-10 … kept 380 / 3857 docs\n",
      "Fetching 2022-11 … kept 357 / 4742 docs\n",
      "Fetching 2022-12 … kept 327 / 3548 docs\n",
      "Fetching 2023-01 … kept 327 / 3423 docs\n",
      "Fetching 2023-02 … kept 325 / 3260 docs\n",
      "Fetching 2023-03 … kept 380 / 5769 docs\n",
      "Fetching 2023-04 … kept 317 / 3718 docs\n",
      "Fetching 2023-05 … kept 350 / 3881 docs\n",
      "Fetching 2023-06 … kept 321 / 3788 docs\n",
      "Fetching 2023-07 … kept 339 / 3548 docs\n",
      "Fetching 2023-08 … kept 304 / 3668 docs\n",
      "Fetching 2023-09 … kept 351 / 3823 docs\n",
      "Fetching 2023-10 … kept 349 / 3906 docs\n",
      "Fetching 2023-11 … kept 336 / 3734 docs\n",
      "Fetching 2023-12 … kept 306 / 3525 docs\n",
      "Fetching 2024-01 … kept 331 / 3785 docs\n",
      "Fetching 2024-02 … kept 308 / 3791 docs\n",
      "Fetching 2024-03 … kept 343 / 4242 docs\n",
      "Fetching 2024-04 … kept 336 / 3954 docs\n",
      "Fetching 2024-05 … kept 323 / 4268 docs\n",
      "Fetching 2024-06 … kept 291 / 3847 docs\n",
      "Fetching 2024-07 … kept 342 / 4134 docs\n",
      "Fetching 2024-08 … kept 323 / 4047 docs\n",
      "Fetching 2024-09 … kept 316 / 4054 docs\n",
      "Fetching 2024-10 … kept 320 / 4306 docs\n",
      "Fetching 2024-11 … kept 300 / 4602 docs\n",
      "Fetching 2024-12 … kept 334 / 3666 docs\n",
      "Fetching 2025-01 … kept 370 / 4142 docs\n",
      "✅ All months processed. CSV complete!\n"
     ]
    }
   ],
   "source": [
    "### NYT Article Summary Data\n",
    "import os, time, json, sys\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY = os.getenv(\"NYT_API_KEY\", \"your_real_key\")\n",
    "\n",
    "START_DATE  = date(2015, 1, 1)\n",
    "END_DATE    = date(2025, 1, 1)\n",
    "CSV_PATH    = \"nyt_business_archive.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC   = 60 / REQS_PER_MIN        # The 12s pause forces code to stay within the API rate limit of 5-calls per minute\n",
    "CHECKPOINT  = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "def archive_month(year: int, month: int, max_retries: int = 5) -> Dict: # Wrapper handles 429 and retries\n",
    "    \"\"\"Fetch one month from NYT Archive API with rate-limit back-off.\"\"\"\n",
    "    url    = ARCHIVE_URL.format(year=year, month=month)\n",
    "    params = {\"api-key\": NYT_API_KEY}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        resp = requests.get(url, params=params, timeout=60)\n",
    "        if resp.status_code == 429:                # Too Many Requests\n",
    "            wait = int(resp.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"429 received — sleeping {wait}s and retrying …\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    raise RuntimeError(f\"Failed to fetch {year}-{month:02} after {max_retries} retries\")\n",
    "\n",
    "def iter_months(start: date, end: date):\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        current = date(y, m, 1)\n",
    "        if current > end.replace(day=1):\n",
    "            break\n",
    "        yield current\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(d: date):\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "def append_rows(rows: List[Dict]):\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "def is_business(doc: Dict) -> bool:\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "def main():\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} — {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)      \n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d} docs\")\n",
    "        time.sleep(SLEEP_SEC)                              \n",
    "\n",
    "    print(\"All months processed. CSV downloaded\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted — progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43b836",
   "metadata": {},
   "source": [
    "### Aggregated Sector Mapping Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2d743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No header problem detected — nothing changed.\n",
      "Resuming after 2025-01-01 – 0 months left.\n",
      "✅ NYT crawl complete.\n",
      "✅ Aggregated file written: nyt_aggregated_data.csv\n"
     ]
    }
   ],
   "source": [
    "### Aggregate Sector Mapping for NYT Article Summaries\n",
    "import os, time, json, sys, re\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "path = \"nyt_business_archive.csv\"\n",
    "\n",
    "df = pd.read_csv(path, header=None)  # no header to see all columns\n",
    "max_cols = df.shape[1]\n",
    "\n",
    "if max_cols == 5 and \"Summary\" not in pd.read_csv(path, nrows=0).columns:\n",
    "    df.columns = [\"Date\", \"Headline\", \"Summary\", \"Section\", \"URL\"]\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"✔ Added missing 'Summary' header and rewrote file.\")\n",
    "else:\n",
    "    print(\"No header problem detected — nothing changed.\")\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY  = os.getenv(\"NYT_API_KEY\", \"your_key\")\n",
    "START_DATE   = date(2015, 1, 1)\n",
    "END_DATE     = date(2025, 1, 1)\n",
    "CSV_PATH     = \"nyt_business_archive.csv\"\n",
    "AGG_PATH     = \"nyt_aggregated_data.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC    = 60 / REQS_PER_MIN       # 12 s → 5 req/min\n",
    "CHECKPOINT   = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL  = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "def archive_month(year: int, month: int, retries=5) -> Dict: # NYT API call with 429 back-off to handle rate limits\n",
    "    url, params = ARCHIVE_URL.format(year=year, month=month), {\"api-key\": NYT_API_KEY}\n",
    "    for _ in range(retries):\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        if r.status_code == 429:                       # Too Many Requests\n",
    "            wait = int(r.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"⚠429 – sleeping {wait}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    raise RuntimeError(f\"Failed after {retries} retries: {year}-{month:02}\")\n",
    "\n",
    "def iter_months(start: date, end: date): # Generator = iterates over months in a date range\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        first = date(y, m, 1)\n",
    "        if first > end.replace(day=1):\n",
    "            break\n",
    "        yield first\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "def load_checkpoint(): # Setting up checkpoint management for resuming crawls\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "def save_checkpoint(d: date): # Saving the last processed month to a checkpoint file\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "def append_rows(rows: List[Dict]): # Rows to the CSV file, creating it if it doesn't exist\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "def is_business(doc: Dict) -> bool: # Business filter\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "def crawl_nyt_archive(): # Main crawling function to fetch NYT archive data\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} – {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d}\")\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "    print(\"NYT crawl complete.\")\n",
    "\n",
    "# Sector classification & aggregation\n",
    "SECTOR_MAP: dict[str, list[str]] = {\n",
    "    # Information Tech Secotr:\n",
    "    \"Software & IT Services\": [\n",
    "        \"software\", \"saas\", \"cloud\", \"it services\", \"consulting\",\n",
    "        \"microsoft\", \"adobe\", \"oracle\", \"sap\", \"salesforce\", \"servicenow\",\n",
    "        \"workday\", \"vmware\", \"accenture\", \"infosys\", \"tcs\", \"capgemini\",\n",
    "    ],\n",
    "    \"Hardware & Devices\": [\n",
    "        \"hardware\", \"pc\", \"laptop\", \"smartphone\", \"iphone\", \"ipad\",\n",
    "        \"dell\", \"hp\", \"lenovo\", \"asus\", \"acer\", \"logitech\",\n",
    "    ],\n",
    "    \"Semiconductors\": [\n",
    "        \"chip\", \"chips\", \"semiconductor\", \"fab\", \"foundry\",\n",
    "        \"intel\", \"amd\", \"nvidia\", \"qualcomm\", \"tsmc\", \"broadcom\",\n",
    "        \"micron\", \"arm holdings\", \"sk hynix\",\n",
    "    ],\n",
    "    \"Internet & Social Media\": [\n",
    "        \"google\", \"alphabet\", \"youtube\", \"search engine\",\n",
    "        \"meta\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"twitter\", \"x corp\", \"snapchat\", \"tiktok\", \"reddit\",\n",
    "        \"linkedin\", \"pinterest\", \"social media\",\n",
    "    ],\n",
    "    # Communication & Media Sector:\n",
    "    \"Telecommunications\": [\n",
    "        \"telecom\", \"5g\", \"wireless\", \"broadband\",\n",
    "        \"verizon\", \"at&t\", \"t-mobile\", \"comcast\", \"charter\",\n",
    "        \"vodafone\", \"telefonica\", \"bt group\", \"rogers\", \"singtel\",\n",
    "    ],\n",
    "    \"Media & Entertainment\": [\n",
    "        \"media\", \"streaming\", \"disney\", \"espn\", \"hulu\",\n",
    "        \"netflix\", \"warner bros\", \"hbo\", \"paramount\", \"peacock\",\n",
    "        \"sony pictures\", \"universal\", \"box office\", \"cinema\",\n",
    "    ],\n",
    "    # Consumer Sector:\n",
    "    \"Retail & E-Commerce\": [\n",
    "        \"retail\", \"e-commerce\", \"amazon\", \"alibaba\", \"shopify\",\n",
    "        \"ebay\", \"etsy\", \"walmart\", \"target\", \"costco\", \"kroger\",\n",
    "        \"best buy\", \"flipkart\", \"mercado libre\",\n",
    "    ],\n",
    "    \"Consumer Goods & Apparel\": [\n",
    "        \"nike\", \"adidas\", \"lululemon\", \"puma\", \"under armour\",\n",
    "        \"apparel\", \"footwear\", \"luxury\", \"lvmh\", \"gucci\", \"burberry\",\n",
    "        \"rolex\", \"hermes\", \"tapestry\",\n",
    "    ],\n",
    "    \"Food & Beverage\": [\n",
    "        \"food\", \"beverage\", \"coca-cola\", \"pepsico\", \"nestlé\",\n",
    "        \"restaurant\", \"fast food\", \"mcdonald\", \"starbucks\",\n",
    "        \"yum brands\", \"kfc\", \"pizza hut\", \"chipotle\",\n",
    "        \"kraft\", \"general mills\", \"heinz\", \"tyson foods\",\n",
    "    ],\n",
    "    \"Hospitality & Leisure\": [\n",
    "        \"hotel\", \"marriott\", \"hilton\", \"hyatt\", \"airbnb\",\n",
    "        \"booking.com\", \"expedia\", \"travel\", \"cruise\", \"carnival\",\n",
    "        \"royal caribbean\", \"las vegas sands\", \"mgm resorts\",\n",
    "    ],\n",
    "    \"Automotive\": [\n",
    "        \"automotive\", \"auto\", \"car\", \"vehicle\", \"ev\",\n",
    "        \"tesla\", \"general motors\", \"ford\", \"stellantis\",\n",
    "        \"volkswagen\", \"toyota\", \"nissan\", \"bmw\", \"mercedes\",\n",
    "        \"hyundai\", \"kia\", \"rivian\", \"lucid\",\n",
    "    ],\n",
    "    # Healthcare Sector:\n",
    "    \"Pharmaceuticals\": [\n",
    "        \"pharma\", \"drug\", \"medicine\", \"vaccine\", \"fda\",\n",
    "        \"pfizer\", \"moderna\", \"johnson & johnson\", \"merck\",\n",
    "        \"novartis\", \"roche\", \"astrazeneca\", \"bayer\", \"gsk\",\n",
    "    ],\n",
    "    \"Biotechnology\": [\n",
    "        \"biotech\", \"gene therapy\", \"crispr\", \"genomics\",\n",
    "        \"illumina\", \"gilead\", \"amgen\", \"biogen\", \"regeneron\",\n",
    "        \"vertex\", \"bluebird bio\",\n",
    "    ],\n",
    "    \"Medical Devices & Services\": [\n",
    "        \"medical device\", \"medtech\", \"diagnostics\", \"surgical\",\n",
    "        \"medtronic\", \"boston scientific\", \"abbott\", \"stryker\",\n",
    "        \"philips healthcare\", \"siemens healthineers\", \"cardinal health\",\n",
    "        \"hospital\", \"clinic\", \"healthcare services\",\n",
    "    ],\n",
    "    # Energy & Utilities Sector:\n",
    "    \"Oil & Gas\": [\n",
    "        \"oil\", \"gas\", \"petroleum\", \"upstream\", \"downstream\",\n",
    "        \"exxon\", \"chevron\", \"bp\", \"shell\", \"totalenergies\",\n",
    "        \"conocophillips\", \"aramco\", \"occidental\", \"slb\",\n",
    "    ],\n",
    "    \"Renewables & Clean Energy\": [\n",
    "        \"renewable\", \"solar\", \"wind\", \"geothermal\", \"hydro\",\n",
    "        \"clean energy\", \"green energy\", \"next era\", \"sunpower\",\n",
    "        \"first solar\", \"enphase\", \"vestas\", \"siemens gamesa\",\n",
    "        \"hydrogen\", \"electrolyzer\", \"fuel cell\",\n",
    "    ],\n",
    "    \"Utilities\": [\n",
    "        \"utility\", \"power grid\", \"electricity\", \"water utility\",\n",
    "        \"natural gas utility\", \"duke energy\", \"southern company\",\n",
    "        \"dominion\", \"pg&e\", \"national grid\", \"aes\",\n",
    "    ],\n",
    "    # Financials Sector:\n",
    "    \"Banks\": [\n",
    "        \"bank\", \"commercial bank\", \"jpmorgan\", \"bank of america\",\n",
    "        \"citigroup\", \"wells fargo\", \"goldman sachs\", \"morgan stanley\",\n",
    "        \"u.s. bancorp\", \"hsbc\", \"barclays\", \"santander\", \"dbs\",\n",
    "    ],\n",
    "    \"Investment & Asset Management\": [\n",
    "        \"asset manager\", \"blackrock\", \"vanguard\", \"fidelity\",\n",
    "        \"state street\", \"schwab\", \"hedge fund\", \"private equity\",\n",
    "        \"kkr\", \"carried interest\", \"mutual fund\", \"etf\",\n",
    "        \"sovereign wealth fund\",\n",
    "    ],\n",
    "    \"Insurance\": [\n",
    "        \"insurance\", \"insurer\", \"aig\", \"allstate\", \"progressive\",\n",
    "        \"metlife\", \"prudential\", \"chubb\", \"berkshire hathaway insurance\",\n",
    "        \"reinsurance\", \"lloyd's\", \"actuarial\",\n",
    "    ],\n",
    "    \"Fintech & Payments\": [\n",
    "        \"fintech\", \"payment\", \"visa\", \"mastercard\",\n",
    "        \"american express\", \"paypal\", \"block inc\", \"square\",\n",
    "        \"stripe\", \"sofi\", \"robinhood\", \"buy now pay later\",\n",
    "        \"klarna\", \"affirm\", \"ant financial\",\n",
    "    ],\n",
    "    \"Cryptocurrency & Blockchain\": [\n",
    "        \"bitcoin\", \"ethereum\", \"crypto\", \"blockchain\",\n",
    "        \"coinbase\", \"binance\", \"defi\", \"nft\",\n",
    "        \"stablecoin\", \"mining rig\", \"hashrate\",\n",
    "    ],\n",
    "    # Industrial Sector:\n",
    "    \"Aerospace & Defense\": [\n",
    "        \"aerospace\", \"defense\", \"boeing\", \"airbus\", \"northrop\",\n",
    "        \"lockheed martin\", \"raytheon\", \"bae systems\", \"general dynamics\",\n",
    "        \"drones\", \"satellite\", \"nasa contract\",\n",
    "    ],\n",
    "    \"Transportation & Logistics\": [\n",
    "        \"shipping\", \"freight\", \"logistics\", \"supply chain\",\n",
    "        \"fedex\", \"ups\", \"dhl\", \"maersk\", \"csx\", \"union pacific\",\n",
    "        \"delta airlines\", \"american airlines\", \"united airlines\",\n",
    "        \"railroad\", \"port congestion\",\n",
    "    ],\n",
    "    \"Manufacturing & Machinery\": [\n",
    "        \"manufacturing\", \"factory\", \"industrial\", \"caterpillar\",\n",
    "        \"3m\", \"general electric\", \"siemens\", \"honeywell\", \"emerson\",\n",
    "        \"robotics\", \"automation\", \"abb\", \"fanuc\",\n",
    "    ],\n",
    "    \"Construction & Engineering\": [\n",
    "        \"construction\", \"engineering\", \"infrastructure\",\n",
    "        \"bechtel\", \"fluor\", \"jacobs\", \"skanska\", \"kiewit\",\n",
    "        \"turner construction\", \"architect\", \"building materials\",\n",
    "    ],\n",
    "    \"Chemicals & Specialty Materials\": [\n",
    "        \"chemical\", \"chemicals\", \"specialty chemical\",\n",
    "        \"dupont\", \"dow\", \"basf\", \"lyondellbasell\", \"air products\",\n",
    "        \"eastman\", \"evonik\", \"synthetic rubber\", \"petrochemical\",\n",
    "    ],\n",
    "    \"Metals & Mining\": [\n",
    "        \"mining\", \"metal\", \"steel\", \"aluminum\", \"copper\",\n",
    "        \"iron ore\", \"rio tinto\", \"bhp\", \"vale\", \"newmont\",\n",
    "        \"glencore\", \"lithium\", \"nickel\", \"rare earth\",\n",
    "    ],\n",
    "    \"Agriculture\": [\n",
    "        \"agriculture\", \"farming\", \"crop\", \"soybean\", \"corn\",\n",
    "        \"wheat\", \"cargill\", \"archer daniels midland\", \"bunge\",\n",
    "        \"deere\", \"monsanto\", \"fertilizer\", \"nutrien\", \"potash\",\n",
    "    ],\n",
    "    # Real Estate Sector:\n",
    "    \"Real Estate\": [\n",
    "        \"real estate\", \"realtor\", \"reit\", \"property\", \"mortgage\",\n",
    "        \"office vacancy\", \"housing market\", \"zillow\", \"redfin\",\n",
    "        \"wework\", \"commercial property\", \"residential property\",\n",
    "        \"industrial park\", \"logistics park\",\n",
    "    ],\n",
    "    # ESG / Government / Education Sectors:\n",
    "    \"Environmental & ESG\": [\n",
    "        \"esg\", \"sustainability\", \"carbon\", \"emissions\",\n",
    "        \"carbon credit\", \"offset\", \"green bond\", \"climate risk\",\n",
    "        \"cop28\", \"environmental regulation\",\n",
    "    ],\n",
    "    \"Government & Policy\": [\n",
    "        \"government\", \"regulation\", \"legislation\", \"policy\",\n",
    "        \"federal reserve\", \"congress\", \"white house\",\n",
    "        \"eu commission\", \"trade tariff\", \"sanction\", \"geopolitics\",\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"education\", \"edtech\", \"university\", \"college\", \"school\",\n",
    "        \"coursera\", \"edx\", \"udemy\", \"chegg\", \"student loan\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def assign_sector(text: str) -> str: # Function to assign sectors based on keywords in the summary text\n",
    "    text_low = text.lower()\n",
    "    for sector, keywords in SECTOR_MAP.items():\n",
    "        if any(kw in text_low for kw in keywords):\n",
    "            return sector\n",
    "    return \"General\"\n",
    "  \n",
    "def aggregate_nyt(df: pd.DataFrame) -> pd.DataFrame: # Aggregation function to combine headlines and summaries by Date and Sector\n",
    "    \"\"\"Combine all Business headlines/summaries into one row per Date × Sector.\"\"\"\n",
    "    return (\n",
    "        df.groupby([\"Date\", \"Sector\"])\n",
    "          .agg({\n",
    "              \"Headline\": lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "              \"Summary\":  lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "          })\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "# Main function to run the entire process\n",
    "def main():\n",
    "    crawl_nyt_archive() # Crawl or resume NYT archive\n",
    "\n",
    "    nyt_df = pd.read_csv(CSV_PATH) # Loading full Business CSV\n",
    "    nyt_df[\"Headline\"] = nyt_df[\"Headline\"].astype(str)\n",
    "    nyt_df[\"Summary\"] = nyt_df[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "    nyt_df[\"Sector\"] = nyt_df[\"Summary\"].fillna(\"\").apply(assign_sector) # Assigning sectors\n",
    "\n",
    "    nyt_aggregated = aggregate_nyt(nyt_df) # Aggregate is saved\n",
    "    nyt_aggregated.to_csv(AGG_PATH, index=False)\n",
    "    print(f\"Aggregated file written: {AGG_PATH}\")\n",
    "  \n",
    "    if \"news_df\" in globals(): # Reuters clean-up if news_df already exists\n",
    "        news_df[\"Article\"] = (\n",
    "            news_df[\"Article\"]\n",
    "            .str.replace(r\"By .*? \\|\", \"\", regex=True)\n",
    "            .str.replace(r\"\\n+\", \" \", regex=True)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        print(\"Reuters news_df cleaned.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted – progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20aa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date                 Sector  \\\n",
      "0  2015-01-01            Agriculture   \n",
      "1  2015-01-01             Automotive   \n",
      "2  2015-01-01                General   \n",
      "3  2015-01-01     Hardware & Devices   \n",
      "4  2015-01-01  Media & Entertainment   \n",
      "\n",
      "                                            Headline  \\\n",
      "0  By ‘Editing’ Plant Genes, Companies Avoid Regu...   \n",
      "1  Government Spending, Edging Up, Is a Stimulus ...   \n",
      "2  Standouts in Tech: Drones, Virtual Reality, In...   \n",
      "3                Carmakers Take a Hint From Tablets    \n",
      "4      Digital Tax Increase to Take Effect in Europe   \n",
      "\n",
      "                                             Summary  \n",
      "0  Critics of bioengineered crops are concerned t...  \n",
      "1  State and local governments are spending on pr...  \n",
      "2  Farhad Manjoo picks four products from 2014 th...  \n",
      "3  By adopting tabletlike, touch-screen systems, ...  \n",
      "4  New rules are coming into force for services l...  \n",
      "         Date               Close                High                 Low  \\\n",
      "0         NaN               ^GSPC               ^GSPC               ^GSPC   \n",
      "1  2015-01-05  2020.5799560546875    2054.43994140625  2017.3399658203125   \n",
      "2  2015-01-06  2002.6099853515625             2030.25    1992.43994140625   \n",
      "3  2015-01-07  2025.9000244140625  2029.6099853515625   2005.550048828125   \n",
      "4  2015-01-08   2062.139892578125      2064.080078125  2030.6099853515625   \n",
      "\n",
      "                 Open      Volume    Return                 VIX  \n",
      "0               ^GSPC       ^GSPC       NaN                ^VIX  \n",
      "1    2054.43994140625  3799120000 -0.018278  19.920000076293945  \n",
      "2  2022.1500244140625  4460110000 -0.008893    21.1200008392334  \n",
      "3   2005.550048828125  3805480000  0.011630  19.309999465942383  \n",
      "4  2030.6099853515625  3934010000  0.017888  17.010000228881836  \n"
     ]
    }
   ],
   "source": [
    "nyt_aggregated = pd.read_csv(\"nyt_aggregated_data.csv\") # Printing the first few rows of the NYT scrapped aggregate data file\n",
    "print(nyt_aggregated.head())\n",
    "\n",
    "merged_data = pd.read_csv(\"sp500_vix_data.csv\") # Printing the first few rows of the merged S&P 500 and VIX data\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cd63b",
   "metadata": {},
   "source": [
    "## Senitment Analysis Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5b30d",
   "metadata": {},
   "source": [
    "### Unified FinBERT and GPT-4 Fall Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7689f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: FinBERT sentiment file found. Loading and skipping FinBERT step.\n",
      "Checkpoint: GPT-4 fallback file found. Loading and skipping fallback step.\n",
      "Total pipeline runtime: 0.96 seconds.\n",
      "Checkpoint: Final merged LSTM-ready data saved to derived\\final_merged_for_lstm.csv\n",
      "All CSVs saved to: derived/\n",
      "Output files:\n",
      " - nyt_with_finbert_sentiment.csv\n",
      " - nyt_with_gpt4_fallback_sentiment.csv\n",
      " - daily_sentiment_aggregate.csv\n",
      " - final_merged_for_lstm.csv\n"
     ]
    }
   ],
   "source": [
    "### Hybrid: FinBERT + GPT-4 Sentiment Analysis Pipeline\n",
    "### Follows implementation by ProsusAI https://github.com/ProsusAI/finBERT\n",
    "#  FinBERT gives primary sentiment score (pos‑prob − neg‑prob)\n",
    "#  The FinBERT-onyl score is extracted for benchmarking\n",
    "#  GPT‑4 called upon for fallback when FinBERT is effectively\n",
    "#  Fallback activated when FinBERT score is neutral (|score| < GPT4_CONFIDENCE_THRESHOLD)\n",
    "#  Carried out with Python 3.13\n",
    "\n",
    "import os\n",
    "import time \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "USE_GPT4_FALLBACK = True\n",
    "GPT4_CONFIDENCE_THRESHOLD = 0.05                     \n",
    "OPENAI_MODEL = \"gpt-4o-mini\" # GPT model choice supported by literature\n",
    "OPENAI_MAX_TOKENS = 10\n",
    "\n",
    "DERIVED_PATH = \"derived\"\n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "GPT4_CHECKPOINT_EVERY = 250   # save after every 250 GPT-4 calls\n",
    "\n",
    "if USE_GPT4_FALLBACK: # Add API key to .env in same directory\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise EnvironmentError(\"OPENAI_API_KEY not set.\")\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df = pd.read_csv(\"nyt_aggregated_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df.rename(columns={\"Summary\": \"summary\", \"Headline\": \"headline\", \"Sector\": \"sector\"}, inplace=True)\n",
    "\n",
    "trading_days = set(market_df[\"Date\"].dt.normalize()) # Aligning news dates with trading calendar\n",
    "news_df = news_df[news_df[\"Date\"].dt.normalize().isin(trading_days)].reset_index(drop=True)\n",
    "\n",
    "FINBERT_MODEL = \"ProsusAI/finbert\" # FinBERT model - ProsusAI\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)\n",
    "finbert.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def finbert_score(texts: List[str], batch_size: int = 32) -> Tuple[List[float], List[np.ndarray]]:\n",
    "    scores, probs_all = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        out = finbert(**enc)\n",
    "        probs = torch.nn.functional.softmax(out.logits, dim=1).cpu().numpy()\n",
    "        batch_scores = probs[:,1] - probs[:,2]\n",
    "        scores.extend(batch_scores.tolist())\n",
    "        probs_all.extend(probs)\n",
    "    return scores, probs_all\n",
    "\n",
    "def gpt4_sentiment_single(text):\n",
    "    \"\"\"Call GPT-4 for sentiment fallback, return -1, 0, or 1.\"\"\"        \n",
    "    system_msg = \"You are a financial news analyst.\"\n",
    "    user_msg = (\n",
    "        \"Classify the sentiment of the given NY Times news article summary {summary}, which is closely related to the {sector} industry, as positive for buy, negative for sell, or neutral for hold position, for the US Stock market and provide the probability values for your classification.\"\n",
    "        \"Answer with just one word.\\n\\n\"\n",
    "        f\"Summary: \\\"{text}\\\"\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\", \"content\": user_msg}],\n",
    "            max_tokens=OPENAI_MAX_TOKENS,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        ans = response.choices[0].message.content.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(\"GPT-4 error (treated as neutral):\", e)\n",
    "        return 0.0\n",
    "    if ans.startswith(\"pos\"):\n",
    "        return 1.0\n",
    "    if ans.startswith(\"neg\"):\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "\n",
    "def add_sentiment(news_df, use_gpt4=False):\n",
    "    finbert_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\") # Checkpoint 1: If FinBERT file exists, skip FinBERT and load\n",
    "    if os.path.exists(finbert_path):\n",
    "        print(\"Checkpoint: FinBERT sentiment file found. Loading and skipping FinBERT step.\")\n",
    "        news_df = pd.read_csv(finbert_path, parse_dates=[\"Date\"])\n",
    "    else:\n",
    "        print(\"Scoring FinBERT…\")\n",
    "        start_finbert = time.time()\n",
    "        fin_scores, fin_probs = finbert_score(news_df[\"summary\"].tolist())\n",
    "        end_finbert = time.time()\n",
    "        news_df[\"FinBERT_score\"] = fin_scores\n",
    "        news_df[\"FinBERT_prob_neu\"] = [p[0] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_pos\"] = [p[1] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_neg\"] = [p[2] for p in fin_probs]\n",
    "        news_df[\"FinBERT_confidence\"] = news_df[\"FinBERT_score\"].abs()\n",
    "        news_df[\"Sentiment\"] = news_df[\"FinBERT_score\"]  # Start with FinBERT\n",
    "        print(f\"FinBERT sentiment scored in {end_finbert - start_finbert:.2f} seconds.\")\n",
    "        news_df.to_csv(finbert_path, index=False)\n",
    "        print(f\"Checkpoint: FinBERT sentiment saved to {finbert_path}\")\n",
    "\n",
    "    num_gpt4 = 0\n",
    "    gpt4_path = os.path.join(DERIVED_PATH, \"nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "\n",
    "    if use_gpt4:\n",
    "        if os.path.exists(gpt4_path): # If checkpoint exists, load and skip fallback\n",
    "            print(\"Checkpoint: GPT-4 fallback file found. Loading and skipping fallback step.\")\n",
    "            news_df = pd.read_csv(gpt4_path, parse_dates=[\"Date\"])\n",
    "        else:\n",
    "            print(\"Running GPT-4 fallback…\")\n",
    "            low_conf_mask = news_df[\"FinBERT_confidence\"] < GPT4_CONFIDENCE_THRESHOLD\n",
    "            num_gpt4 = low_conf_mask.sum()\n",
    "            print(f\"News needing GPT-4 fallback: {num_gpt4} of {len(news_df)}\")\n",
    "            start_gpt4 = time.time()\n",
    "            checkpoint_counter = 0\n",
    "            for idx_num, idx in enumerate(news_df[low_conf_mask].index):\n",
    "                news_df.at[idx, \"Sentiment\"] = gpt4_sentiment_single(news_df.at[idx, \"summary\"])\n",
    "                checkpoint_counter += 1\n",
    "                if checkpoint_counter % GPT4_CHECKPOINT_EVERY == 0: # Intermediate checkpoint\n",
    "                    news_df.to_csv(gpt4_path, index=False)\n",
    "                    print(f\"Checkpoint: Saved GPT-4 fallback at {checkpoint_counter} / {num_gpt4} GPT-4 calls.\")\n",
    "            end_gpt4 = time.time()\n",
    "            print(f\"GPT-4 fallback completed in {end_gpt4 - start_gpt4:.2f} seconds.\")\n",
    "            news_df.to_csv(gpt4_path, index=False)\n",
    "            print(f\"Checkpoint: Final GPT-4 fallback saved to {gpt4_path}\")\n",
    "    else:\n",
    "        num_gpt4 = 0\n",
    "\n",
    "    return news_df, num_gpt4\n",
    "   \n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True) # Main Pipeline\n",
    "pipeline_start = time.time()\n",
    "news_df, num_gpt4 = add_sentiment(news_df, use_gpt4=USE_GPT4_FALLBACK)\n",
    "pipeline_end = time.time()\n",
    "print(f\"Total pipeline runtime: {pipeline_end - pipeline_start:.2f} seconds.\")\n",
    "\n",
    "# Calculaitng the daily sentiment aggregate\n",
    "sent_daily = (\n",
    "    news_df.groupby(news_df[\"Date\"].dt.normalize())[\"Sentiment\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sent_daily_path = os.path.join(DERIVED_PATH, \"daily_sentiment_aggregate.csv\")\n",
    "sent_daily.to_csv(sent_daily_path, index=False)\n",
    "\n",
    "# Merging with market data for modeling (LSTM)\n",
    "market_merge = pd.merge(\n",
    "    market_df, \n",
    "    sent_daily, \n",
    "    left_on=market_df[\"Date\"].dt.normalize(), \n",
    "    right_on=sent_daily[\"Date\"].dt.normalize(), \n",
    "    how=\"left\", \n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_path = os.path.join(DERIVED_PATH, \"final_merged_for_lstm.csv\")\n",
    "market_merge.to_csv(final_out_path, index=False)\n",
    "print(f\"Checkpoint: Final merged LSTM-ready data saved to {final_out_path}\")\n",
    "\n",
    "print(f\"All CSVs saved to: {DERIVED_PATH}/\")\n",
    "print(\"Output files:\")\n",
    "print(\" - nyt_with_finbert_sentiment.csv\")\n",
    "print(\" - nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "print(\" - daily_sentiment_aggregate.csv\")\n",
    "print(\" - final_merged_for_lstm.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f24325",
   "metadata": {},
   "source": [
    "### FinBERT-only Sentiment Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c20294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: FinBERT-only daily sentiment aggregate saved to derived\\daily_FinBERT_sentiment_aggregate.csv\n",
      "Checkpoint: Final merged FinBERT-only LSTM-ready data saved to derived\\final_merged_FinBERT_for_lstm.csv\n"
     ]
    }
   ],
   "source": [
    "finbert_only_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\") # Creating daily FinBERT-only sentiment aggregate for benchmarking\n",
    "finbert_df = pd.read_csv(finbert_only_path, parse_dates=[\"Date\"])\n",
    "\n",
    "daily_finbert_sent = (\n",
    "    finbert_df.groupby(finbert_df[\"Date\"].dt.normalize())[\"FinBERT_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "daily_finbert_sent_path = os.path.join(DERIVED_PATH, \"daily_FinBERT_sentiment_aggregate.csv\")\n",
    "daily_finbert_sent.to_csv(daily_finbert_sent_path, index=False)\n",
    "\n",
    "# Merging with market data for modeling (LSTM) FinBERT-only version, acts as a benchmark\n",
    "market_merge_finbert = pd.merge(\n",
    "    market_df,\n",
    "    daily_finbert_sent,\n",
    "    left_on=market_df[\"Date\"].dt.normalize(),\n",
    "    right_on=daily_finbert_sent[\"Date\"].dt.normalize(),\n",
    "    how=\"left\",\n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_finbert_path = os.path.join(DERIVED_PATH, \"final_merged_FinBERT_for_lstm.csv\")\n",
    "market_merge_finbert.to_csv(final_out_finbert_path, index=False)\n",
    "\n",
    "print(f\"Checkpoint: FinBERT-only daily sentiment aggregate saved to {daily_finbert_sent_path}\")\n",
    "print(f\"Checkpoint: Final merged FinBERT-only LSTM-ready data saved to {final_out_finbert_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6b23d",
   "metadata": {},
   "source": [
    "#Note: The OpenAI API usage as shown might need adaptation for actual GPT-4 (which might require using openai.ChatCompletion.create with messages in the new API format rather than the older Completion.create). But the idea stands – send the article text and get a sentiment label. The prompt here is simplified; in practice, one could include examples or a role specification for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f705a",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae406e",
   "metadata": {},
   "source": [
    "#### Implementation laid out by Jakob Aungiers https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4756c607",
   "metadata": {},
   "source": [
    "##### Conda virtual enviroment is implemented to run Python 3.11 throughout the remainder of this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1967a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!pip install numpy pandas scikit-learn tensorflow keras matplotlib\n",
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce71d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b459e0",
   "metadata": {},
   "source": [
    "### Hybrid (FinBERT+GPT) Sentiment LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning up columns\n",
    "\n",
    "for col in df.columns: # Ensuring all columns except Date are numeric\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "\n",
    "N_LAGS = 2 # Lagged features added below\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling (IMPORTANT) \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c1989",
   "metadata": {},
   "source": [
    "### FinBERT-only LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d53e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    " \n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning columns\n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']: # Removing column headers like '^GSPC' in Close/Open/etc (non-numeric entries)\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns: # Ensure all except 'Date' are numeric\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering \n",
    "N_LAGS = 2  # Adding lags for target and sentiment\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # aligning with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bc6953",
   "metadata": {},
   "source": [
    "#### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829710d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for name in [\"derived/lstm_test_predictions.csv\",\n",
    "             \"derived/lstm_FinBERT_only_test_predictions.csv\"]:\n",
    "    df = pd.read_csv(name, parse_dates=[\"Date\"])\n",
    "    dup = df.duplicated(subset=[\"Date\"]).sum()\n",
    "    print(name, \"rows:\", len(df), \"duplicates:\", dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eec805",
   "metadata": {},
   "source": [
    "## Evaluaiton and Visualizaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb66cd8",
   "metadata": {},
   "source": [
    "### Forecast Biasness Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133a6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "df_gpt   = pd.read_csv(\"derived/lstm_test_predictions.csv\",\n",
    "                       parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\",\n",
    "                       parse_dates=[\"Date\"])\n",
    "market   = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = (df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]]\n",
    "        .rename(columns={\"Return\":\"Market_Return\",\n",
    "                         \"Predicted_Return\":\"LSTM_GPT4\"})\n",
    "      .merge(df_fbert[[\"Date\",\"Predicted_Return\"]]\n",
    "                .rename(columns={\"Predicted_Return\":\"LSTM_FinBERT\"}),\n",
    "             on=\"Date\", how=\"left\")\n",
    "      .merge(market[[\"Date\",\"Return\",\"VIX\"]]\n",
    "                .rename(columns={\"Return\":\"Market_True_Return\"}),\n",
    "             on=\"Date\", how=\"left\")\n",
    "      .dropna(subset=[\"Market_True_Return\",\"LSTM_GPT4\",\"LSTM_FinBERT\"]))\n",
    "\n",
    "y_true   = df[\"Market_True_Return\"].values # Handles\n",
    "pred_gpt = df[\"LSTM_GPT4\"].values\n",
    "pred_fb  = df[\"LSTM_FinBERT\"].values\n",
    "\n",
    "def signal_metrics(y, yhat): # Forecast-Evaluation Metrics \n",
    "    \"\"\"Return dict of point & trading metrics.\"\"\"\n",
    "    mse  = np.mean((y - yhat)**2)\n",
    "    mae  = np.mean(np.abs(y - yhat))\n",
    "    rmse = np.sqrt(mse)\n",
    "    strat_ret = np.where(yhat>0, 1, -1) * y # Trading signal is long if yhat>0, short otherwise\n",
    "    sharpe = np.mean(strat_ret)/(np.std(strat_ret)+1e-9)*np.sqrt(252)\n",
    "    r2 = sm.OLS(y, sm.add_constant(yhat)).fit().rsquared\n",
    "    return dict(MSE=mse, MAE=mae, Directional_Acc=acc,\n",
    "                Sharpe=sharpe, R2=r2)\n",
    "   \n",
    "m_gpt = signal_metrics(y_true, pred_gpt)\n",
    "m_fb  = signal_metrics(y_true, pred_fb)\n",
    "\n",
    "def dm_test(e1, e2, h=1): # Diebold–Mariano (squared-error loss)\n",
    "    d   = e1-e2\n",
    "    T   = len(d)\n",
    "    var = np.var(d, ddof=1) + 2*sum(\n",
    "          np.cov(d[:-k],d[k:])[0,1] for k in range(1,h))\n",
    "    dm  = np.mean(d)/np.sqrt(var/T)\n",
    "    p   = 2*(1-stats.norm.cdf(abs(dm)))\n",
    "    return dm, p\n",
    "\n",
    "dm_stat, dm_p = dm_test((y_true-pred_gpt)**2, (y_true-pred_fb)**2)\n",
    "\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\") # Regime (VIX median) Sharpe comparison\n",
    "vix_med = df[\"VIX\"].median()\n",
    "reg   = {}\n",
    "for regime, lab in [(df[\"VIX\"]>vix_med,\"High-VIX\"),\n",
    "                    (df[\"VIX\"]<=vix_med,\"Low-VIX\")]:\n",
    "    reg[lab] = dict(GPT4   = signal_metrics(\n",
    "                                y_true[regime], pred_gpt[regime])[\"Sharpe\"],\n",
    "                    FinBERT = signal_metrics(\n",
    "                                y_true[regime], pred_fb[regime])[\"Sharpe\"])\n",
    "\n",
    "def bias_tests(y, yhat, label): # Forecast-Bias Section\n",
    "    err = y - yhat\n",
    "\n",
    "    t, p = stats.ttest_1samp(err, 0.0) # Mean-error t-test\n",
    "\n",
    "    X = sm.add_constant(yhat) # Mincer–Zarnowitz regression\n",
    "    mz = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Joint test α=0, β=1  (forecast unbiased)\n",
    "    R = np.eye(2)\n",
    "    q = np.array([0,1])\n",
    "    ftest = mz.f_test((R,q))\n",
    "    return dict(\n",
    "        Model          = label,\n",
    "        Mean_Error     = err.mean(),\n",
    "        MeanErr_tstat  = t,\n",
    "        MeanErr_pval   = p,\n",
    "        MZ_alpha       = mz.params[0],\n",
    "        MZ_beta        = mz.params[1],\n",
    "        MZ_alpha_p     = mz.pvalues[0],\n",
    "        MZ_beta_p      = mz.pvalues[1],\n",
    "        MZ_F_pvalue    = float(ftest.pvalue)\n",
    "    )\n",
    "\n",
    "bias_gpt = bias_tests(y_true, pred_gpt, \"Hybrid (GPT-4)\")\n",
    "bias_fb  = bias_tests(y_true, pred_fb , \"FinBERT-only\")\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Forecast-evaluation\n",
    "eval_tbl = pd.DataFrame([m_gpt, m_fb], index=[\"Hybrid\",\"FinBERT\"])\n",
    "print(\"Forecast-Evaluation Metrics\")\n",
    "print(tabulate(eval_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "print(\"\\nDiebold–Mariano statistic: {:.3f}   p-value: {:.3f}\" # DM-test summary\n",
    "      .format(dm_stat, dm_p))\n",
    "\n",
    "print(\"Regime-dependent Sharpe\") # Regime Sharpe\n",
    "print(tabulate(pd.DataFrame(reg).T, headers=\"keys\",\n",
    "               tablefmt=\"github\", floatfmt=\".3f\"))\n",
    "\n",
    "bias_tbl = pd.DataFrame([bias_gpt, bias_fb]).set_index(\"Model\") # Bias tests\n",
    "print(\"Forecast-Bias Diagnostics\")\n",
    "print(tabulate(bias_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "from sklearn.metrics import accuracy_score # computing validation accuracy for both models\n",
    "\n",
    "print(\"Validation Accuracy\")\n",
    "\n",
    "val_acc_gpt = accuracy_score(\n",
    "    (df[\"Market_True_Return\"].values > 0),   # true direction\n",
    "    (df[\"LSTM_GPT4\"].values          > 0)    # predicted direction\n",
    ")\n",
    "\n",
    "val_acc_fb = accuracy_score(\n",
    "    (df[\"Market_True_Return\"].values > 0),\n",
    "    (df[\"LSTM_FinBERT\"].values       > 0)\n",
    ")\n",
    "\n",
    "print(f\"Hybrid (GPT-4) Validation Accuracy:  {val_acc_gpt:.2%}\")\n",
    "print(f\"FinBERT-only Validation Accuracy:    {val_acc_fb:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e60e5",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a169775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Aligning data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60): # Computing metrics for each model (with rolling mean/std)\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Creating valid data masks and calculate for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "# Table (include rolling mean/std)\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"\\n===== Model Comparison Table =====\\n\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "# 4. Regime split (by VIX): show model dominance in high/low volatility\n",
    "vix_median = df[\"VIX\"].median()\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime ===\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1364d2",
   "metadata": {},
   "source": [
    "### Resulting Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8346cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, r2_score\n",
    ")\n",
    "\n",
    "fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "gpt4 = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "for df in [fbert, gpt4]: # changing objects (str) columns to numeric\n",
    "    for col in df.columns:\n",
    "        if col != \"Date\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\") # General settings\n",
    "sns.set(font_scale=1.2)\n",
    "window = 21  # 1 month rolling was chosen\n",
    "\n",
    "df = fbert[[\"Date\", \"Return\", \"VIX\", \"FinBERT_score\", \"Predicted_Return\"]].copy() # Mergeing for side-by-side plots\n",
    "df = df.rename(columns={\"Predicted_Return\": \"FinBERT_Pred\"})\n",
    "df[\"GPT4_Pred\"] = gpt4[\"Predicted_Return\"].values\n",
    "df[\"GPT4_Sentiment\"] = gpt4[\"Sentiment\"].values if \"Sentiment\" in gpt4 else np.nan\n",
    "\n",
    "\n",
    "# Actual vs LSTM Forecasts: Time Series Overlap\n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"], color='black', label='Market Return', linewidth=2)\n",
    "plt.plot(df[\"Date\"], df[\"FinBERT_Pred\"], color='orange', label='FinBERT-only LSTM', alpha=0.8)\n",
    "plt.plot(df[\"Date\"], df[\"GPT4_Pred\"], color='red', label='FinBERT+GPT-4 LSTM', alpha=0.7)\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Market Returns vs LSTM Model Forecasts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Model Error: Residuals Over Time \n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"FinBERT_Pred\"], label=\"Error: FinBERT LSTM\", color=\"orange\", alpha=0.6)\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"GPT4_Pred\"], label=\"Error: GPT-4 LSTM\", color=\"dodgerblue\", alpha=0.6)\n",
    "plt.axhline(0, color=\"black\", linewidth=1, linestyle=\":\")\n",
    "plt.ylabel(\"Residual (Error)\")\n",
    "plt.title(\"Model Residuals: Market - Model Forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Distribution of Forecasts: Histogram & KDE \n",
    "plt.figure(figsize=(14,5))\n",
    "sns.histplot(df[\"Return\"], label=\"Market Return\", color=\"black\", kde=True, stat=\"density\", bins=40)\n",
    "sns.histplot(df[\"FinBERT_Pred\"], label=\"FinBERT LSTM\", color=\"orange\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "sns.histplot(df[\"GPT4_Pred\"], label=\"GPT-4 LSTM\", color=\"dodgerblue\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Market Returns vs LSTM Model Forecasts\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Actual vs Predicted: Scatter Plots and Regression Fit\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,6), sharey=True)\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"FinBERT_Pred\"], ax=axs[0], line_kws={\"color\": \"orange\"})\n",
    "axs[0].set_title(\"FinBERT LSTM: Actual vs Predicted\")\n",
    "axs[0].set_xlabel(\"Actual Return\")\n",
    "axs[0].set_ylabel(\"Predicted Return\")\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"GPT4_Pred\"], ax=axs[1], line_kws={\"color\": \"dodgerblue\"})\n",
    "axs[1].set_title(\"GPT-4 LSTM: Actual vs Predicted\")\n",
    "axs[1].set_xlabel(\"Actual Return\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Rolling Model RMSE (21d window)\n",
    "rmse_fbert = (df[\"Return\"] - df[\"FinBERT_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "rmse_gpt4 = (df[\"Return\"] - df[\"GPT4_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.plot(df[\"Date\"], rmse_fbert, label=\"Rolling RMSE: FinBERT LSTM\", color=\"orange\")\n",
    "plt.plot(df[\"Date\"], rmse_gpt4, label=\"Rolling RMSE: GPT-4 LSTM\", color=\"dodgerblue\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"21-Day Rolling RMSE: Model Performance Over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ROC Curve: Directional Signal of Models\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "true_bin = (df[\"Return\"] > 0).astype(int)\n",
    "fpr_fbert, tpr_fbert, _ = roc_curve(true_bin, df[\"FinBERT_Pred\"])\n",
    "fpr_gpt4, tpr_gpt4, _ = roc_curve(true_bin, df[\"GPT4_Pred\"])\n",
    "auc_fbert = auc(fpr_fbert, tpr_fbert)\n",
    "auc_gpt4 = auc(fpr_gpt4, tpr_gpt4)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr_fbert, tpr_fbert, label=f\"FinBERT LSTM (AUC={auc_fbert:.2f})\", color=\"orange\")\n",
    "plt.plot(fpr_gpt4, tpr_gpt4, label=f\"GPT-4 LSTM (AUC={auc_gpt4:.2f})\", color=\"dodgerblue\")\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: Model Directional Prediction Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Error Autocorrelation\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "plt.figure(figsize=(8,4))\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"FinBERT_Pred\"])\n",
    "plt.title(\"Error Autocorrelation: FinBERT LSTM\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,4))\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"GPT4_Pred\"])\n",
    "plt.title(\"Error Autocorrelation: GPT-4 LSTM\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cumulative Return Plot\n",
    "df = pd.DataFrame({ # Aligning data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],  # Use market_df[\"Return\"] if better aligned\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)  # Long/short signal\n",
    "    return signal * true_returns\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Establishing valid data masks and calculate for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'] # Align dates\n",
    "\n",
    "gpt_signal = np.where(df.loc[mask_gpt, 'LSTM_GPT4'] > 0, 1, -1) # Computing trading strategy returns for both models\n",
    "finbert_signal = np.where(df.loc[mask_fbert, 'LSTM_FinBERT'] > 0, 1, -1)\n",
    "\n",
    "gpt_strat_returns = gpt_signal * df.loc[mask_gpt, 'Market_True_Return'].values\n",
    "finbert_strat_returns = finbert_signal * df.loc[mask_fbert, 'Market_True_Return'].values\n",
    "\n",
    "gpt_cum_return = np.cumsum(gpt_strat_returns) # Computing cumulative returns\n",
    "finbert_cum_return = np.cumsum(finbert_strat_returns)\n",
    "market_cum_return = np.cumsum(df.loc[mask_gpt, 'Market_True_Return'].values)\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(dates, market_cum_return, label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(dates, gpt_cum_return, label=\"Hybrid Cumulative Return\", color='red')\n",
    "plt.plot(dates, finbert_cum_return, label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "plt.title(\"Cumulative Strategy Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Rolling Directional Accuracy and Sharpe Ratio \n",
    "win = 63      \n",
    "def rolling_directional_acc(true_ret, pred_ret, win=win):\n",
    "    \"\"\"\n",
    "    Percentage of days inside a rolling window where sign(pred)==sign(true).\n",
    "    win=63 ≈ one quarter of trading days.\n",
    "    \"\"\"\n",
    "    hit = (np.sign(true_ret) == np.sign(pred_ret)).astype(int)\n",
    "    return pd.Series(hit).rolling(win, min_periods=1).mean() * 100  # % scale\n",
    "\n",
    "mkt_true = df.loc[mask_gpt, \"Market_True_Return\"].values  # Aligning the same true-return vector you used for Sharpe\n",
    "pred_gpt = df.loc[mask_gpt, \"LSTM_GPT4\"].values\n",
    "pred_fbt = df.loc[mask_fbert, \"LSTM_FinBERT\"].values\n",
    "dates_da = df.loc[mask_gpt, \"Date\"].values\n",
    "\n",
    "da_gpt   = rolling_directional_acc(mkt_true, pred_gpt)\n",
    "da_fbt   = rolling_directional_acc(mkt_true, pred_fbt)\n",
    "\n",
    "dates_all = df.loc[mask_gpt, \"Date\"].values \n",
    "\n",
    "def rolling_sharpe(returns, win=win): # Defining sharpe_gpt and sharpe_fbert\n",
    "    \"\"\"\n",
    "    Calculate rolling Sharpe ratio over a specified window.\n",
    "    Returns annualised Sharpe ratio.\n",
    "    \"\"\"\n",
    "    mean_ret = returns.rolling(win).mean()\n",
    "    std_ret  = returns.rolling(win).std(ddof=0)  # population std\n",
    "    return (mean_ret / (std_ret + 1e-9)) * np.sqrt(252)  # annualised\n",
    "sharpe_gpt   = rolling_sharpe(df.loc[mask_gpt, \"Market_True_Return\"] - df.loc[mask_gpt, \"LSTM_GPT4\"])\n",
    "sharpe_fbert = rolling_sharpe(df.loc[mask_fbert, \"Market_True_Return\"] - df.loc[mask_fbert, \"LSTM_FinBERT\"])\n",
    "\n",
    "trim = win - 1 # Trimming FIRST (win-1) observations where rolling metrics\n",
    "dates_trim        = dates_all[trim:]          # shared for both plots\n",
    "sharpe_gpt_trim   = sharpe_gpt.iloc[trim:]\n",
    "sharpe_fbt_trim   = sharpe_fbert.iloc[trim:]\n",
    "da_gpt_trim       = da_gpt.iloc[trim:]\n",
    "da_fbt_trim       = da_fbt.iloc[trim:]\n",
    "\n",
    "plt.figure(figsize=(15,5)) # Rolling Sharpe (after trim)\n",
    "plt.plot(dates_trim, sharpe_gpt_trim,   label=\"Hybrid rolling Sharpe\",  c=\"red\")\n",
    "plt.plot(dates_trim, sharpe_fbt_trim,   label=\"FinBERT rolling Sharpe\", c=\"darkorange\")\n",
    "plt.axhline(0, ls='--', c='k')\n",
    "plt.title(f\"Rolling Sharpe Ratio ({win}-day Window)\")\n",
    "plt.ylabel(\"Sharpe Ratio (annualised)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5)) # Rolling Directional-Accuracy (after trim)\n",
    "plt.plot(dates_trim, da_gpt_trim, label=\"Hybrid directional accuracy\",  c=\"red\")\n",
    "plt.plot(dates_trim, da_fbt_trim, label=\"FinBERT directional accuracy\", c=\"darkorange\")\n",
    "plt.axhline(50, ls='--', c='k', lw=0.8)       # coin-flip baseline\n",
    "plt.ylim(0, 100)\n",
    "plt.title(f\"Rolling Directional Accuracy ({win}-day Window)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_conf_mat(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true > 0, y_pred > 0)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Down\", \"Up\"], yticklabels=[\"Down\", \"Up\"], ax=ax)\n",
    "    ax.set_xlabel(\"Model Signal\"); ax.set_ylabel(\"Market Direction\"); ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "plot_conf_mat(df.loc[mask_gpt,\"Market_True_Return\"],\n",
    "              df.loc[mask_gpt,\"LSTM_GPT4\"],\n",
    "              \"Hybrid\", axes[0])\n",
    "plot_conf_mat(df.loc[mask_fbert,\"Market_True_Return\"],\n",
    "              df.loc[mask_fbert,\"LSTM_FinBERT\"],\n",
    "              \"FinBERT-only\", axes[1])\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# Turnover Function Vs Net-Long Ratio\n",
    "def turnover(signal):\n",
    "    return (np.abs(np.diff(signal)) / 2).mean() # Average fraction of days where direction flips\n",
    "\n",
    "sig_gpt   = np.where(df.loc[mask_gpt,  'LSTM_GPT4']   > 0, 1, -1)\n",
    "sig_fbert = np.where(df.loc[mask_fbert,'LSTM_FinBERT']> 0, 1, -1)\n",
    "\n",
    "print(f\"Hybrid turnover:  {turnover(sig_gpt):.2%}\")\n",
    "print(f\"FinBERT turnover: {turnover(sig_fbert):.2%}\")\n",
    "print(f\"Hybrid net-long ratio:  {(sig_gpt==1).mean():.2%}\")\n",
    "print(f\"FinBERT net-long ratio: {(sig_fbert==1).mean():.2%}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sig_gpt   = np.where(df.loc[mask_gpt,  'LSTM_GPT4']   > 0, 1, -1) # Re-computing signals (or reuse sig_gpt / sig_fbert from earlier)\n",
    "sig_fbert = np.where(df.loc[mask_fbert,'LSTM_FinBERT']> 0, 1, -1)\n",
    "\n",
    "def turnover(s):                      # share of days that flip direction\n",
    "    return (np.abs(np.diff(s)) / 2).mean()\n",
    "\n",
    "turn = [turnover(sig_gpt), turnover(sig_fbert)]\n",
    "netL = [(sig_gpt == 1).mean(), (sig_fbert == 1).mean()]   # net-long ratios\n",
    "\n",
    "labels = [\"Hybrid\\n(FinBERT+GPT-4)\", \"FinBERT-only\"]\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, turn, width,    # Left y-axis => Turnover\n",
    "                color=\"steelblue\", alpha=.85, label=\"Turnover\")\n",
    "ax1.set_ylabel(\"Turnover (% of days)\", color=\"steelblue\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, fontsize=11)\n",
    "ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n",
    "\n",
    "ax2 = ax1.twinx()   # Right y-axis => Net-Long exposure\n",
    "bars2 = ax2.bar(x + width/2, netL, width,\n",
    "                color=\"darkorange\", alpha=.85, label=\"Net-Long Ratio\")\n",
    "ax2.set_ylabel(\"Net-Long Exposure\", color=\"darkorange\")\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkorange\")\n",
    "\n",
    "for rect in bars1 + bars2:   # Annotating bars\n",
    "    height = rect.get_height()\n",
    "    ax = ax1 if rect in bars1 else ax2\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 0.02,\n",
    "            f\"{height:.2%}\", ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.title(\"Turnover vs Net-Long Ratio\", pad=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38106b0",
   "metadata": {},
   "source": [
    "## Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7524b9a8",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: Hybrid Method (Shorter Time Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1449bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Clean Up Columns\n",
    "\n",
    "for col in df.columns: # Ensuring all columns except Date are numeric\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "N_LAGS = 2 #lagged features\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Directional accuracy\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_prac_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1b240",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: FinBERT-only Method (Shorter Time Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2786f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Clean columns \n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']: # Removing non-numeric entry columns\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns: # Set all except 'Date' are numeric\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering\n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # aligning with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb5ea5b",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e59b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_prac_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Align data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60): # Computing all metrics for each model (with rolling mean/std)\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Data masks and calculated for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "comparison_table = pd.DataFrame({ # Table\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"Model Comparison Table\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "print(\"Confusion Matrices\") # Confusion matrices for appendix\n",
    "print(\"Hybrid (FinBERT+GPT-4):\\n\", metrics_gpt[\"Confusion_Matrix\"])\n",
    "print(\"FinBERT-only:\\n\", metrics_finbert[\"Confusion_Matrix\"])\n",
    "np.savetxt(\"hybrid_confusion_matrix.csv\", metrics_gpt[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "np.savetxt(\"finbert_confusion_matrix.csv\", metrics_finbert[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "\n",
    "vix_median = df[\"VIX\"].median() # Regime split (by VIX) shows model dominance in high/low volatility\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime ===\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738074c",
   "metadata": {},
   "source": [
    "### Shortened Time-frame Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_prac_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({ # Align data by date\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],  # Use market_df[\"Return\"] if better aligned\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "def trading_signal_returns(true_returns, predicted_returns): # Signal-based trading returns\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)  # Long/short signal\n",
    "    return signal * true_returns\n",
    "\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull() # Data masks and calculated for both models\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'] # Align dates\n",
    "\n",
    "gpt_signal = np.where(df.loc[mask_gpt, 'LSTM_GPT4'] > 0, 1, -1) # Trading strategy returns for both models\n",
    "finbert_signal = np.where(df.loc[mask_fbert, 'LSTM_FinBERT'] > 0, 1, -1)\n",
    "\n",
    "gpt_strat_returns = gpt_signal * df.loc[mask_gpt, 'Market_True_Return'].values\n",
    "finbert_strat_returns = finbert_signal * df.loc[mask_fbert, 'Market_True_Return'].values\n",
    "\n",
    "gpt_cum_return = np.cumsum(gpt_strat_returns) # Cumulative returns\n",
    "finbert_cum_return = np.cumsum(finbert_strat_returns)\n",
    "market_cum_return = np.cumsum(df.loc[mask_gpt, 'Market_True_Return'].values)\n",
    "\n",
    "dates = df.loc[mask_gpt, 'Date'].reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12, 5)) # Plotting cumulative returns\n",
    "plt.plot(dates, market_cum_return, label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(dates, gpt_cum_return, label=\"Hybrid Cumulative Return\", color='red')\n",
    "plt.plot(dates, finbert_cum_return, label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "plt.title(\"Cumulative Strategy Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
