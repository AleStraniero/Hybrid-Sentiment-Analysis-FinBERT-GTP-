{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55850c79",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ef5b5",
   "metadata": {},
   "source": [
    "#### Implementation laid out by Jakob Aungiers https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c51dc7",
   "metadata": {},
   "source": [
    "#### Conda virtual enviroment is implemented to run Python 3.11 throughout the remainder of this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d4beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!pip install numpy pandas scikit-learn tensorflow keras matplotlib\n",
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18735360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795cd8b",
   "metadata": {},
   "source": [
    "### Hybrid (FinBERT+GPT) Sentiment LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ced87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning up columns\n",
    "\n",
    "for col in df.columns: # Ensuring all columns except Date are numeric\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "\n",
    "N_LAGS = 2 # Lagged features added below\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling (IMPORTANT) \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e068e5a",
   "metadata": {},
   "source": [
    "### FinBERT-only LSTM Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda20a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    " \n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns]) # Cleaning columns\n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']: # Removing column headers like '^GSPC' in Close/Open/etc (non-numeric entries)\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns: # Ensure all except 'Date' are numeric\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering \n",
    "N_LAGS = 2  # Adding lags for target and sentiment\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Evaluating …\") # Evaluation\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "\n",
    "actual_dir = (y_test_seq > 0) # Directional accuracy\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # aligning with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28769aba",
   "metadata": {},
   "source": [
    "#### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for name in [\"derived/lstm_test_predictions.csv\",\n",
    "             \"derived/lstm_FinBERT_only_test_predictions.csv\"]:\n",
    "    df = pd.read_csv(name, parse_dates=[\"Date\"])\n",
    "    dup = df.duplicated(subset=[\"Date\"]).sum()\n",
    "    print(name, \"rows:\", len(df), \"duplicates:\", dup)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
