{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2549fddd",
   "metadata": {},
   "source": [
    "# Thesis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21429fdb",
   "metadata": {},
   "source": [
    "#### Using a Python 3.13 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d3f9f",
   "metadata": {},
   "source": [
    "### Setup and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e204833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a72024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import openai         \n",
    "import transformers    \n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.tsa.stattools as stt\n",
    "import statsmodels.tsa.holtwinters as smt\n",
    "import statsmodels.tsa.seasonal as smt\n",
    "import statsmodels.tsa.arima.model as smt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36c14a",
   "metadata": {},
   "source": [
    "## S&P 500 Index Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838db252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "# SP500 data\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2015-01-01\", end=\"2025-01-01\")\n",
    "\n",
    "# Calculating log returns: log(Close_t / Close_{t-1})\n",
    "sp500['Return'] = np.log(sp500['Close'] / sp500['Close'].shift(1))\n",
    "\n",
    "# Reset index for merging\n",
    "sp500 = sp500.reset_index()\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# VIX data\n",
    "vix = yf.download(\"^VIX\", start=\"2015-01-01\", end=\"2025-01-01\")[['Close']]\n",
    "vix.rename(columns={'Close':'VIX'}, inplace=True)\n",
    "vix = vix.reset_index()\n",
    "vix['Date'] = pd.to_datetime(vix['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merging the two DataFrames on Date\n",
    "merged_data = pd.merge(sp500, vix, on='Date', how='inner')\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "\n",
    "# Drop any rows with NA values (from log return calculation)\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "merged_data.to_csv('sp500_vix_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a0762",
   "metadata": {},
   "source": [
    "## New York Times Article Summary Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66202ff",
   "metadata": {},
   "source": [
    "### General Article Scriping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cb64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, sys\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY = os.getenv(\"NYT_API_KEY\", \"NYT_API_KEY_NOT_SET\")  # Set your NYT API key here or in .env file\n",
    "\n",
    "START_DATE  = date(2015, 1, 1)\n",
    "END_DATE    = date(2025, 1, 1)\n",
    "CSV_PATH    = \"nyt_business_archive.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC   = 60 / REQS_PER_MIN        # The 12s pause guarantees we stay within the API rate limit of 5-calls per minute\n",
    "CHECKPOINT  = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "\n",
    "# Wrapper that handles 429 and retries\n",
    "def archive_month(year: int, month: int, max_retries: int = 5) -> Dict:\n",
    "    \"\"\"Fetch one month from NYT Archive API with rate-limit back-off.\"\"\"\n",
    "    url    = ARCHIVE_URL.format(year=year, month=month)\n",
    "    params = {\"api-key\": NYT_API_KEY}\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        resp = requests.get(url, params=params, timeout=60)\n",
    "        if resp.status_code == 429:                # Too Many Requests\n",
    "            wait = int(resp.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"⚠️  429 received — sleeping {wait}s and retrying …\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "\n",
    "    raise RuntimeError(f\"Failed to fetch {year}-{month:02} after {max_retries} retries\")\n",
    "\n",
    "\n",
    "def iter_months(start: date, end: date):\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        current = date(y, m, 1)\n",
    "        if current > end.replace(day=1):\n",
    "            break\n",
    "        yield current\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "\n",
    "def load_checkpoint():\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_checkpoint(d: date):\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "\n",
    "def append_rows(rows: List[Dict]):\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "\n",
    "def is_business(doc: Dict) -> bool:\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "\n",
    "def main():\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} — {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)        # uses new wrapper\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d} docs\")\n",
    "        time.sleep(SLEEP_SEC)                               # NEW slower pause\n",
    "\n",
    "    print(\"✅ All months processed. CSV complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted — progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43b836",
   "metadata": {},
   "source": [
    "### Aggregated Sector Mapping Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json, sys, re\n",
    "from datetime import date\n",
    "from typing import List, Dict\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "path = \"nyt_business_archive.csv\"\n",
    "\n",
    "df = pd.read_csv(path, header=None)  # no header to see all columns\n",
    "max_cols = df.shape[1]\n",
    "\n",
    "if max_cols == 5 and \"Summary\" not in pd.read_csv(path, nrows=0).columns:\n",
    "    df.columns = [\"Date\", \"Headline\", \"Summary\", \"Section\", \"URL\"]\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"✔ Added missing 'Summary' header and rewrote file.\")\n",
    "else:\n",
    "    print(\"No header problem detected — nothing changed.\")\n",
    "\n",
    "# Configuration:\n",
    "NYT_API_KEY  = os.getenv(\"NYT_API_KEY\", \"SyfY3kXFPKxqBqdhMTAVblDmVJwLQEAV\")\n",
    "START_DATE   = date(2015, 1, 1)\n",
    "END_DATE     = date(2025, 1, 1)\n",
    "CSV_PATH     = \"nyt_business_archive.csv\"\n",
    "AGG_PATH     = \"nyt_aggregated_data.csv\"\n",
    "REQS_PER_MIN = 5\n",
    "SLEEP_SEC    = 60 / REQS_PER_MIN       # 12 s → 5 req/min\n",
    "CHECKPOINT   = \"archive_checkpoint.json\"\n",
    "ARCHIVE_URL  = \"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "\n",
    "\n",
    "# NYT API call with 429 back-off to handle rate limits\n",
    "def archive_month(year: int, month: int, retries=5) -> Dict:\n",
    "    url, params = ARCHIVE_URL.format(year=year, month=month), {\"api-key\": NYT_API_KEY}\n",
    "    for _ in range(retries):\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        if r.status_code == 429:                       # Too Many Requests\n",
    "            wait = int(r.headers.get(\"Retry-After\", \"15\"))\n",
    "            print(f\"⚠️  429 – sleeping {wait}s\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    raise RuntimeError(f\"Failed after {retries} retries: {year}-{month:02}\")\n",
    "\n",
    "# Generator to iterate over months in a date range\n",
    "def iter_months(start: date, end: date):\n",
    "    y, m = start.year, start.month\n",
    "    while True:\n",
    "        first = date(y, m, 1)\n",
    "        if first > end.replace(day=1):\n",
    "            break\n",
    "        yield first\n",
    "        m += 1\n",
    "        if m == 13:\n",
    "            m, y = 1, y + 1\n",
    "\n",
    "# Checkpoint management for resuming crawls\n",
    "def load_checkpoint():\n",
    "    if os.path.isfile(CHECKPOINT):\n",
    "        with open(CHECKPOINT) as fp:\n",
    "            return date.fromisoformat(json.load(fp)[\"last_month\"])\n",
    "    return None\n",
    "\n",
    "# Save the last processed month to a checkpoint file\n",
    "def save_checkpoint(d: date):\n",
    "    with open(CHECKPOINT, \"w\") as fp:\n",
    "        json.dump({\"last_month\": d.isoformat()}, fp)\n",
    "\n",
    "# Append rows to the CSV file, creating it if it doesn't exist\n",
    "def append_rows(rows: List[Dict]):\n",
    "    pd.DataFrame(rows).to_csv(\n",
    "        CSV_PATH,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not os.path.isfile(CSV_PATH),\n",
    "    )\n",
    "\n",
    "\n",
    "# Business filter\n",
    "def is_business(doc: Dict) -> bool:\n",
    "    sec  = (doc.get(\"section_name\") or \"\").lower()\n",
    "    desk = (doc.get(\"news_desk\")    or \"\").lower()\n",
    "    return \"business\" in sec or \"business\" in desk\n",
    "\n",
    "# Main crawling function to fetch NYT archive data\n",
    "def crawl_nyt_archive():\n",
    "    resume_from = load_checkpoint()\n",
    "    months      = list(iter_months(START_DATE, END_DATE))\n",
    "    if resume_from:\n",
    "        months = [m for m in months if m > resume_from]\n",
    "        print(f\"Resuming after {resume_from} – {len(months)} months left.\")\n",
    "\n",
    "    for first in months:\n",
    "        print(f\"Fetching {first:%Y-%m} …\", end=\" \", flush=True)\n",
    "        data = archive_month(first.year, first.month)\n",
    "        docs = data[\"response\"][\"docs\"]\n",
    "\n",
    "        rows = [\n",
    "            {\n",
    "                \"Date\": doc[\"pub_date\"][:10],\n",
    "                \"Headline\": doc[\"headline\"][\"main\"],\n",
    "                \"Summary\": doc.get(\"abstract\") or doc.get(\"snippet\", \"\"),\n",
    "                \"Section\": doc.get(\"section_name\") or doc.get(\"news_desk\"),\n",
    "                \"URL\": doc[\"web_url\"],\n",
    "            }\n",
    "            for doc in docs if is_business(doc)\n",
    "        ]\n",
    "\n",
    "        append_rows(rows)\n",
    "        save_checkpoint(first)\n",
    "        print(f\"kept {len(rows):3d} / {len(docs):3d}\")\n",
    "        time.sleep(SLEEP_SEC)\n",
    "\n",
    "    print(\"✅ NYT crawl complete.\")\n",
    "\n",
    "\n",
    "# Sector classification & aggregation\n",
    "SECTOR_MAP: dict[str, list[str]] = {\n",
    "    # Information Tech Secotr:\n",
    "    \"Software & IT Services\": [\n",
    "        \"software\", \"saas\", \"cloud\", \"it services\", \"consulting\",\n",
    "        \"microsoft\", \"adobe\", \"oracle\", \"sap\", \"salesforce\", \"servicenow\",\n",
    "        \"workday\", \"vmware\", \"accenture\", \"infosys\", \"tcs\", \"capgemini\",\n",
    "    ],\n",
    "    \"Hardware & Devices\": [\n",
    "        \"hardware\", \"pc\", \"laptop\", \"smartphone\", \"iphone\", \"ipad\",\n",
    "        \"dell\", \"hp\", \"lenovo\", \"asus\", \"acer\", \"logitech\",\n",
    "    ],\n",
    "    \"Semiconductors\": [\n",
    "        \"chip\", \"chips\", \"semiconductor\", \"fab\", \"foundry\",\n",
    "        \"intel\", \"amd\", \"nvidia\", \"qualcomm\", \"tsmc\", \"broadcom\",\n",
    "        \"micron\", \"arm holdings\", \"sk hynix\",\n",
    "    ],\n",
    "    \"Internet & Social Media\": [\n",
    "        \"google\", \"alphabet\", \"youtube\", \"search engine\",\n",
    "        \"meta\", \"facebook\", \"instagram\", \"whatsapp\", \"threads\",\n",
    "        \"twitter\", \"x corp\", \"snapchat\", \"tiktok\", \"reddit\",\n",
    "        \"linkedin\", \"pinterest\", \"social media\",\n",
    "    ],\n",
    "    # Communication & Media Sector:\n",
    "    \"Telecommunications\": [\n",
    "        \"telecom\", \"5g\", \"wireless\", \"broadband\",\n",
    "        \"verizon\", \"at&t\", \"t-mobile\", \"comcast\", \"charter\",\n",
    "        \"vodafone\", \"telefonica\", \"bt group\", \"rogers\", \"singtel\",\n",
    "    ],\n",
    "    \"Media & Entertainment\": [\n",
    "        \"media\", \"streaming\", \"disney\", \"espn\", \"hulu\",\n",
    "        \"netflix\", \"warner bros\", \"hbo\", \"paramount\", \"peacock\",\n",
    "        \"sony pictures\", \"universal\", \"box office\", \"cinema\",\n",
    "    ],\n",
    "    # Consumer Sector:\n",
    "    \"Retail & E-Commerce\": [\n",
    "        \"retail\", \"e-commerce\", \"amazon\", \"alibaba\", \"shopify\",\n",
    "        \"ebay\", \"etsy\", \"walmart\", \"target\", \"costco\", \"kroger\",\n",
    "        \"best buy\", \"flipkart\", \"mercado libre\",\n",
    "    ],\n",
    "    \"Consumer Goods & Apparel\": [\n",
    "        \"nike\", \"adidas\", \"lululemon\", \"puma\", \"under armour\",\n",
    "        \"apparel\", \"footwear\", \"luxury\", \"lvmh\", \"gucci\", \"burberry\",\n",
    "        \"rolex\", \"hermes\", \"tapestry\",\n",
    "    ],\n",
    "    \"Food & Beverage\": [\n",
    "        \"food\", \"beverage\", \"coca-cola\", \"pepsico\", \"nestlé\",\n",
    "        \"restaurant\", \"fast food\", \"mcdonald\", \"starbucks\",\n",
    "        \"yum brands\", \"kfc\", \"pizza hut\", \"chipotle\",\n",
    "        \"kraft\", \"general mills\", \"heinz\", \"tyson foods\",\n",
    "    ],\n",
    "    \"Hospitality & Leisure\": [\n",
    "        \"hotel\", \"marriott\", \"hilton\", \"hyatt\", \"airbnb\",\n",
    "        \"booking.com\", \"expedia\", \"travel\", \"cruise\", \"carnival\",\n",
    "        \"royal caribbean\", \"las vegas sands\", \"mgm resorts\",\n",
    "    ],\n",
    "    \"Automotive\": [\n",
    "        \"automotive\", \"auto\", \"car\", \"vehicle\", \"ev\",\n",
    "        \"tesla\", \"general motors\", \"ford\", \"stellantis\",\n",
    "        \"volkswagen\", \"toyota\", \"nissan\", \"bmw\", \"mercedes\",\n",
    "        \"hyundai\", \"kia\", \"rivian\", \"lucid\",\n",
    "    ],\n",
    "    # Healthcare Sector:\n",
    "    \"Pharmaceuticals\": [\n",
    "        \"pharma\", \"drug\", \"medicine\", \"vaccine\", \"fda\",\n",
    "        \"pfizer\", \"moderna\", \"johnson & johnson\", \"merck\",\n",
    "        \"novartis\", \"roche\", \"astrazeneca\", \"bayer\", \"gsk\",\n",
    "    ],\n",
    "    \"Biotechnology\": [\n",
    "        \"biotech\", \"gene therapy\", \"crispr\", \"genomics\",\n",
    "        \"illumina\", \"gilead\", \"amgen\", \"biogen\", \"regeneron\",\n",
    "        \"vertex\", \"bluebird bio\",\n",
    "    ],\n",
    "    \"Medical Devices & Services\": [\n",
    "        \"medical device\", \"medtech\", \"diagnostics\", \"surgical\",\n",
    "        \"medtronic\", \"boston scientific\", \"abbott\", \"stryker\",\n",
    "        \"philips healthcare\", \"siemens healthineers\", \"cardinal health\",\n",
    "        \"hospital\", \"clinic\", \"healthcare services\",\n",
    "    ],\n",
    "    # Energy & Utilities Sector:\n",
    "    \"Oil & Gas\": [\n",
    "        \"oil\", \"gas\", \"petroleum\", \"upstream\", \"downstream\",\n",
    "        \"exxon\", \"chevron\", \"bp\", \"shell\", \"totalenergies\",\n",
    "        \"conocophillips\", \"aramco\", \"occidental\", \"slb\",\n",
    "    ],\n",
    "    \"Renewables & Clean Energy\": [\n",
    "        \"renewable\", \"solar\", \"wind\", \"geothermal\", \"hydro\",\n",
    "        \"clean energy\", \"green energy\", \"next era\", \"sunpower\",\n",
    "        \"first solar\", \"enphase\", \"vestas\", \"siemens gamesa\",\n",
    "        \"hydrogen\", \"electrolyzer\", \"fuel cell\",\n",
    "    ],\n",
    "    \"Utilities\": [\n",
    "        \"utility\", \"power grid\", \"electricity\", \"water utility\",\n",
    "        \"natural gas utility\", \"duke energy\", \"southern company\",\n",
    "        \"dominion\", \"pg&e\", \"national grid\", \"aes\",\n",
    "    ],\n",
    "    # Financials Sector:\n",
    "    \"Banks\": [\n",
    "        \"bank\", \"commercial bank\", \"jpmorgan\", \"bank of america\",\n",
    "        \"citigroup\", \"wells fargo\", \"goldman sachs\", \"morgan stanley\",\n",
    "        \"u.s. bancorp\", \"hsbc\", \"barclays\", \"santander\", \"dbs\",\n",
    "    ],\n",
    "    \"Investment & Asset Management\": [\n",
    "        \"asset manager\", \"blackrock\", \"vanguard\", \"fidelity\",\n",
    "        \"state street\", \"schwab\", \"hedge fund\", \"private equity\",\n",
    "        \"kkr\", \"carried interest\", \"mutual fund\", \"etf\",\n",
    "        \"sovereign wealth fund\",\n",
    "    ],\n",
    "    \"Insurance\": [\n",
    "        \"insurance\", \"insurer\", \"aig\", \"allstate\", \"progressive\",\n",
    "        \"metlife\", \"prudential\", \"chubb\", \"berkshire hathaway insurance\",\n",
    "        \"reinsurance\", \"lloyd's\", \"actuarial\",\n",
    "    ],\n",
    "    \"Fintech & Payments\": [\n",
    "        \"fintech\", \"payment\", \"visa\", \"mastercard\",\n",
    "        \"american express\", \"paypal\", \"block inc\", \"square\",\n",
    "        \"stripe\", \"sofi\", \"robinhood\", \"buy now pay later\",\n",
    "        \"klarna\", \"affirm\", \"ant financial\",\n",
    "    ],\n",
    "    \"Cryptocurrency & Blockchain\": [\n",
    "        \"bitcoin\", \"ethereum\", \"crypto\", \"blockchain\",\n",
    "        \"coinbase\", \"binance\", \"defi\", \"nft\",\n",
    "        \"stablecoin\", \"mining rig\", \"hashrate\",\n",
    "    ],\n",
    "    # Industrial Sector:\n",
    "    \"Aerospace & Defense\": [\n",
    "        \"aerospace\", \"defense\", \"boeing\", \"airbus\", \"northrop\",\n",
    "        \"lockheed martin\", \"raytheon\", \"bae systems\", \"general dynamics\",\n",
    "        \"drones\", \"satellite\", \"nasa contract\",\n",
    "    ],\n",
    "    \"Transportation & Logistics\": [\n",
    "        \"shipping\", \"freight\", \"logistics\", \"supply chain\",\n",
    "        \"fedex\", \"ups\", \"dhl\", \"maersk\", \"csx\", \"union pacific\",\n",
    "        \"delta airlines\", \"american airlines\", \"united airlines\",\n",
    "        \"railroad\", \"port congestion\",\n",
    "    ],\n",
    "    \"Manufacturing & Machinery\": [\n",
    "        \"manufacturing\", \"factory\", \"industrial\", \"caterpillar\",\n",
    "        \"3m\", \"general electric\", \"siemens\", \"honeywell\", \"emerson\",\n",
    "        \"robotics\", \"automation\", \"abb\", \"fanuc\",\n",
    "    ],\n",
    "    \"Construction & Engineering\": [\n",
    "        \"construction\", \"engineering\", \"infrastructure\",\n",
    "        \"bechtel\", \"fluor\", \"jacobs\", \"skanska\", \"kiewit\",\n",
    "        \"turner construction\", \"architect\", \"building materials\",\n",
    "    ],\n",
    "    \"Chemicals & Specialty Materials\": [\n",
    "        \"chemical\", \"chemicals\", \"specialty chemical\",\n",
    "        \"dupont\", \"dow\", \"basf\", \"lyondellbasell\", \"air products\",\n",
    "        \"eastman\", \"evonik\", \"synthetic rubber\", \"petrochemical\",\n",
    "    ],\n",
    "    \"Metals & Mining\": [\n",
    "        \"mining\", \"metal\", \"steel\", \"aluminum\", \"copper\",\n",
    "        \"iron ore\", \"rio tinto\", \"bhp\", \"vale\", \"newmont\",\n",
    "        \"glencore\", \"lithium\", \"nickel\", \"rare earth\",\n",
    "    ],\n",
    "    \"Agriculture\": [\n",
    "        \"agriculture\", \"farming\", \"crop\", \"soybean\", \"corn\",\n",
    "        \"wheat\", \"cargill\", \"archer daniels midland\", \"bunge\",\n",
    "        \"deere\", \"monsanto\", \"fertilizer\", \"nutrien\", \"potash\",\n",
    "    ],\n",
    "    # Real Estate Sector:\n",
    "    \"Real Estate\": [\n",
    "        \"real estate\", \"realtor\", \"reit\", \"property\", \"mortgage\",\n",
    "        \"office vacancy\", \"housing market\", \"zillow\", \"redfin\",\n",
    "        \"wework\", \"commercial property\", \"residential property\",\n",
    "        \"industrial park\", \"logistics park\",\n",
    "    ],\n",
    "    # ESG / Government / Education Sectors:\n",
    "    \"Environmental & ESG\": [\n",
    "        \"esg\", \"sustainability\", \"carbon\", \"emissions\",\n",
    "        \"carbon credit\", \"offset\", \"green bond\", \"climate risk\",\n",
    "        \"cop28\", \"environmental regulation\",\n",
    "    ],\n",
    "    \"Government & Policy\": [\n",
    "        \"government\", \"regulation\", \"legislation\", \"policy\",\n",
    "        \"federal reserve\", \"congress\", \"white house\",\n",
    "        \"eu commission\", \"trade tariff\", \"sanction\", \"geopolitics\",\n",
    "    ],\n",
    "    \"Education\": [\n",
    "        \"education\", \"edtech\", \"university\", \"college\", \"school\",\n",
    "        \"coursera\", \"edx\", \"udemy\", \"chegg\", \"student loan\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Function to assign sectors based on keywords in the summary text\n",
    "def assign_sector(text: str) -> str:\n",
    "    text_low = text.lower()\n",
    "    for sector, keywords in SECTOR_MAP.items():\n",
    "        if any(kw in text_low for kw in keywords):\n",
    "            return sector\n",
    "    return \"General\"\n",
    "\n",
    "# Aggregation function to combine headlines and summaries by Date and Sector\n",
    "def aggregate_nyt(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Combine all Business headlines/summaries into one row per Date × Sector.\"\"\"\n",
    "    return (\n",
    "        df.groupby([\"Date\", \"Sector\"])\n",
    "          .agg({\n",
    "              \"Headline\": lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "              \"Summary\":  lambda x: \" | \".join(x.dropna().astype(str)),\n",
    "          })\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "# Main function to run the entire process\n",
    "def main():\n",
    "    # Resume NYT archive\n",
    "    crawl_nyt_archive()\n",
    "\n",
    "    # Loading the full Business CSV\n",
    "    nyt_df = pd.read_csv(CSV_PATH)\n",
    "    nyt_df[\"Headline\"] = nyt_df[\"Headline\"].astype(str)\n",
    "    nyt_df[\"Summary\"] = nyt_df[\"Summary\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Assigning sectors\n",
    "    nyt_df[\"Sector\"] = nyt_df[\"Summary\"].fillna(\"\").apply(assign_sector)\n",
    "\n",
    "    # Aggregate\n",
    "    nyt_aggregated = aggregate_nyt(nyt_df)\n",
    "    nyt_aggregated.to_csv(AGG_PATH, index=False)\n",
    "    print(f\"Aggregated file written: {AGG_PATH}\")\n",
    "\n",
    "    # Clean-up if news_df already exists\n",
    "    if \"news_df\" in globals():\n",
    "        news_df[\"Article\"] = (\n",
    "            news_df[\"Article\"]\n",
    "            .str.replace(r\"By .*? \\|\", \"\", regex=True)\n",
    "            .str.replace(r\"\\n+\", \" \", regex=True)\n",
    "            .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        print(\"Reuters news_df cleaned.\")\n",
    "\n",
    "# Main function when this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted – progress saved, just rerun to continue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd20aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the NYT scrapped aggregate data file\n",
    "nyt_aggregated = pd.read_csv(\"nyt_aggregated_data.csv\")\n",
    "print(nyt_aggregated.head())\n",
    "# Print the first few rows of the merged S&P 500 and VIX data\n",
    "merged_data = pd.read_csv(\"sp500_vix_data.csv\")\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3cd63b",
   "metadata": {},
   "source": [
    "## Senitment Analysis Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5b30d",
   "metadata": {},
   "source": [
    "### Unified FinBERT and GPT-4 Fall Back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Config\n",
    "USE_GPT4_FALLBACK = True\n",
    "GPT4_CONFIDENCE_THRESHOLD = 0.05                     \n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "OPENAI_MAX_TOKENS = 10\n",
    "\n",
    "DERIVED_PATH = \"derived\"\n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True)\n",
    "\n",
    "GPT4_CHECKPOINT_EVERY = 250   # save after every 250 GPT-4 calls\n",
    "\n",
    "# Ensure OpenAI key only if fallback is active\n",
    "if USE_GPT4_FALLBACK:\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        raise EnvironmentError(\"OPENAI_API_KEY not set.\")\n",
    "    import openai\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Load market_dfdata\n",
    " = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df = pd.read_csv(\"nyt_aggregated_data.csv\", parse_dates=[\"Date\"])\n",
    "news_df.rename(columns={\"Summary\": \"summary\", \"Headline\": \"headline\", \"Sector\": \"sector\"}, inplace=True)\n",
    "\n",
    "# Aligning news dates with trading calendar\n",
    "trading_days = set(market_df[\"Date\"].dt.normalize())\n",
    "news_df = news_df[news_df[\"Date\"].dt.normalize().isin(trading_days)].reset_index(drop=True)\n",
    "\n",
    "# FinBERT model - ProsusAI\n",
    "FINBERT_MODEL = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)\n",
    "finbert.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def finbert_score(texts: List[str], batch_size: int = 32) -> Tuple[List[float], List[np.ndarray]]:\n",
    "    scores, probs_all = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "        out = finbert(**enc)\n",
    "        probs = torch.nn.functional.softmax(out.logits, dim=1).cpu().numpy()\n",
    "        batch_scores = probs[:,1] - probs[:,2]\n",
    "        scores.extend(batch_scores.tolist())\n",
    "        probs_all.extend(probs)\n",
    "    return scores, probs_all\n",
    "\n",
    "def gpt4_sentiment_single(text):\n",
    "    \"\"\"Call GPT-4 for sentiment fallback, return -1, 0, or 1.\"\"\"        \n",
    "    system_msg = \"You are a financial news analyst.\"\n",
    "    user_msg = (\n",
    "        \"Classify the sentiment of the given NY Times news article summary {summary}, which is closely related to the {sector} industry, as positive for buy, negative for sell, or neutral for hold position, for the US Stock market and provide the probability values for your classification.\"\n",
    "        \"Answer with just one word.\\n\\n\"\n",
    "        f\"Summary: \\\"{text}\\\"\"\n",
    "    )\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=OPENAI_MODEL,\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\", \"content\": user_msg}],\n",
    "            max_tokens=OPENAI_MAX_TOKENS,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        ans = response.choices[0].message.content.strip().lower()\n",
    "    except Exception as e:\n",
    "        print(\"GPT-4 error (treated as neutral):\", e)\n",
    "        return 0.0\n",
    "    if ans.startswith(\"pos\"):\n",
    "        return 1.0\n",
    "    if ans.startswith(\"neg\"):\n",
    "        return -1.0\n",
    "    return 0.0\n",
    "\n",
    "def add_sentiment(news_df, use_gpt4=False):\n",
    "    # Checkpoint 1: If FinBERT file exists, skip FinBERT and load\n",
    "    finbert_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\")\n",
    "    if os.path.exists(finbert_path):\n",
    "        print(\"Checkpoint: FinBERT sentiment file found. Loading and skipping FinBERT step.\")\n",
    "        news_df = pd.read_csv(finbert_path, parse_dates=[\"Date\"])\n",
    "    else:\n",
    "        print(\"Scoring FinBERT…\")\n",
    "        start_finbert = time.time()\n",
    "        fin_scores, fin_probs = finbert_score(news_df[\"summary\"].tolist())\n",
    "        end_finbert = time.time()\n",
    "        news_df[\"FinBERT_score\"] = fin_scores\n",
    "        news_df[\"FinBERT_prob_neu\"] = [p[0] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_pos\"] = [p[1] for p in fin_probs]\n",
    "        news_df[\"FinBERT_prob_neg\"] = [p[2] for p in fin_probs]\n",
    "        news_df[\"FinBERT_confidence\"] = news_df[\"FinBERT_score\"].abs()\n",
    "        news_df[\"Sentiment\"] = news_df[\"FinBERT_score\"]  # Start with FinBERT\n",
    "        print(f\"FinBERT sentiment scored in {end_finbert - start_finbert:.2f} seconds.\")\n",
    "        news_df.to_csv(finbert_path, index=False)\n",
    "        print(f\"Checkpoint: FinBERT sentiment saved to {finbert_path}\")\n",
    "\n",
    "    num_gpt4 = 0\n",
    "    gpt4_path = os.path.join(DERIVED_PATH, \"nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "\n",
    "    if use_gpt4:\n",
    "        # If checkpoint exists, load and skip fallback\n",
    "        if os.path.exists(gpt4_path):\n",
    "            print(\"Checkpoint: GPT-4 fallback file found. Loading and skipping fallback step.\")\n",
    "            news_df = pd.read_csv(gpt4_path, parse_dates=[\"Date\"])\n",
    "        else:\n",
    "            print(\"Running GPT-4 fallback…\")\n",
    "            low_conf_mask = news_df[\"FinBERT_confidence\"] < GPT4_CONFIDENCE_THRESHOLD\n",
    "            num_gpt4 = low_conf_mask.sum()\n",
    "            print(f\"News needing GPT-4 fallback: {num_gpt4} of {len(news_df)}\")\n",
    "            start_gpt4 = time.time()\n",
    "            checkpoint_counter = 0\n",
    "            for idx_num, idx in enumerate(news_df[low_conf_mask].index):\n",
    "                news_df.at[idx, \"Sentiment\"] = gpt4_sentiment_single(news_df.at[idx, \"summary\"])\n",
    "                checkpoint_counter += 1\n",
    "                # Intermediate checkpoint\n",
    "                if checkpoint_counter % GPT4_CHECKPOINT_EVERY == 0:\n",
    "                    news_df.to_csv(gpt4_path, index=False)\n",
    "                    print(f\"Checkpoint: Saved GPT-4 fallback at {checkpoint_counter} / {num_gpt4} GPT-4 calls.\")\n",
    "            end_gpt4 = time.time()\n",
    "            print(f\"GPT-4 fallback completed in {end_gpt4 - start_gpt4:.2f} seconds.\")\n",
    "            news_df.to_csv(gpt4_path, index=False)\n",
    "            print(f\"Checkpoint: Final GPT-4 fallback saved to {gpt4_path}\")\n",
    "    else:\n",
    "        num_gpt4 = 0\n",
    "\n",
    "    return news_df, num_gpt4\n",
    "   \n",
    "# Main Pipeline\n",
    "Path(DERIVED_PATH).mkdir(exist_ok=True)\n",
    "pipeline_start = time.time()\n",
    "news_df, num_gpt4 = add_sentiment(news_df, use_gpt4=USE_GPT4_FALLBACK)\n",
    "pipeline_end = time.time()\n",
    "print(f\"Total pipeline runtime: {pipeline_end - pipeline_start:.2f} seconds.\")\n",
    "\n",
    "# Daily sentiment aggregate\n",
    "sent_daily = (\n",
    "    news_df.groupby(news_df[\"Date\"].dt.normalize())[\"Sentiment\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "sent_daily_path = os.path.join(DERIVED_PATH, \"daily_sentiment_aggregate.csv\")\n",
    "sent_daily.to_csv(sent_daily_path, index=False)\n",
    "\n",
    "# Merge with market data for modeling (LSTM)\n",
    "market_merge = pd.merge(\n",
    "    market_df, \n",
    "    sent_daily, \n",
    "    left_on=market_df[\"Date\"].dt.normalize(), \n",
    "    right_on=sent_daily[\"Date\"].dt.normalize(), \n",
    "    how=\"left\", \n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_path = os.path.join(DERIVED_PATH, \"final_merged_for_lstm.csv\")\n",
    "market_merge.to_csv(final_out_path, index=False)\n",
    "print(f\"Checkpoint: Final merged LSTM-ready data saved to {final_out_path}\")\n",
    "\n",
    "print(f\"All CSVs saved to: {DERIVED_PATH}/\")\n",
    "print(\"Output files:\")\n",
    "print(\" - nyt_with_finbert_sentiment.csv\")\n",
    "print(\" - nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "print(\" - daily_sentiment_aggregate.csv\")\n",
    "print(\" - final_merged_for_lstm.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f24325",
   "metadata": {},
   "source": [
    "#### FinBERT-only Sentiment Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c20294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra: Create daily FinBERT-only sentiment aggregate\n",
    "finbert_only_path = os.path.join(DERIVED_PATH, \"nyt_with_finbert_sentiment.csv\")\n",
    "finbert_df = pd.read_csv(finbert_only_path, parse_dates=[\"Date\"])\n",
    "\n",
    "daily_finbert_sent = (\n",
    "    finbert_df.groupby(finbert_df[\"Date\"].dt.normalize())[\"FinBERT_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "daily_finbert_sent_path = os.path.join(DERIVED_PATH, \"daily_FinBERT_sentiment_aggregate.csv\")\n",
    "daily_finbert_sent.to_csv(daily_finbert_sent_path, index=False)\n",
    "\n",
    "# Merge with market data for modeling (LSTM), FinBERT-only version\n",
    "market_merge_finbert = pd.merge(\n",
    "    market_df,\n",
    "    daily_finbert_sent,\n",
    "    left_on=market_df[\"Date\"].dt.normalize(),\n",
    "    right_on=daily_finbert_sent[\"Date\"].dt.normalize(),\n",
    "    how=\"left\",\n",
    "    suffixes=('', '_sent')\n",
    ")\n",
    "final_out_finbert_path = os.path.join(DERIVED_PATH, \"final_merged_FinBERT_for_lstm.csv\")\n",
    "market_merge_finbert.to_csv(final_out_finbert_path, index=False)\n",
    "\n",
    "print(f\"Checkpoint: FinBERT-only daily sentiment aggregate saved to {daily_finbert_sent_path}\")\n",
    "print(f\"Checkpoint: Final merged FinBERT-only LSTM-ready data saved to {final_out_finbert_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a8035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DERIVED_PATH = 'derived'\n",
    "\n",
    "# Load market data (adjust path if needed)\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "market_df['Date'] = pd.to_datetime(market_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# FinBERT-only sentiment\n",
    "finbert_sent_df = pd.read_csv(os.path.join(DERIVED_PATH, \"daily_FinBERT_sentiment_aggregate.csv\"))\n",
    "finbert_sent_df['Date'] = pd.to_datetime(finbert_sent_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "merged_finbert = pd.merge(\n",
    "    market_df, finbert_sent_df, on='Date', how='left'\n",
    ")\n",
    "merged_finbert.to_csv(os.path.join(DERIVED_PATH, \"final_merged_FinBERT_for_lstm.csv\"), index=False)\n",
    "print(\"Checkpoint: FinBERT-only LSTM-ready data saved.\")\n",
    "\n",
    "# Hybrid (GPT-4 fallback) sentiment\n",
    "hybrid_sent_df = pd.read_csv(os.path.join(DERIVED_PATH, \"daily_sentiment_aggregate.csv\"))\n",
    "hybrid_sent_df['Date'] = pd.to_datetime(hybrid_sent_df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "merged_hybrid = pd.merge(\n",
    "    market_df, hybrid_sent_df, on='Date', how='left'\n",
    ")\n",
    "merged_hybrid.to_csv(os.path.join(DERIVED_PATH, \"final_merged_for_lstm.csv\"), index=False)\n",
    "print(\"Checkpoint: Hybrid LSTM-ready data saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6b23d",
   "metadata": {},
   "source": [
    "#Note: The OpenAI API usage as shown might need adaptation for actual GPT-4 (which might require using openai.ChatCompletion.create with messages in the new API format rather than the older Completion.create). But the idea stands – send the article text and get a sentiment label. The prompt here is simplified; in practice, one could include examples or a role specification for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79853b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"nyt_aggregated_data.csv\")\n",
    "\n",
    "# Count sector frequencies\n",
    "sector_counts = df['Sector'].value_counts()\n",
    "N = 12 \n",
    "top_sectors = sector_counts.index[:N]\n",
    "sizes = sector_counts.values[:N].tolist()\n",
    "labels = sector_counts.index[:N].tolist()\n",
    "\n",
    "# \"Other\" combines all the rest\n",
    "if len(sector_counts) > N:\n",
    "    other_size = sector_counts.values[N:].sum()\n",
    "    sizes.append(other_size)\n",
    "    labels.append(\"Other\")\n",
    "\n",
    "# Colors\n",
    "colors = plt.get_cmap('tab20').colors[:len(labels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    sizes,\n",
    "    labels=None,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=colors,\n",
    "    pctdistance=0.75,\n",
    "    wedgeprops=dict(width=0.38, edgecolor='w')\n",
    ")\n",
    "\n",
    "# Improved labeling\n",
    "bbox_props = dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"k\", lw=0.7)\n",
    "kw = dict(arrowprops=dict(arrowstyle=\"-\"), bbox=bbox_props, zorder=10, va=\"center\")\n",
    "\n",
    "# Finding the index of the \"Other\" label\n",
    "other_index = labels.index(\"Other\") if \"Other\" in labels else None\n",
    "\n",
    "for i, (p, label) in enumerate(zip(wedges, labels)):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "\n",
    "    if label == \"Other\":\n",
    "        label_x, label_y = -2.1, 0\n",
    "        horizontalalignment = \"right\"\n",
    "        \n",
    "        ax.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(label_x, label_y),\n",
    "            horizontalalignment=horizontalalignment,\n",
    "            fontsize=10,\n",
    "            arrowprops=dict(arrowstyle=\"-\", lw=1.8, color=\"gray\", connectionstyle=\"arc3,rad=0\"),\n",
    "            bbox=bbox_props, va=\"center\"\n",
    "        )\n",
    "    else:\n",
    "        label_x = 1.44 * np.sign(x)\n",
    "        label_y = 1.5 * y\n",
    "        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "        connectionstyle = f\"angle,angleA=0,angleB={ang}\"\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            xy=(x, y), xytext=(label_x, label_y),\n",
    "            horizontalalignment=horizontalalignment,\n",
    "            fontsize=12,\n",
    "            arrowprops=dict(arrowstyle=\"-\", lw=1.3, color=\"gray\", connectionstyle=connectionstyle),\n",
    "            bbox=bbox_props, va=\"center\"\n",
    "        )\n",
    "\n",
    "plt.setp(autotexts, size=10, color='black')\n",
    "plt.title('NYT Aggregated Data – Sector Distribution', fontsize=20, fontweight='bold', pad=25)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"nyt_sector_donut_final.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
