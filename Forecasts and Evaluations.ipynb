{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38d3f9f",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703041ab",
   "metadata": {},
   "source": [
    "#### Using a Python 3.11 Kernel though a Virtual Conda Enviornment, packages must me downloaded thourgh Conda before running in this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel\n",
    "!pip install numpy pandas scikit-learn tensorflow keras matplotlib\n",
    "!pip install --upgrade notebook\n",
    "!pip install python-dotenv\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install threadpoolctl\n",
    "!pip install tensorflow\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn-intelex\n",
    "!pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a72024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4ed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Article-level sentiment data:\n",
    "finbert_articles = pd.read_csv(\"derived/nyt_with_finbert_sentiment.csv\")\n",
    "hybrid_articles = pd.read_csv(\"derived/nyt_with_gpt4_fallback_sentiment.csv\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(finbert_articles[\"FinBERT_score\"], bins=30, color=\"orange\", label=\"FinBERT-only\", kde=True, stat=\"density\", alpha=0.55)\n",
    "sns.histplot(hybrid_articles[\"Sentiment\"], bins=30, color=\"red\", label=\"Hybrid (FinBERT+GPT-4)\", kde=True, stat=\"density\", alpha=0.55)\n",
    "\n",
    "plt.title(\"Overlayed Distribution of Article Sentiment Scores\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb15b5e8",
   "metadata": {},
   "source": [
    "### FinBERT+GPT-4 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Cleaning Up Columns\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns])\n",
    "\n",
    "# Ensuring all columns except Date are numeric\n",
    "for col in df.columns:\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Ensuring only meaningful features are used\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "# Adding lagged features below\n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Creating LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluating\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "# Directional accuracy\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "# Aligning index properly\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef75b5",
   "metadata": {},
   "source": [
    "### FinBERT LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a03b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Cleaning columns \n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns])\n",
    "\n",
    "# Remove column headers like '^GSPC' in Close/Open/etc (strip out non-numeric entries)\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "# Ensure only 'Date' is not numeric\n",
    "for col in df.columns:\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering: Adding lags for target & sentiment \n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2021, 12, 31)\n",
    "VAL_END = date(2022, 12, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scaling Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # align with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a6092",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95479857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for name in [\"derived/lstm_test_predictions.csv\",\n",
    "             \"derived/lstm_FinBERT_only_test_predictions.csv\"]:\n",
    "    df = pd.read_csv(name, parse_dates=[\"Date\"])\n",
    "    dup = df.duplicated(subset=[\"Date\"]).sum()\n",
    "    print(name, \"rows:\", len(df), \"duplicates:\", dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05241e8",
   "metadata": {},
   "source": [
    "## Forecast Bias Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706cded1",
   "metadata": {},
   "source": [
    "### Biasness Test for Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e005007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = (\n",
    "    df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]]\n",
    "        .rename(columns={\"Return\": \"Market_Return\", \"Predicted_Return\": \"LSTM_GPT4\"})\n",
    "        .merge(\n",
    "            df_fbert[[\"Date\", \"Predicted_Return\"]].rename(columns={\"Predicted_Return\": \"LSTM_FinBERT\"}),\n",
    "            on=\"Date\", how=\"inner\"\n",
    "        )\n",
    "        .merge(\n",
    "            market[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}),\n",
    "            on=\"Date\", how=\"inner\"\n",
    "        )\n",
    "        .dropna(subset=[\"Market_True_Return\", \"LSTM_GPT4\", \"LSTM_FinBERT\"])\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Arrays\n",
    "y_true   = df[\"Market_True_Return\"].values\n",
    "pred_gpt = df[\"LSTM_GPT4\"].values\n",
    "pred_fb  = df[\"LSTM_FinBERT\"].values\n",
    "\n",
    "# Forecasting R² function (explained variance)\n",
    "def forecast_r2(y, yhat):\n",
    "    ss_res = np.sum((y - yhat) ** 2)\n",
    "    ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "# Metric aggregation\n",
    "def signal_metrics(y, yhat):\n",
    "    mse  = np.mean((y - yhat) ** 2)\n",
    "    mae  = np.mean(np.abs(y - yhat))\n",
    "    acc  = np.mean(np.sign(y) == np.sign(yhat))\n",
    "    strat_ret = np.where(yhat > 0, 1, -1) * y\n",
    "    sharpe = np.mean(strat_ret) / (np.std(strat_ret) + 1e-9) * np.sqrt(252)\n",
    "    r2 = forecast_r2(y, yhat)\n",
    "    return dict(MSE=mse, MAE=mae, Directional_Acc=acc, Sharpe=sharpe, R2=r2)\n",
    "\n",
    "m_gpt = signal_metrics(y_true, pred_gpt)\n",
    "m_fb  = signal_metrics(y_true, pred_fb)\n",
    "\n",
    "# Diebold–Mariano test (squared-error loss)\n",
    "def dm_test(e1, e2):\n",
    "    d = e1 - e2\n",
    "    T = len(d)\n",
    "    d_mean = np.mean(d)\n",
    "    d_var = np.var(d, ddof=1)\n",
    "    dm_stat = d_mean / np.sqrt(d_var / T)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "    return dm_stat, p_value\n",
    "\n",
    "dm_stat, dm_p = dm_test((y_true - pred_gpt) ** 2, (y_true - pred_fb) ** 2)\n",
    "\n",
    "# Regime-dependent Sharpe (split by VIX median)\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "vix_med = df[\"VIX\"].median()\n",
    "reg = {}\n",
    "for regime_mask, label in [(df[\"VIX\"] > vix_med, \"High-VIX\"), (df[\"VIX\"] <= vix_med, \"Low-VIX\")]:\n",
    "    idx = regime_mask.values\n",
    "    reg[label] = dict(\n",
    "        GPT4=signal_metrics(y_true[idx], pred_gpt[idx])[\"Sharpe\"],\n",
    "        FinBERT=signal_metrics(y_true[idx], pred_fb[idx])[\"Sharpe\"]\n",
    "    )\n",
    "\n",
    "# Forecast-bias diagnostics (optional, still useful for regression analysis)\n",
    "def bias_tests(y, yhat, label):\n",
    "    err = y - yhat\n",
    "    t, p = stats.ttest_1samp(err, 0.0)\n",
    "    X = sm.add_constant(yhat)\n",
    "    mz = sm.OLS(y, X).fit()\n",
    "    R = np.eye(2)\n",
    "    q = np.array([0, 1])\n",
    "    ftest = mz.f_test((R, q))\n",
    "    return dict(\n",
    "        Model=label,\n",
    "        Mean_Error=err.mean(),\n",
    "        MeanErr_tstat=t,\n",
    "        MeanErr_pval=p,\n",
    "        MZ_alpha=mz.params[0],\n",
    "        MZ_beta=mz.params[1],\n",
    "        MZ_alpha_p=mz.pvalues[0],\n",
    "        MZ_beta_p=mz.pvalues[1],\n",
    "        MZ_F_pvalue=float(ftest.pvalue)\n",
    "    )\n",
    "\n",
    "bias_gpt = bias_tests(y_true, pred_gpt, \"Hybrid (GPT-4)\")\n",
    "bias_fb = bias_tests(y_true, pred_fb, \"FinBERT-only\")\n",
    "\n",
    "print(\"\\n### Forecast-Evaluation Metrics\")\n",
    "eval_tbl = pd.DataFrame([m_gpt, m_fb], index=[\"Hybrid\", \"FinBERT\"])\n",
    "print(tabulate(eval_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "print(\"\\nDiebold–Mariano statistic: {:.3f}   p-value: {:.3f}\".format(dm_stat, dm_p))\n",
    "\n",
    "print(\"\\n### Regime-dependent Sharpe\")\n",
    "print(tabulate(pd.DataFrame(reg).T, headers=\"keys\", tablefmt=\"github\", floatfmt=\".3f\"))\n",
    "\n",
    "bias_tbl = pd.DataFrame([bias_gpt, bias_fb]).set_index(\"Model\")\n",
    "print(\"\\n### Forecast-Bias Diagnostics\")\n",
    "print(tabulate(bias_tbl, headers=\"keys\", tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "# Validation accuracy for both models (directional up/down accuracy)\n",
    "print(\"\\n### Validation Accuracy\")\n",
    "val_acc_gpt = accuracy_score((df[\"Market_True_Return\"].values > 0), (df[\"LSTM_GPT4\"].values > 0))\n",
    "val_acc_fb = accuracy_score((df[\"Market_True_Return\"].values > 0), (df[\"LSTM_FinBERT\"].values > 0))\n",
    "print(f\"Hybrid (GPT-4) Validation Accuracy:  {val_acc_gpt:.2%}\")\n",
    "print(f\"FinBERT-only Validation Accuracy:    {val_acc_fb:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36188a3",
   "metadata": {},
   "source": [
    "## Result Comparison and Visualizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score, precision_score,\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, r2_score\n",
    ")\n",
    "\n",
    "fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "gpt4 = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# If any columns are objects (str) = to numeric for math/plotting\n",
    "for df in [fbert, gpt4]:\n",
    "    for col in df.columns:\n",
    "        if col != \"Date\":\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "# General settings\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set(font_scale=1.2)\n",
    "window = 21  # 1 month rolling\n",
    "\n",
    "# Mergeing for side-by-side plots\n",
    "df = fbert[[\"Date\", \"Return\", \"VIX\", \"FinBERT_score\", \"Predicted_Return\"]].copy()\n",
    "df = df.rename(columns={\"Predicted_Return\": \"FinBERT_Pred\"})\n",
    "df[\"GPT4_Pred\"] = gpt4[\"Predicted_Return\"].values\n",
    "df[\"GPT4_Sentiment\"] = gpt4[\"Sentiment\"].values if \"Sentiment\" in gpt4 else np.nan\n",
    "\n",
    "# Actual vs LSTM Forecasts: Time Series Overlap\n",
    "# Shows daily and cumulative return performance for market, FinBERT-only, and FinBERT+GPT-4 models, which model tracks or outperforms the market over time?\n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"], color='black', label='Market Return', linewidth=1, linestyle='--')\n",
    "plt.plot(df[\"Date\"], df[\"FinBERT_Pred\"], color='orange', label='FinBERT-only LSTM', alpha=0.8)\n",
    "plt.plot(df[\"Date\"], df[\"GPT4_Pred\"], color='red', label='FinBERT+GPT-4 LSTM', alpha=0.7)\n",
    "plt.ylabel(\"Return\")\n",
    "plt.title(\"Market Returns vs LSTM Model Forecasts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model Error: Residuals Over Time \n",
    "# Shows model residuals, identifying periods where either model systematically overshoots or undershoots\n",
    "plt.figure(figsize=(17,7))\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"FinBERT_Pred\"], label=\"Error: FinBERT LSTM\", color=\"orange\", alpha=0.6)\n",
    "plt.plot(df[\"Date\"], df[\"Return\"] - df[\"GPT4_Pred\"], label=\"Error: GPT-4 LSTM\", color=\"red\", alpha=0.6)\n",
    "plt.axhline(0, color=\"black\", linewidth=1, linestyle=\":\")\n",
    "plt.ylabel(\"Residual (Error)\")\n",
    "plt.title(\"Model Residuals: Market - Model Forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Forecasts: Histogram & KDE \n",
    "# Distribution and density of returns/forecasts to see if model returns are over/under-dispersed\n",
    "plt.figure(figsize=(14,5))\n",
    "sns.histplot(df[\"Return\"], label=\"Market Return\", color=\"black\", kde=True, stat=\"density\", bins=40)\n",
    "sns.histplot(df[\"FinBERT_Pred\"], label=\"FinBERT LSTM\", color=\"orange\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "sns.histplot(df[\"GPT4_Pred\"], label=\"GPT-4 LSTM\", color=\"dodgerblue\", kde=True, stat=\"density\", bins=40, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Market Returns vs LSTM Model Forecasts\")\n",
    "plt.show()\n",
    "\n",
    "# Actual vs Predicted: Scatter Plots and Regression Fit\n",
    "# Scatter/regression of predicted vs actual, with R² value (not shown but you can print it), to test linear forecast strength\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16,6), sharey=True)\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"FinBERT_Pred\"], ax=axs[0], line_kws={\"color\": \"orange\"})\n",
    "axs[0].set_title(\"FinBERT LSTM: Actual vs Predicted\")\n",
    "axs[0].set_xlabel(\"Actual Return\")\n",
    "axs[0].set_ylabel(\"Predicted Return\")\n",
    "sns.regplot(x=df[\"Return\"], y=df[\"GPT4_Pred\"], ax=axs[1], line_kws={\"color\": \"dodgerblue\"})\n",
    "axs[1].set_title(\"GPT-4 LSTM: Actual vs Predicted\")\n",
    "axs[1].set_xlabel(\"Actual Return\")\n",
    "plt.show()\n",
    "\n",
    "# Rolling Model RMSE (21d window)\n",
    "# Rolling RMSE: when is a model more/less accurate? Is one model consistently better?\n",
    "rmse_fbert = (df[\"Return\"] - df[\"FinBERT_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "rmse_gpt4 = (df[\"Return\"] - df[\"GPT4_Pred\"]).rolling(window).apply(lambda x: np.sqrt(np.mean(x**2)))\n",
    "plt.figure(figsize=(17,6))\n",
    "plt.plot(df[\"Date\"], rmse_fbert, label=\"Rolling RMSE: FinBERT LSTM\", color=\"orange\")\n",
    "plt.plot(df[\"Date\"], rmse_gpt4, label=\"Rolling RMSE: GPT-4 LSTM\", color=\"dodgerblue\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"21-Day Rolling RMSE: Model Performance Over Time\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    roc_curve, roc_auc_score\n",
    ")\n",
    "\n",
    "# Aligning both models rows\n",
    "roc_df = df.dropna(subset=[\"Return\", \"FinBERT_Pred\", \"GPT4_Pred\"]).copy()\n",
    "\n",
    "# Directional Classification: Up/Down Day target\n",
    "y_true        = (roc_df[\"Return\"].values > 0).astype(int)   # 1 = up-day\n",
    "score_fbert   = roc_df[\"FinBERT_Pred\"].values               # signed predictions\n",
    "score_gpt4    = roc_df[\"GPT4_Pred\"].values\n",
    "\n",
    "# Base precision for GPT-4 sign-only model\n",
    "gpt_sign      = (score_gpt4 > 0).astype(int)           # 1 if model predicts “up”\n",
    "base_prec_gpt = precision_score(y_true, gpt_sign)     # precision at threshold 0\n",
    "base_rec_gpt  = recall_score   (y_true, gpt_sign)     # recall at threshold 0\n",
    "\n",
    "# Precision-Recall Curve: How well does prediction magnitude separate correct from incorrect sign?\n",
    "prec_fbert, rec_fbert, _ = precision_recall_curve(y_true, score_fbert)\n",
    "prec_gpt4, rec_gpt4, _   = precision_recall_curve(y_true, score_gpt4)\n",
    "ap_fbert = average_precision_score(y_true, score_fbert)\n",
    "ap_gpt4  = average_precision_score(y_true, score_gpt4)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(rec_fbert, prec_fbert, label=f\"FinBERT LSTM (AP = {ap_fbert:.2f})\",  alpha=0.9,  color=\"orange\")\n",
    "plt.plot(rec_gpt4,  prec_gpt4, label=f\"GPT-4 LSTM (AP = {ap_gpt4:.2f})\",     alpha=0.9,  color=\"dodgerblue\")\n",
    "plt.hlines(y_true.mean(), 0, 1, linestyle=\"--\", color=\"grey\",\n",
    "           label=\"Random baseline (π = P(up))\")\n",
    "plt.hlines(base_prec_gpt, 0, 1,\n",
    "           linestyle=\":\",  color=\"red\",\n",
    "           label=f\"GPT-4 sign-only baseline (precision = {base_prec_gpt:.2f})\")\n",
    "\n",
    "# Marking the exact (recall, precision) point of the sign-only model\n",
    "plt.plot(base_rec_gpt, base_prec_gpt, marker=\"o\", color=\"red\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve: Direction Classification\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve:\n",
    "fpr_fbert, tpr_fbert, _ = roc_curve(y_true, score_fbert)\n",
    "fpr_gpt4,  tpr_gpt4,  _ = roc_curve(y_true, score_gpt4)\n",
    "auc_fbert = roc_auc_score(y_true, score_fbert)\n",
    "auc_gpt4  = roc_auc_score(y_true, score_gpt4)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_fbert, tpr_fbert,\n",
    "         label=f\"FinBERT LSTM (AUC = {auc_fbert:.2f})\",  alpha=0.9, color=\"orange\")\n",
    "plt.plot(fpr_gpt4,  tpr_gpt4,\n",
    "         label=f\"GPT-4 LSTM (AUC = {auc_gpt4:.2f})\", alpha=0.9, color=\"dodgerblue\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', label=\"Random classifier\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve: Direction Classification\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance (Permutation, quick-and-dirty)\n",
    "# Feature importance via Random Forest, as a check on which signals the market return is most related to\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Quick look at which features most drive market returns, for context\n",
    "X = df[[col for col in df.columns if col not in ['Date', 'Return', 'FinBERT_Pred', 'GPT4_Pred']]]\n",
    "rf = RandomForestRegressor(n_estimators=50)\n",
    "rf.fit(X, df[\"Return\"])\n",
    "perm = permutation_importance(rf, X, df[\"Return\"], n_repeats=10)\n",
    "importances = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(12,6))\n",
    "importances.plot(kind='bar')\n",
    "plt.ylabel(\"Permutation Importance\")\n",
    "plt.title(\"Feature Importance for Market Return (Random Forest Proxy)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot the two autocorrelation plots inside the same figure\n",
    "# define autocorrelation plot function\n",
    "def autocorrelation_plot(series, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    import statsmodels.api as sm\n",
    "    sm.graphics.tsa.plot_acf(series, lags=40, ax=ax)\n",
    "    ax.set_xlabel(\"Lags\")\n",
    "    ax.set_ylabel(\"Autocorrelation\")\n",
    "    return ax\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"FinBERT_Pred\"], ax=axs[0])\n",
    "axs[0].set_title(\"Error Autocorrelation: FinBERT LSTM\")\n",
    "autocorrelation_plot(df[\"Return\"] - df[\"GPT4_Pred\"], ax=axs[1])\n",
    "axs[1].set_title(\"Error Autocorrelation: GPT-4 LSTM\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# vix volatility plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df[\"Date\"], df[\"VIX\"], label=\"VIX\", color=\"purple\")\n",
    "plt.title(\"VIX Volatility Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"VIX\")\n",
    "plt.legend()    \n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b7444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Merge on Date\n",
    "df = df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]].rename(\n",
    "    columns={\"Return\": \"Market_Return\", \"Predicted_Return\": \"LSTM_GPT4\"}\n",
    ").merge(\n",
    "    df_fbert[[\"Date\", \"Predicted_Return\"]].rename(\n",
    "        columns={\"Predicted_Return\": \"LSTM_FinBERT\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"inner\"\n",
    ").merge(\n",
    "    market_df[[\"Date\", \"Return\", \"VIX\"]].rename(\n",
    "        columns={\"Return\": \"Market_True_Return\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"left\"\n",
    ")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Signal-based returns\n",
    "def trading_signal_returns(true_returns, predicted_returns):\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "# Compounded cumulative return (not just cumsum)\n",
    "def compound_cumret(returns):\n",
    "    return np.cumprod(1 + returns) - 1\n",
    "\n",
    "# Masks for valid rows (overlapping dates)\n",
    "mask = df[\"Market_True_Return\"].notnull() & df[\"LSTM_GPT4\"].notnull() & df[\"LSTM_FinBERT\"].notnull()\n",
    "if not mask.any():\n",
    "    raise ValueError(\"No valid rows after masking. Check merge.\")\n",
    "\n",
    "df_valid = df[mask].copy().reset_index(drop=True)\n",
    "\n",
    "# Strategy returns\n",
    "df_valid[\"LSTM_GPT4_Strategy\"] = trading_signal_returns(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_GPT4\"])\n",
    "df_valid[\"FinBERT_Strategy\"] = trading_signal_returns(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_FinBERT\"])\n",
    "\n",
    "# Cumulative compounded returns\n",
    "df_valid[\"Market_CumReturn\"] = compound_cumret(df_valid[\"Market_True_Return\"])\n",
    "df_valid[\"LSTM_GPT4_CumReturn\"] = compound_cumret(df_valid[\"LSTM_GPT4_Strategy\"])\n",
    "df_valid[\"FinBERT_CumReturn\"] = compound_cumret(df_valid[\"FinBERT_Strategy\"])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"Market_CumReturn\"], label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"LSTM_GPT4_CumReturn\"], label=\"Hybrid LSTM-GPT4 Cumulative Return\", color='red')\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"FinBERT_CumReturn\"], label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "\n",
    "# Add shaded areas\n",
    "start1 = pd.to_datetime(\"2023-06-01\")\n",
    "end1 = pd.to_datetime(\"2023-07-01\")\n",
    "start2 = pd.to_datetime(\"2023-11-15\")\n",
    "end2 = pd.to_datetime(\"2023-12-01\")\n",
    "start3 = pd.to_datetime(\"2023-03-28\")\n",
    "end3 = pd.to_datetime(\"2023-04-01\")\n",
    "plt.axvspan(start1, end1, color='grey', alpha=0.15, label='Strong Divergence Period')\n",
    "plt.axvspan(start2, end2, color='grey', alpha=0.15)\n",
    "plt.axvspan(start3, end3, color='grey', alpha=0.15)\n",
    "\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.4, color='black', alpha=0.7)\n",
    "plt.axhline(0, color='black', linestyle='-', linewidth=0.9)\n",
    "plt.gca().set_facecolor(\"#f0f0f0e1\")\n",
    "plt.title(\"Cumulative Compounded Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return (Compounded)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc3530d",
   "metadata": {},
   "source": [
    "### Cumulative Compounding Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdd4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Merge on Date\n",
    "df = df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]].rename(\n",
    "    columns={\"Return\": \"Market_Return\", \"Predicted_Return\": \"LSTM_GPT4\"}\n",
    ").merge(\n",
    "    df_fbert[[\"Date\", \"Predicted_Return\"]].rename(\n",
    "        columns={\"Predicted_Return\": \"LSTM_FinBERT\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"inner\"\n",
    ").merge(\n",
    "    market_df[[\"Date\", \"Return\", \"VIX\"]].rename(\n",
    "        columns={\"Return\": \"Market_True_Return\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"left\"\n",
    ")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Computing daily returns for each strategy\n",
    "# Hybrid strategy: go long if LSTM_GPT4 > 0, short if < 0, flat if == 0\n",
    "df['Hybrid_Strategy_Return'] = np.sign(df['LSTM_GPT4']) * df['Market_True_Return']\n",
    "# FinBERT-only strategy\n",
    "df['FinBERT_Strategy_Return'] = np.sign(df['LSTM_FinBERT']) * df['Market_True_Return']\n",
    "# Buy-and-hold: just the market return itself\n",
    "df['Market_Strategy_Return'] = df['Market_True_Return']\n",
    "\n",
    "# Computing cumulative returns (wealth paths)\n",
    "def to_cum_wealth(returns):\n",
    "    return np.cumprod(1 + returns)\n",
    "\n",
    "df['Hybrid_Wealth'] = to_cum_wealth(df['Hybrid_Strategy_Return'])\n",
    "df['FinBERT_Wealth'] = to_cum_wealth(df['FinBERT_Strategy_Return'])\n",
    "df['Market_Wealth'] = to_cum_wealth(df['Market_Strategy_Return'])\n",
    "\n",
    "# Final values and cumulative returns\n",
    "start_value = 1.0\n",
    "hybrid_final = df['Hybrid_Wealth'].iloc[-1]\n",
    "finbert_final = df['FinBERT_Wealth'].iloc[-1]\n",
    "market_final = df['Market_Wealth'].iloc[-1]\n",
    "\n",
    "hybrid_cum_return = (hybrid_final - start_value) / start_value * 100\n",
    "finbert_cum_return = (finbert_final - start_value) / start_value * 100\n",
    "market_cum_return = (market_final - start_value) / start_value * 100\n",
    "\n",
    "# Print formatted results for your paragraph\n",
    "print(f\"Hybrid strategy’s final wealth: ${hybrid_final:.2f} (cumulative return: {hybrid_cum_return:.1f}%)\")\n",
    "print(f\"S&P 500 (buy-and-hold) final wealth: ${market_final:.2f} (cumulative return: {market_cum_return:.1f}%)\")\n",
    "print(f\"FinBERT-only strategy final wealth: ${finbert_final:.2f} (cumulative return: {finbert_cum_return:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def compounded_cum_return(returns):\n",
    "    \"\"\"Compute compounded cumulative return time series.\"\"\"\n",
    "    return pd.Series(np.cumprod(1 + returns) - 1)\n",
    "\n",
    "def compute_all_metrics(y_true, y_pred):\n",
    "    # Regression metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    # Classification (directional)\n",
    "    y_true_dir = (y_true > 0).astype(int)\n",
    "    y_pred_dir = (y_pred > 0).astype(int)\n",
    "    accuracy = accuracy_score(y_true_dir, y_pred_dir)\n",
    "    precision = precision_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "    recall = recall_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "    f1 = f1_score(y_true_dir, y_pred_dir, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true_dir, y_pred_dir)\n",
    "    # Strategy returns\n",
    "    signal = np.where(y_pred > 0, 1, -1)\n",
    "    daily_pnl = signal * y_true\n",
    "    rolling_cum = compounded_cum_return(daily_pnl)\n",
    "    sharpe = np.mean(daily_pnl) / (np.std(daily_pnl) + 1e-9) * np.sqrt(252)\n",
    "    # Max drawdown (from compounded cumulative return)\n",
    "    equity = rolling_cum + 1\n",
    "    running_max = np.maximum.accumulate(equity)\n",
    "    drawdown = running_max - equity\n",
    "    max_dd = drawdown.max()\n",
    "    return {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "        \"ROC AUC\": roc_auc,\n",
    "        \"Rolling_CumReturn\": rolling_cum,\n",
    "        \"Sharpe\": sharpe,\n",
    "        \"Max_Drawdown\": max_dd,\n",
    "        \"Daily_PnL\": daily_pnl,\n",
    "        \"Signal\": signal\n",
    "    }\n",
    "\n",
    "def rolling_directional_acc(true_ret, pred_ret, win=63):\n",
    "    \"\"\"Percentage of days inside a rolling window where sign(pred)==sign(true).\"\"\"\n",
    "    hit = (np.sign(true_ret) == np.sign(pred_ret)).astype(int)\n",
    "    return pd.Series(hit).rolling(win, min_periods=1).mean() * 100\n",
    "\n",
    "def rolling_sharpe(returns, win=63):\n",
    "    \"\"\"Rolling annualized Sharpe ratio over a specified window.\"\"\"\n",
    "    returns = pd.Series(returns)\n",
    "    mean_ret = returns.rolling(win).mean()\n",
    "    std_ret  = returns.rolling(win).std(ddof=0)\n",
    "    return (mean_ret / (std_ret + 1e-9)) * np.sqrt(252)\n",
    "\n",
    "def plot_conf_mat(y_true, y_pred, title, ax):\n",
    "    cm = confusion_matrix(y_true > 0, y_pred > 0)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Down\", \"Up\"], yticklabels=[\"Down\", \"Up\"], ax=ax)\n",
    "    ax.set_xlabel(\"Model Signal\")\n",
    "    ax.set_ylabel(\"Market Direction\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "def turnover(signal):\n",
    "    \"\"\"Average fraction of days where direction flips.\"\"\"\n",
    "    return (np.abs(np.diff(signal)) / 2).mean()\n",
    "\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Merging and Align Data by Date\n",
    "df = pd.DataFrame({\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "\n",
    "# Using \"Return\" from market_df as the ground truth for all performance metrics\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Apply a joint mask: all series must be valid\n",
    "mask = df[\"Market_True_Return\"].notnull() & df[\"LSTM_GPT4\"].notnull() & df[\"LSTM_FinBERT\"].notnull()\n",
    "df_valid = df[mask].copy().reset_index(drop=True)\n",
    "print(f\"\\nValid rows for all models: {len(df_valid)}\")\n",
    "\n",
    "# Metrics and Cumulative Return Series\n",
    "\n",
    "metrics_gpt = compute_all_metrics(df_valid[\"Market_True_Return\"].values, df_valid[\"LSTM_GPT4\"].values)\n",
    "metrics_finbert = compute_all_metrics(df_valid[\"Market_True_Return\"].values, df_valid[\"LSTM_FinBERT\"].values)\n",
    "market_cum = compounded_cum_return(df_valid[\"Market_True_Return\"].values)\n",
    "\n",
    "# Rolling Directional Accuracy and Sharpe\n",
    "\n",
    "win = 63\n",
    "mkt_true = df_valid[\"Market_True_Return\"].values\n",
    "pred_gpt = df_valid[\"LSTM_GPT4\"].values\n",
    "pred_fbt = df_valid[\"LSTM_FinBERT\"].values\n",
    "dates_all = df_valid[\"Date\"].values\n",
    "\n",
    "da_gpt   = rolling_directional_acc(mkt_true, pred_gpt, win)\n",
    "da_fbt   = rolling_directional_acc(mkt_true, pred_fbt, win)\n",
    "sharpe_gpt   = rolling_sharpe(metrics_gpt[\"Daily_PnL\"], win)\n",
    "sharpe_fbert = rolling_sharpe(metrics_finbert[\"Daily_PnL\"], win)\n",
    "\n",
    "# Trim first (win-1) observations\n",
    "trim = win - 1\n",
    "dates_trim = dates_all[trim:]\n",
    "sharpe_gpt_trim = sharpe_gpt.iloc[trim:]\n",
    "sharpe_fbt_trim = sharpe_fbert.iloc[trim:]\n",
    "da_gpt_trim = da_gpt.iloc[trim:]\n",
    "da_fbt_trim = da_fbt.iloc[trim:]\n",
    "\n",
    "# Plot Rolling Sharpe Ratio\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(dates_trim, sharpe_gpt_trim, label=\"Hybrid rolling Sharpe\",  c=\"red\")\n",
    "plt.plot(dates_trim, sharpe_fbt_trim, label=\"FinBERT rolling Sharpe\", c=\"darkorange\")\n",
    "plt.axhline(0, ls='--', c='k')\n",
    "plt.title(f\"Rolling Sharpe Ratio ({win}-day Window)\")\n",
    "plt.ylabel(\"Sharpe Ratio (annualized)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Rolling Directional Accuracy\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(dates_trim, da_gpt_trim, label=\"Hybrid directional accuracy\",  c=\"red\")\n",
    "plt.plot(dates_trim, da_fbt_trim, label=\"FinBERT directional accuracy\", c=\"darkorange\")\n",
    "plt.axhline(50, ls='--', c='k', lw=0.8)       # coin-flip baseline\n",
    "plt.ylim(0, 100)\n",
    "plt.title(f\"Rolling Directional Accuracy ({win}-day Window)\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1,2,figsize=(10,4))\n",
    "plot_conf_mat(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_GPT4\"], \"Hybrid\", axes[0])\n",
    "plot_conf_mat(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_FinBERT\"], \"FinBERT-only\", axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Turnover & Net-Long Ratio\n",
    "sig_gpt   = metrics_gpt[\"Signal\"]\n",
    "sig_fbert = metrics_finbert[\"Signal\"]\n",
    "turn = [turnover(sig_gpt), turnover(sig_fbert)]\n",
    "netL = [(sig_gpt == 1).mean(), (sig_fbert == 1).mean()]   # net-long ratios\n",
    "\n",
    "labels = [\"Hybrid\\n(FinBERT+GPT-4)\", \"FinBERT-only\"]\n",
    "x = np.arange(len(labels))\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x - width/2, turn, width, color=\"steelblue\", alpha=.85, label=\"Turnover\")\n",
    "ax1.set_ylabel(\"Turnover (% of days)\", color=\"steelblue\")\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, fontsize=11)\n",
    "ax1.tick_params(axis='y', labelcolor=\"steelblue\")\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(x + width/2, netL, width, color=\"darkorange\", alpha=.85, label=\"Net-Long Ratio\")\n",
    "ax2.set_ylabel(\"Net-Long Exposure\", color=\"darkorange\")\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkorange\")\n",
    "for rect in list(bars1) + list(bars2):\n",
    "    height = rect.get_height()\n",
    "    ax = ax1 if rect in bars1 else ax2\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 0.02,\n",
    "            f\"{height:.2%}\", ha='center', va='bottom', fontsize=9)\n",
    "plt.title(\"Turnover vs Net-Long Ratio\", pad=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49cb37c",
   "metadata": {},
   "source": [
    "## Comparative Measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08de7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Diebold-Mariano test function\n",
    "def diebold_mariano_test(e1, e2, h=1, alternative='two-sided'):\n",
    "    \"\"\"\n",
    "    Diebold-Mariano test for equal predictive accuracy.\n",
    "    e1, e2: forecast errors (arrays)\n",
    "    h: forecast horizon (1 for one-step ahead)\n",
    "    alternative: 'two-sided', 'less', 'greater'\n",
    "    \"\"\"\n",
    "    d = e1 - e2\n",
    "    mean_d = np.mean(d)\n",
    "    T = len(d)\n",
    "    # Newey-West variance estimator with lag h-1\n",
    "    gamma = [np.cov(d[:-lag], d[lag:])[0,1] if lag != 0 else np.var(d, ddof=1) for lag in range(h)]\n",
    "    V_d = gamma[0] + 2 * np.sum(gamma[1:])\n",
    "    dm_stat = mean_d / np.sqrt(V_d / T)\n",
    "    if alternative == 'two-sided':\n",
    "        p_value = 2 * (1 - norm.cdf(np.abs(dm_stat)))\n",
    "    elif alternative == 'less':\n",
    "        p_value = norm.cdf(dm_stat)\n",
    "    else:\n",
    "        p_value = 1 - norm.cdf(dm_stat)\n",
    "    return dm_stat, p_value\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Signal-based trading returns\n",
    "def trading_signal_returns(true_returns, predicted_returns):\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "# Computing all metrics for each model (with rolling mean/std)\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60):\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "# Valid data masks and calculate for both models\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull()\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "# Tables for thesis (including rolling mean/std)\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"Model Comparison Table\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "# Confusion matrices for appendix\n",
    "print(\"Confusion Matrices\")\n",
    "print(\"Hybrid (FinBERT+GPT-4):\\n\", metrics_gpt[\"Confusion_Matrix\"])\n",
    "print(\"FinBERT-only:\\n\", metrics_finbert[\"Confusion_Matrix\"])\n",
    "np.savetxt(\"hybrid_confusion_matrix.csv\", metrics_gpt[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "np.savetxt(\"finbert_confusion_matrix.csv\", metrics_finbert[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "\n",
    "# Diebold-Mariano test (MSE loss differential)\n",
    "e1 = (df.loc[mask_gpt, \"Market_True_Return\"].values - df.loc[mask_gpt, \"LSTM_GPT4\"].values) ** 2\n",
    "e2 = (df.loc[mask_fbert, \"Market_True_Return\"].values - df.loc[mask_fbert, \"LSTM_FinBERT\"].values) ** 2\n",
    "dm_stat, dm_pvalue = diebold_mariano_test(e1, e2, h=1, alternative='two-sided')\n",
    "print(\"Diebold-Mariano Test\")\n",
    "print(f\"DM statistic: {dm_stat:.3f}, p-value: {dm_pvalue:.3g}\")\n",
    "\n",
    "# Regime split (by VIX) show model dominance in high/low volatility\n",
    "vix_median = df[\"VIX\"].median()\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime ===\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])\n",
    "\n",
    "# Distribution plots of strategy returns (for tail risk/outlier events)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(metrics_gpt['Signal_Returns'], bins=50, kde=True, color=\"blue\", stat=\"density\", label=\"Hybrid\")\n",
    "sns.histplot(metrics_finbert['Signal_Returns'], bins=50, kde=True, color=\"orange\", stat=\"density\", label=\"FinBERT-only\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Strategy Returns\")\n",
    "plt.xlabel(\"Strategy Return per Period\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nHybrid strategy return quantiles:\\n\", pd.Series(metrics_gpt[\"Signal_Returns\"]).quantile([.01, .05, .95, .99]))\n",
    "print(\"\\nFinBERT-only strategy return quantiles:\\n\", pd.Series(metrics_finbert[\"Signal_Returns\"]).quantile([.01, .05, .95, .99]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990e365",
   "metadata": {},
   "source": [
    "## Robustness Appendix Code: Log-Returns Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2dfd1",
   "metadata": {},
   "source": [
    "##### If differences between metrics (e.g., RMSE, Sharpe, accuracy) are small, you can claim that your results are robust to the choice of return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "df_gpt   = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market   = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Compute close-to-close log-return\n",
    "market = market.sort_values(\"Date\")  # Already loaded above\n",
    "\n",
    "# Convert Close column to float if not already\n",
    "market[\"Close\"] = pd.to_numeric(market[\"Close\"], errors=\"coerce\")\n",
    "\n",
    "# Calculate close-to-close log return\n",
    "market[\"LogReturn\"] = np.log(market[\"Close\"] / market[\"Close\"].shift(1))\n",
    "\n",
    "# Align test set with log-return target\n",
    "df = df_gpt[[\"Date\", \"Predicted_Return\"]].rename(columns={\"Predicted_Return\": \"LSTM_GPT4\"})\n",
    "df[\"LSTM_FinBERT\"] = df_fbert[\"Predicted_Return\"].values\n",
    "df = df.merge(market[[\"Date\", \"Return\", \"LogReturn\", \"VIX\"]], on=\"Date\", how=\"left\")\n",
    "df.dropna(subset=[\"LogReturn\", \"LSTM_GPT4\", \"LSTM_FinBERT\"], inplace=True)\n",
    "\n",
    "# Metric Functions\n",
    "def trading_signal_returns(true_returns, predicted_returns):\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60):\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "    try:\n",
    "        ap = average_precision_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        ap = np.nan\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1,\n",
    "        \"ROC_AUC\": roc, \"P-R AP\": ap,\n",
    "        \"Sharpe\": sharpe,\n",
    "    }\n",
    "\n",
    "# Metrics: Simple vs Log-Returns as targets\n",
    "metrics_simple = compute_all_metrics(df[\"Return\"].values, df[\"LSTM_GPT4\"].values)\n",
    "metrics_log = compute_all_metrics(df[\"LogReturn\"].values, df[\"LSTM_GPT4\"].values)\n",
    "\n",
    "metrics_simple_fb = compute_all_metrics(df[\"Return\"].values, df[\"LSTM_FinBERT\"].values)\n",
    "metrics_log_fb = compute_all_metrics(df[\"LogReturn\"].values, df[\"LSTM_FinBERT\"].values)\n",
    "\n",
    "# Creating Summary table\n",
    "summary = pd.DataFrame({\n",
    "    \"Hybrid_LSTM_SimpleReturn\": metrics_simple,\n",
    "    \"Hybrid_LSTM_LogReturn\": metrics_log,\n",
    "    \"FinBERT_LSTM_SimpleReturn\": metrics_simple_fb,\n",
    "    \"FinBERT_LSTM_LogReturn\": metrics_log_fb,\n",
    "})\n",
    "print(\"\\n===== Robustness Appendix: Model Performance (Simple vs Log-Returns) =====\\n\")\n",
    "print(summary)\n",
    "\n",
    "summary.to_csv(\"robustness_appendix_lstm_log_vs_simple.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095723fd",
   "metadata": {},
   "source": [
    "## Weak Stationarity Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4915d9",
   "metadata": {},
   "source": [
    "### Augmented Dickey-Fuller (ADF) and KPSS Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d1972",
   "metadata": {},
   "source": [
    "### ARCH-LM Test (for conditional variance stationarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Align\n",
    "df = pd.DataFrame({\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(\n",
    "    market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}),\n",
    "    on=\"Date\", how=\"left\"\n",
    ")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "returns = df[\"Market_True_Return\"].dropna().values\n",
    "\n",
    "# Running staitonarity and ARCH tests\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "# Augmented Dickey-Fuller\n",
    "adf_result = adfuller(returns)\n",
    "print(\"Augmented Dickey-Fuller (ADF) Test\")\n",
    "print(f\"ADF Statistic: {adf_result[0]:.3f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n",
    "print(\"Null hypothesis: series is non-stationary\")\n",
    "print(\"If p < 0.05, reject null (series is stationary)\")\n",
    "\n",
    "# KPSS Test\n",
    "kpss_result = kpss(returns, regression='c', nlags='auto')\n",
    "print(\"KPSS Test\")\n",
    "print(f\"KPSS Statistic: {kpss_result[0]:.3f}\")\n",
    "print(f\"p-value: {kpss_result[1]:.4f}\")\n",
    "print(\"Null hypothesis: series is stationary\")\n",
    "print(\"If p < 0.05, reject null (series is non-stationary)\")\n",
    "\n",
    "# ARCH-LM Test\n",
    "arch_test = het_arch(returns)\n",
    "print(\"ARCH-LM Test (for volatility clustering)\")\n",
    "print(f\"ARCH-LM Stat: {arch_test[0]:.2f}\")\n",
    "print(f\"p-value: {arch_test[1]:.4f}\")\n",
    "print(\"Null hypothesis: no ARCH effects (homoskedasticity)\")\n",
    "print(\"If p < 0.05, volatility is time-varying (common in finance)\")\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.stats.diagnostic import het_arch\n",
    "\n",
    "# Run all tests on returns (as before)\n",
    "returns = df[\"Market_True_Return\"].dropna().values\n",
    "\n",
    "adf_stat, adf_p, _, _, _, _ = adfuller(returns)\n",
    "kpss_stat, kpss_p, _, _ = kpss(returns, regression='c', nlags='auto')\n",
    "arch_stat, arch_p, _, _ = het_arch(returns)\n",
    "\n",
    "# Building summary tables as DataFrames\n",
    "stationarity_table = pd.DataFrame({\n",
    "    \"Test\": [\"ADF\", \"KPSS\"],\n",
    "    \"Statistic\": [adf_stat, kpss_stat],\n",
    "    \"p-value\": [adf_p, kpss_p],\n",
    "    \"Null Hypothesis\": [\n",
    "        \"Non-stationary (unit root present)\",\n",
    "        \"Stationary (no unit root)\"\n",
    "    ],\n",
    "    \"Interpretation\": [\n",
    "        \"Reject if p < 0.05 (series is stationary)\",\n",
    "        \"Reject if p < 0.05 (series is non-stationary)\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "arch_table = pd.DataFrame({\n",
    "    \"Test\": [\"ARCH-LM\"],\n",
    "    \"Statistic\": [arch_stat],\n",
    "    \"p-value\": [arch_p],\n",
    "    \"Null Hypothesis\": [\"No ARCH effects (homoskedasticity)\"],\n",
    "    \"Interpretation\": [\"Reject if p < 0.05 (volatility is time-varying)\"]\n",
    "})\n",
    "\n",
    "print(\"Stationarity Test Results\")\n",
    "print(stationarity_table.to_string(index=False))\n",
    "\n",
    "print(\"Conditional Heteroskedasticity (ARCH) Test\")\n",
    "print(arch_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9eac9b",
   "metadata": {},
   "source": [
    "## Practical Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad48ab7",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: Hybrid Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd880ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Cleaning Up Columns\n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns])\n",
    "\n",
    "# Ensuring all columns except Date are numeric\n",
    "for col in df.columns:\n",
    "    if col not in ['Date']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Using only meaningful features\n",
    "exclude_cols = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume'] # Remove prices and volume\n",
    "feature_cols = ['Return', 'VIX', 'Sentiment']\n",
    "# adding lagged features\n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"Sentiment_lag{lag}\"] = df[\"Sentiment\"].shift(lag)\n",
    "feature_cols += [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "feature_cols += [f\"Sentiment_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split by time\n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Scaling (IMPORTANT) \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(df_train[feature_cols])\n",
    "X_val = scaler.transform(df_val[feature_cols])\n",
    "X_test = scaler.transform(df_test[feature_cols])\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Create LSTM Sequences\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "# Directional accuracy\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_out = df_test.iloc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_out[\"Predicted_Return\"] = pred_test\n",
    "df_out.to_csv(\"derived/lstm_prac_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_out[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d26358",
   "metadata": {},
   "source": [
    "### Practical Applicaiton: FinBERT-only Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb411991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "data_path = \"derived/final_merged_FinBERT_for_lstm.csv\"\n",
    "df = pd.read_csv(data_path, parse_dates=[\"Date\"])\n",
    "\n",
    "# Cleaning columns \n",
    "df = df.drop(columns=[col for col in ['key_0', 'Date_sent'] if col in df.columns])\n",
    "\n",
    "for col in ['Close', 'High', 'Low', 'Open', 'Volume']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "if \"VIX\" in df.columns:\n",
    "    df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors='coerce')\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != 'Date':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering: Add lags for target & sentiment \n",
    "N_LAGS = 2\n",
    "for lag in range(1, N_LAGS+1):\n",
    "    df[f\"Return_lag{lag}\"] = df[\"Return\"].shift(lag)\n",
    "    df[f\"FinBERT_score_lag{lag}\"] = df[\"FinBERT_score\"].shift(lag)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Train/val/test split \n",
    "TRAIN_END = date(2022, 12, 31)\n",
    "VAL_END = date(2024, 5, 31)\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df_train = df[df[\"Date\"] <= pd.to_datetime(TRAIN_END)]\n",
    "df_val = df[(df[\"Date\"] > pd.to_datetime(TRAIN_END)) & (df[\"Date\"] <= pd.to_datetime(VAL_END))]\n",
    "df_test = df[df[\"Date\"] > pd.to_datetime(VAL_END)]\n",
    "\n",
    "# Features and Scaling\n",
    "FEATURES = ['Close', 'High', 'Low', 'Open', 'Volume', 'VIX', 'FinBERT_score'] + \\\n",
    "           [f\"Return_lag{lag}\" for lag in range(1, N_LAGS+1)] + \\\n",
    "           [f\"FinBERT_score_lag{lag}\" for lag in range(1, N_LAGS+1)]\n",
    "X_train = df_train[FEATURES].astype(np.float32).values\n",
    "X_val = df_val[FEATURES].astype(np.float32).values\n",
    "X_test = df_test[FEATURES].astype(np.float32).values\n",
    "y_train = df_train[\"Return\"].values\n",
    "y_val = df_val[\"Return\"].values\n",
    "y_test = df_test[\"Return\"].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Creating LSTM sequences (window=5)\n",
    "SEQ_LEN = 5\n",
    "def create_sequences(x, y, seq_len):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(x) - seq_len):\n",
    "        xs.append(x[i : i + seq_len])\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.array(xs), np.array(ys)\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, SEQ_LEN)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val, y_val, SEQ_LEN)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, SEQ_LEN)\n",
    "\n",
    "print(f\"Train samples: {X_train_seq.shape[0]}, Val: {X_val_seq.shape[0]}, Test: {X_test_seq.shape[0]}\")\n",
    "\n",
    "# LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(SEQ_LEN, X_train_seq.shape[2])),\n",
    "    Dense(1),\n",
    "])\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "print(\"Training LSTM …\")\n",
    "model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Evaluating …\")\n",
    "pred_test = model.predict(X_test_seq).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test_seq, pred_test))\n",
    "print(f\"Test RMSE: {rmse:.6f}\")\n",
    "actual_dir = (y_test_seq > 0)\n",
    "pred_dir = (pred_test > 0)\n",
    "acc = accuracy_score(actual_dir, pred_dir)\n",
    "print(f\"Directional accuracy: {acc:.2%}\")\n",
    "\n",
    "df_test = df_test.iloc[SEQ_LEN:].copy()  # align with y_test_seq\n",
    "df_test[\"Predicted_Return\"] = pred_test\n",
    "df_test.to_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", index=False)\n",
    "\n",
    "print(df_test[[\"Date\", \"Return\", \"Predicted_Return\"]].head())\n",
    "print(\"Final LSTM model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43581fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, accuracy_score,\n",
    "    precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Align data by date\n",
    "df = pd.DataFrame({\n",
    "    \"Date\": df_gpt[\"Date\"],\n",
    "    \"Market_Return\": df_gpt[\"Return\"],\n",
    "    \"LSTM_GPT4\": df_gpt[\"Predicted_Return\"],\n",
    "    \"LSTM_FinBERT\": df_fbert[\"Predicted_Return\"],\n",
    "})\n",
    "df = df.merge(market_df[[\"Date\", \"Return\", \"VIX\"]].rename(columns={\"Return\": \"Market_True_Return\"}), on=\"Date\", how=\"left\")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Data Diagnostics\")\n",
    "print(\"df columns:\", list(df.columns))\n",
    "for col in [\"Market_True_Return\", \"LSTM_GPT4\", \"LSTM_FinBERT\"]:\n",
    "    print(f\"{col} has {df[col].notnull().sum()} valid rows out of {len(df)}\")\n",
    "print(\"Rows in df:\", len(df))\n",
    "print(df.head())\n",
    "\n",
    "# Signal-based trading returns\n",
    "def trading_signal_returns(true_returns, predicted_returns):\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "# Compute all metrics for each model (with rolling mean/std)\n",
    "def compute_all_metrics(true_returns, predicted_returns, rolling_window=60):\n",
    "    if len(true_returns) == 0 or len(predicted_returns) == 0:\n",
    "        raise ValueError(\"Empty input arrays! Check your mask and input data.\")\n",
    "    mse = mean_squared_error(true_returns, predicted_returns)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_returns, predicted_returns)\n",
    "\n",
    "    true_up = (true_returns > 0).astype(int)\n",
    "    pred_up = (predicted_returns > 0).astype(int)\n",
    "\n",
    "    acc = accuracy_score(true_up, pred_up)\n",
    "    prec = precision_score(true_up, pred_up, zero_division=0)\n",
    "    rec = recall_score(true_up, pred_up, zero_division=0)\n",
    "    f1 = f1_score(true_up, pred_up, zero_division=0)\n",
    "    try:\n",
    "        roc = roc_auc_score(true_up, predicted_returns)\n",
    "    except:\n",
    "        roc = np.nan\n",
    "\n",
    "    cm = confusion_matrix(true_up, pred_up)\n",
    "    strat_returns = trading_signal_returns(true_returns, predicted_returns)\n",
    "    cum_return = np.cumprod(1 + strat_returns)[-1] - 1 if len(strat_returns) > 0 else np.nan\n",
    "    sharpe = np.mean(strat_returns) / (np.std(strat_returns) + 1e-9) * np.sqrt(252)\n",
    "    roll_sharpe = pd.Series(strat_returns).rolling(rolling_window).apply(\n",
    "        lambda x: np.mean(x) / (np.std(x) + 1e-9) * np.sqrt(252), raw=True)\n",
    "    roll_acc = pd.Series(pred_up == true_up).rolling(rolling_window).mean()\n",
    "    roll_cum_return = (1 + pd.Series(strat_returns)).cumprod() - 1\n",
    "    rolling_sharpe_mean, rolling_sharpe_std = roll_sharpe.mean(), roll_sharpe.std()\n",
    "    rolling_acc_mean, rolling_acc_std = roll_acc.mean(), roll_acc.std()\n",
    "    return {\n",
    "        \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae,\n",
    "        \"Direction_Acc\": acc, \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"ROC_AUC\": roc,\n",
    "        \"Sharpe\": sharpe, \"Cumulative_Return\": cum_return,\n",
    "        \"Rolling_Sharpe_Mean\": rolling_sharpe_mean, \"Rolling_Sharpe_Std\": rolling_sharpe_std,\n",
    "        \"Rolling_Acc_Mean\": rolling_acc_mean, \"Rolling_Acc_Std\": rolling_acc_std,\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Rolling_Sharpe\": roll_sharpe,\n",
    "        \"Rolling_Acc\": roll_acc,\n",
    "        \"Rolling_CumReturn\": roll_cum_return,\n",
    "        \"Signal_Returns\": strat_returns\n",
    "    }\n",
    "\n",
    "# Create valid data masks and calculate for both models\n",
    "mask_gpt = df['Market_True_Return'].notnull() & df['LSTM_GPT4'].notnull()\n",
    "mask_fbert = df['Market_True_Return'].notnull() & df['LSTM_FinBERT'].notnull()\n",
    "\n",
    "print(\"\\nValid rows for GPT:\", mask_gpt.sum())\n",
    "print(\"Valid rows for FinBERT:\", mask_fbert.sum())\n",
    "\n",
    "if mask_gpt.sum() == 0 or mask_fbert.sum() == 0:\n",
    "    raise ValueError(\"No valid data rows for at least one model. Check input data and merges!\")\n",
    "\n",
    "metrics_gpt = compute_all_metrics(\n",
    "    df.loc[mask_gpt, 'Market_True_Return'].values,\n",
    "    df.loc[mask_gpt, 'LSTM_GPT4'].values\n",
    ")\n",
    "metrics_finbert = compute_all_metrics(\n",
    "    df.loc[mask_fbert, 'Market_True_Return'].values,\n",
    "    df.loc[mask_fbert, 'LSTM_FinBERT'].values\n",
    ")\n",
    "\n",
    "# Table for thesis (include rolling mean/std)\n",
    "comparison_table = pd.DataFrame({\n",
    "    \"Hybrid (FinBERT+GPT-4)\": {k: v for k, v in metrics_gpt.items() if not isinstance(v, (np.ndarray, pd.Series, list))},\n",
    "    \"FinBERT-only\": {k: v for k, v in metrics_finbert.items() if not isinstance(v, (np.ndarray, pd.Series, list))}\n",
    "})\n",
    "print(\"Model Comparison Table\")\n",
    "print(comparison_table)\n",
    "comparison_table.to_csv(\"model_performance_comparison.csv\")\n",
    "\n",
    "# Confusion matrices for appendix\n",
    "print(\"Confusion Matrices\")\n",
    "print(\"Hybrid (FinBERT+GPT-4):\\n\", metrics_gpt[\"Confusion_Matrix\"])\n",
    "print(\"FinBERT-only:\\n\", metrics_finbert[\"Confusion_Matrix\"])\n",
    "np.savetxt(\"hybrid_confusion_matrix.csv\", metrics_gpt[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "np.savetxt(\"finbert_confusion_matrix.csv\", metrics_finbert[\"Confusion_Matrix\"], delimiter=\",\")\n",
    "\n",
    "# Regime split (by VIX): show model dominance in high/low volatility\n",
    "vix_median = df[\"VIX\"].median()\n",
    "df[\"VIX_regime\"] = np.where(df[\"VIX\"] > vix_median, \"High_VIX\", \"Low_VIX\")\n",
    "def regime_metrics(regime):\n",
    "    idx = df[\"VIX_regime\"] == regime\n",
    "    gpt_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_gpt, \"Market_True_Return\"].values, df.loc[idx & mask_gpt, \"LSTM_GPT4\"].values\n",
    "    )\n",
    "    finbert_metrics = compute_all_metrics(\n",
    "        df.loc[idx & mask_fbert, \"Market_True_Return\"].values, df.loc[idx & mask_fbert, \"LSTM_FinBERT\"].values\n",
    "    )\n",
    "    return gpt_metrics, finbert_metrics\n",
    "for regime in [\"High_VIX\", \"Low_VIX\"]:\n",
    "    gpt_metrics, finbert_metrics = regime_metrics(regime)\n",
    "    print(f\"\\n=== {regime} Regime\")\n",
    "    print(\"Hybrid Sharpe:\", gpt_metrics[\"Sharpe\"], \"Cumulative:\", gpt_metrics[\"Cumulative_Return\"])\n",
    "    print(\"FinBERT-only Sharpe:\", finbert_metrics[\"Sharpe\"], \"Cumulative:\", finbert_metrics[\"Cumulative_Return\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c601ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_gpt = pd.read_csv(\"derived/lstm_prac_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "df_fbert = pd.read_csv(\"derived/lstm_prac_FinBERT_only_test_predictions.csv\", parse_dates=[\"Date\"])\n",
    "market_df = pd.read_csv(\"sp500_vix_data.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "# Merge on Date\n",
    "df = df_gpt[[\"Date\", \"Return\", \"Predicted_Return\"]].rename(\n",
    "    columns={\"Return\": \"Market_Return\", \"Predicted_Return\": \"LSTM_GPT4\"}\n",
    ").merge(\n",
    "    df_fbert[[\"Date\", \"Predicted_Return\"]].rename(\n",
    "        columns={\"Predicted_Return\": \"LSTM_FinBERT\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"inner\"\n",
    ").merge(\n",
    "    market_df[[\"Date\", \"Return\", \"VIX\"]].rename(\n",
    "        columns={\"Return\": \"Market_True_Return\"}\n",
    "    ),\n",
    "    on=\"Date\", how=\"left\"\n",
    ")\n",
    "df[\"VIX\"] = pd.to_numeric(df[\"VIX\"], errors=\"coerce\")\n",
    "\n",
    "# Signal-based returns\n",
    "def trading_signal_returns(true_returns, predicted_returns):\n",
    "    signal = np.where(predicted_returns > 0, 1, -1)\n",
    "    return signal * true_returns\n",
    "\n",
    "def compound_cumret(returns):\n",
    "    return np.cumprod(1 + returns) - 1\n",
    "\n",
    "# Masking for valid rows (overlapping dates)\n",
    "mask = df[\"Market_True_Return\"].notnull() & df[\"LSTM_GPT4\"].notnull() & df[\"LSTM_FinBERT\"].notnull()\n",
    "if not mask.any():\n",
    "    raise ValueError(\"No valid rows after masking. Check merge.\")\n",
    "\n",
    "df_valid = df[mask].copy().reset_index(drop=True)\n",
    "\n",
    "# Strategy returns\n",
    "df_valid[\"LSTM_GPT4_Strategy\"] = trading_signal_returns(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_GPT4\"])\n",
    "df_valid[\"FinBERT_Strategy\"] = trading_signal_returns(df_valid[\"Market_True_Return\"], df_valid[\"LSTM_FinBERT\"])\n",
    "\n",
    "# Cumulative compounded returns\n",
    "df_valid[\"Market_CumReturn\"] = compound_cumret(df_valid[\"Market_True_Return\"])\n",
    "df_valid[\"LSTM_GPT4_CumReturn\"] = compound_cumret(df_valid[\"LSTM_GPT4_Strategy\"])\n",
    "df_valid[\"FinBERT_CumReturn\"] = compound_cumret(df_valid[\"FinBERT_Strategy\"])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"Market_CumReturn\"], label=\"Market Cumulative Return\", color='black', linewidth=2)\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"LSTM_GPT4_CumReturn\"], label=\"Hybrid LSTM-GPT4 Cumulative Return\", color='red')\n",
    "plt.plot(df_valid[\"Date\"], df_valid[\"FinBERT_CumReturn\"], label=\"FinBERT-only Cumulative Return\", color='darkorange')\n",
    "plt.axhline(0, color='grey', linestyle=':', linewidth=0.7)\n",
    "plt.title(\"Cumulative Compounded Returns Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Return (Compounded)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm-thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
